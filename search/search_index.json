{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Oliver Mussell","text":""},{"location":"#about","title":"About","text":"<p>Working as a Systems Administrator since 2012 in many industries, ranging from Medical, Energy Generation, Environmental, Government and Education.</p> <p>Most recent previous job was maintaining ~5,000 websites for UK schools along with various services to allow schools to communicate with parents.</p> <p>Currently working as an SRE at Crossref.</p> <ul> <li>Managing AWS infrastructure with Terraform</li> <li>Running applications as systemd services on EC2 managed with ansible</li> <li>Building container images in CI/CD pipelines and deploying to Fargate repo</li> <li>Observability with Prometheus, Grafana, Vector, Opensearch Dashboards</li> <li>Migrating legacy on-prem applications to cloud</li> </ul>"},{"location":"#interests","title":"Interests","text":"<p>Video games, cycling, cryptography</p>"},{"location":"#contact","title":"Contact","text":"<p>You can reach me via email at: my first name dot last name @ hotmail.co.uk</p>"},{"location":"#projects","title":"Projects","text":"<ul> <li> <p>quire - Generate a One Time Pad notebook for unbreakable encryption</p> </li> <li> <p>bun - Simple YAML Based Puppet ENC written in Nim</p> </li> <li> <p>dotfiles - Workstation config files</p> </li> </ul>"},{"location":"basecamp/","title":"Basecamp Books","text":""},{"location":"basecamp/#shape-up","title":"Shape Up","text":"<ul> <li>Shaped versus unshaped work</li> <li>Setting appetites instead of estimates</li> <li>Designing at the right level of abstraction</li> <li>Concepting with breadboards and fat marker sketches</li> <li>Making bets with a capped downside (the circuit breaker) and honouring them with uninterrupted time</li> <li>Choosing the right cycle length (six weeks)</li> <li>A cool-down period between cycles</li> <li>Breaking projects apart into scopes</li> <li>Downhill versus uphill work and communicating about unknowns</li> <li>Scope hammering to separate must-haves from nice-to-haves</li> </ul>"},{"location":"bhyve-vm-creation/","title":"Bhyve VM Creation","text":""},{"location":"bhyve-vm-creation/#bhyve-initial-setup","title":"Bhyve Initial Setup","text":"<p>Enable the tap interface in <code>/etc/sysctl.conf</code> and load it on the currently running system</p> <pre><code>net.link.tap.up_on_open=1\nsysctl -f /etc/sysctl.conf\n</code></pre> <p>Enable bhyve, serial console and bridge/tap interface kernel modules in <code>/boot/loader.conf</code>. Reboot to apply changes or use kldload.</p> <pre><code>vmm_load=\"YES\"\nnmdm_load=\"YES\"\nif_bridge_load=\"YES\"\nif_tap_load=\"YES\"\n</code></pre> <p>Set up the network interfaces in <code>/etc/rc.conf</code></p> <pre><code>cloned_interfaces=\"bridge0 tap0\"\nifconfig_bridge0=\"addm re0 addm tap0\"\n</code></pre> <p>Create a ZFS volume</p> <pre><code>zfs create -V16G -o volmode=dev zroot/testvm\n</code></pre> <p>Download the installation image</p> <pre><code>fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/ISO-IMAGES/11.1/FreeBSD-11.1-RELEASE-amd64-disc1.iso \n</code></pre> <p>Start the VM</p> <pre><code>sh /usr/share/examples/bhyve/vmrun.sh -c 1 -m 512M -t tap0 -d /dev/zvol/zroot/testvm -i -I FreeBSD-11.1-RELEASE-amd64-disc1.iso testvm\n</code></pre> <p>Install as normal, following the menu options</p>"},{"location":"bhyve-vm-creation/#new-vm-creation-script","title":"New VM Creation Script","text":"<pre><code>#! /bin/sh\nread -p \"Enter hostname: \" hostname\nzfs create -V16G -o volmode=dev zroot/$hostname\nsh /usr/share/examples/bhyve/vmrun.sh -c 1 -m 512M -t tap0 -d /dev/zvol/zroot/$hostname -i -I ~/FreeBSD-11.1-RELEASE-amd64-disc1.iso $hostname\n</code></pre>"},{"location":"bhyve-vm-creation/#creating-a-linux-guest","title":"Creating a Linux guest","text":"<p>Create a file for the hard disk</p> <pre><code>truncate -s 16G linux.img\n</code></pre> <p>Create the file to map the virtual devices for kernel load</p> <pre><code>~/device.map\n\n(hd0) /root/linux.img\n(cd0) /root/linux.iso\n</code></pre> <p>Load the kernel</p> <pre><code>grub-bhyve -m ~/device.map -r cd0 -M 1024M linuxguest\n</code></pre> <p>Grub should start, choose install as normal</p> <p>Start the VM</p> <pre><code>bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,tap0 -s 3:0,virtio-blk,/root/linux.img -l com1,/dev/nmdm0A -c 1 -m 512M linuxguest\n</code></pre> <p>Access through the serial console</p> <pre><code>cu -l /dev/nmdm0B\n</code></pre>"},{"location":"bhyve-vm-creation/#pfsense-in-a-vm","title":"pfSense in a VM","text":"<p>Download the pfSense disk image from the website using fetch</p> <pre><code>fetch https://frafiles.pfsense.org/mirror/downloads/pfSense-CE-2.3.1-RELEASE-2g-amd64-nanobsd.img.gz -o ~/pfSense.img.gz\n</code></pre> <p>Create the storage</p> <pre><code>zfs create -V2G -o volmode=dev zroot/pfsense\n</code></pre> <p>Unzip the file, and redirect output to the storage via dd</p> <pre><code>gzip -dc pfSense.img.gz | dd of=/dev/zvol/zroot/pfsense obs=64k\n</code></pre> <p>Load the kernel and start the boot process</p> <pre><code>bhyveload -c /dev/nmdm0A -d /dev/zvol/zroot/pfsense -m 256MB pfsense\n</code></pre> <p>Start the VM</p> <pre><code>/usr/sbin/bhyve -c 1 -m 256 -A -H -P -s 0:0,hostbridge -s 1:0,virtio-net,tap0 -s 3:0,ahci-hd,/dev/zvol/zroot/pfsense -s 4:1,lpc -l com1,/dev/nmdm0A pfsense\n</code></pre> <p>Connect to the VM via the serial connection with nmdm</p> <pre><code>cu -l /dev/nmdm0B\n</code></pre> <p>Perform initial configuration through the shell to assign the network interfaces</p> <p>Once done, use the IP address to access through the web console </p> <p>When finished, you can shutdown/reboot</p> <p>To de-allocate the resources, you need to destroy the VM</p> <pre><code>bhyvectl --destroy --vm=pfsense\n</code></pre>"},{"location":"bhyve-vm-creation/#multiple-vms-using-bhyve","title":"Multiple VMs using bhyve","text":"<p>To allow networking on multiple vms, there should be a tap assigned to each vm, connected to the same bridge. </p> <pre><code>cloned_interfaces=\"bridge0 tap0 tap1 tap2\"\nifconfig_bridge0=\"addm re0 addm tap0 addm tap1 addm tap2\"\n</code></pre> <p>Then when you provision vms, assign one of the tap interfaces to them.</p>"},{"location":"bhyve-vm-creation/#vm-bhyve","title":"vm-bhyve","text":"<p>A better way for managing a bhyve hypervisor.</p> <p>Follow the instructions on the repo.</p> <p>When adding the switch to a network interface, it doesn't work with re0. tap1 works, but then internet doesnt work in the VMs. Needs sorting.</p> <p>zfs </p> <p>bsd-cloud-init should be tested, it sets hostname based on openstack image name.</p> <p>otherwise, if we figure out how to make a template VM, you could set the hostname as part of transferring over the rc.conf file</p> <p>create template VM, start it, zfs send/recv?</p>"},{"location":"bonded-nic/","title":"Creating Bonded NICs on Ubuntu 20.04","text":""},{"location":"bonded-nic/#summary","title":"Summary","text":"<p>On the HP MicroServer Gen 8 and MicroServer Gen 10+ there are four ethernet ports. I wanted to group these together into a bonded NIC so that ethernet traffic could run over all four to increase the throughput.</p> <p>There are other names for bonding, like teaming or link aggregation/LACP. They all mean the same thing, multiple network ports joined together to serve traffic.</p>"},{"location":"bonded-nic/#setup","title":"Setup","text":"<p>Install the dependency:</p> <pre><code>apt install ifenslave\n</code></pre> <p>Load the kernel module:</p> <pre><code># Check if already loaded:\nlsmod | grep bonding\n\n# If no output then:\nmodprobe bonding\n</code></pre> <p>This only loads the bonding kernel module while the system is running, it would be lost on reboot. Add it to the modules file to load it at boot as well:</p> <pre><code>vim /etc/modules\n\n# Add the following line to the file:\nbonding\n</code></pre> <p>Find the network interfaces. You can do this in a few different ways, an easy way is just:</p> <pre><code>ip addr\n</code></pre> <p>Which lists all of your network interfaces. <code>lo</code> is for loopback, then the others will be your ethernet interfaces. This can differ between different NIC manufacturers. On this machine, its returning names like <code>eno1</code>, <code>eno2</code> etc. Sometimes it is like <code>enp2s0</code> or <code>enp3s0</code> instead.</p> <p>Edit the netplan config file to add the config. You will need to know the IP address of your gateway/router and the IP addresses for the DNS nameservers. For me, the router IP is <code>192.168.0.1</code> and the DNS servers are <code>192.168.0.15</code> (Rpi) and <code>1.1.1.1</code> (Cloudflare).</p> <p>Also, I've set the bonded NIC to use DHCP to configure its IP address. I'm also using all available <code>eno*</code> ethernet ports in the bond. If you want to use a specific set of ports instead, check out the netplan documentation</p> <pre><code>vim /etc/netplan/00-installer-config.yaml\n\nnetwork:\n  version: 2\n  ethernets:\n    eports:\n      match:\n        name: eno*\n      optional: true\n  bonds:\n    bond0:\n      interfaces: [eports]\n      dhcp4: true\n      gateway4: 192.168.0.1\n      nameservers:\n        addresses: [192.168.0.15, 1.1.1.1]\n      parameters:\n        mode: 802.3ad\n        lacp-rate: fast\n        mii-monitor-interval: 100\n</code></pre> <p>Apply the changes:</p> <pre><code>netplan apply\n</code></pre> <p>If you lose your SSH connection, something went wrong, or DHCP has just decided to give it a different IP address than what you used to connect. Its a good idea to have out of band management or a spare keyboard+monitor plugged in if the network stops working.</p>"},{"location":"bonded-nic/#end-result","title":"End result","text":"<p>If you run <code>ip addr</code> again, you can now see a new network interface has been created called <code>bond0</code>, which is the master. Then the <code>eno*</code> interfaces have been added as slaves of <code>bond0</code>.</p> <pre><code>root@dixie:~# ip addr\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: eno1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP group default qlen 1000\n    link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff\n3: eno2: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP group default qlen 1000\n    link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff\n4: eno3: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP group default qlen 1000\n    link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff\n5: eno4: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP group default qlen 1000\n    link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff\n6: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.21/24 brd 192.168.0.255 scope global dynamic bond0\n       valid_lft 84976sec preferred_lft 84976sec\n    inet6 fe80::dcd5:c6ff:feda:5ba0/64 scope link \n       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"fbsd-update-cache/","title":"Caching freebsd-update and pkg files","text":"<p>Change the domains as appropriate. The proxy_store location is where the cached files will be placed. This directory needs to be accessible by the user that NGINX is running as (defaults to www).</p> <p>NGINX config:</p> <pre><code># pkg\nserver {\n\n  listen *:80;\n\n  server_name           pkg.mydomain.local;\n\n  access_log            /var/log/nginx/pkg.access.log;\n  error_log             /var/log/nginx/pkg.error.log;\n\n  location / {\n    root      /var/cache/packages/freebsd;\n    try_files $uri @pkg_cache;\n  }\n\n  location @pkg_cache {\n    proxy_pass                  https://pkg.freebsd.org;\n    proxy_set_header            Host $host;\n    proxy_cache_lock            on;\n    proxy_cache_lock_timeout    20s;\n    proxy_cache_revalidate      on;\n    proxy_cache_valid           200 301 302 30d;\n    proxy_store                 /var/cache/packages/freebsd/$request_uri;\n  }\n\n}\n\n# freebsd-update\nserver {\n\n  listen *:80;\n\n  server_name           freebsd-update.mydomain.local;\n\n  access_log            /var/log/nginx/freebsd_update.access.log;\n  error_log             /var/log/nginx/freebsd_update.error.log;\n\n  location / {\n    root      /var/cache/freebsd-update;\n    try_files $uri @freebsd_update_cache;\n  }\n\n  location @freebsd_update_cache {\n    proxy_pass                  http://update.freebsd.org;\n    proxy_set_header            Host update.freebsd.org;\n    proxy_cache_lock            on;\n    proxy_cache_lock_timeout    20s;\n    proxy_cache_revalidate      on;\n    proxy_cache_valid           200 301 302 30d;\n    proxy_store                 /var/cache/freebsd-update/$request_uri;\n  }\n\n}\n</code></pre> <p>Client config:</p> <p>Create <code>/usr/local/etc/pkg/repos/FreeBSD.conf</code> with this content:</p> <pre><code>FreeBSD: { enabled: NO }\nMyRepo: {\n    url: \"pkg+http://pkg.mydomain.local/${ABI}/latest\",\n    enabled:    true,\n    signature_type: \"fingerprints\",\n    fingerprints: \"/usr/share/keys/pkg\",\n    mirror_type: \"srv\"\n}\n</code></pre> <p>Edit <code>/etc/freebsd-update.conf</code>, change <code>ServerName</code> value to <code>freebsd-update.mydomain.local</code>.</p>"},{"location":"fff/","title":"Food, Forests and Freedom","text":""},{"location":"firecracker/","title":"Firecracker","text":"<p>Firecracker - Secure and fast microVMs for serverless computing.</p>"},{"location":"firecracker/#quickstart","title":"Quickstart","text":"<p>Quickstart</p>"},{"location":"firecracker/#get-firecracker","title":"Get firecracker","text":"<pre><code>release_url=\"https://github.com/firecracker-microvm/firecracker/releases\"\nlatest=$(basename $(curl -fsSLI -o /dev/null -w  %{url_effective} ${release_url}/latest))\ncurl -L ${release_url}/download/${latest}/firecracker-${latest}-x86_64.tgz | tar -xz\n</code></pre>"},{"location":"firecracker/#get-the-kernel-and-rootfs","title":"Get the kernel and rootfs","text":"<pre><code># Official instructions\n\ndest_kernel=\"hello-vmlinux.bin\"\ndest_rootfs=\"hello-rootfs.ext4\"\nimage_bucket_url=\"https://s3.amazonaws.com/spec.ccfc.min/img/quickstart_guide/x86_64\"\n\n\nkernel=\"${image_bucket_url}/kernels/vmlinux.bin\"\nrootfs=\"${image_bucket_url}/rootfs/bionic.rootfs.ext4\"\n\ncurl -fsSL -o $dest_kernel $kernel\ncurl -fsSL -o $dest_rootfs $rootfs\n\n\n\n# Same but with a recently built image as gleaned from the bucket list\ndest_kernel=\"hello-vmlinux.bin\"\ndest_rootfs=\"hello-rootfs.ext4\"\nkernel=\"https://s3.amazonaws.com/spec.ccfc.min/img-dev/x86_64/ubuntu/kernel/vmlinux.bin\"\nrootfs=\"https://s3.amazonaws.com/spec.ccfc.min/img-dev/x86_64/ubuntu/fsfiles/bionic.rootfs.ext4\"\ncurl -fsSL -o $dest_kernel $kernel\ncurl -fsSL -o $dest_rootfs $rootfs\n</code></pre>"},{"location":"firecracker/#using-firectl","title":"Using firectl","text":""},{"location":"firecracker/#get-firectl","title":"Get firectl","text":"<pre><code>curl -Lo firectl https://firectl-release.s3.amazonaws.com/firectl-v0.1.0\nchmod +x firectl\n</code></pre>"},{"location":"firecracker/#create-microvm","title":"Create microvm","text":"<pre><code>./firectl \\\n  --kernel=hello-vmlinux.bin \\\n  --root-drive=hello-rootfs.ext4\n</code></pre>"},{"location":"firecracker/#using-the-api","title":"Using the API","text":""},{"location":"firecracker/#set-the-guest-kernel","title":"Set the guest kernel","text":"<pre><code>kernel_path=$(pwd)\"/hello-vmlinux.bin\"\n\ncurl --unix-socket /tmp/firecracker.socket -i \\\n  -X PUT 'http://localhost/boot-source'   \\\n  -H 'Accept: application/json'           \\\n  -H 'Content-Type: application/json'     \\\n  -d \"{\n        \\\"kernel_image_path\\\": \\\"${kernel_path}\\\",\n        \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\"\n   }\"\n</code></pre>"},{"location":"firecracker/#set-the-guest-rootf","title":"Set the guest rootf","text":"<pre><code>rootfs_path=$(pwd)\"/hello-rootfs.ext4\"\ncurl --unix-socket /tmp/firecracker.socket -i \\\n  -X PUT 'http://localhost/drives/rootfs' \\\n  -H 'Accept: application/json'           \\\n  -H 'Content-Type: application/json'     \\\n  -d \"{\n        \\\"drive_id\\\": \\\"rootfs\\\",\n        \\\"path_on_host\\\": \\\"${rootfs_path}\\\",\n        \\\"is_root_device\\\": true,\n        \\\"is_read_only\\\": false\n   }\"\n</code></pre>"},{"location":"firecracker/#start-the-guest-machine","title":"Start the guest machine","text":"<pre><code>curl --unix-socket /tmp/firecracker.socket -i \\\n  -X PUT 'http://localhost/actions'       \\\n  -H  'Accept: application/json'          \\\n  -H  'Content-Type: application/json'    \\\n  -d '{\n      \"action_type\": \"InstanceStart\"\n   }'\n</code></pre>"},{"location":"firecracker/#compile-kernel-and-fs-manually","title":"Compile Kernel and FS manually","text":"<p>Follow the steps in here to compile the kernel and base file. </p> <p>On Ubuntu when compiling you need to install dependencies like libssl-dev, libncurses-dev, bison, autoconf.</p> <p>Then if you try and compile and it complains about auto.conf not existing, run make menuconfig, then exit out immediately. That seems to have sorted it.</p> <p>Then when you run make vmlinux it asks lots of questions, but by using the preexisting config file from the repo a lot has already been decided. You could probably pipe yes into this, or otherwise just hold enter. Someone with more kernel experience needs to go over those options and decide if they're necessary. </p> <p>Once compiled continue with the getting started instructions but change the path to the kernel file to the vmlinux you created.</p> <p>I compiled 5.4 kernel and used the existing alpine base from the getting started and it boots just fine.</p>"},{"location":"firecracker/#using-the-jailer","title":"Using the jailer","text":""},{"location":"firecracker/#install","title":"Install","text":"<p>The jailer is used to provide additional isolation for the VMs.</p> <p>The jailer binary is included in the tgz file from the firecracker release.</p> <pre><code># It needs to be built statically linked to musl.\n\napt install -y musl-tools\n\n# install rust via rustup\n\n# cd into the jailer directory in the firecracker repo\ncargo build --target=\"x86_64-unknown-linux-musl\" --release\n\n# the built binary gets created at:\n../../build/cargo_target/x86_64-unknown-linux-musl/release/jailer\n\n# you should probably build with tools/devtool build instead\n</code></pre>"},{"location":"firecracker/#run","title":"Run","text":"<p>Rather than running the jailer binary manually, you can use the <code>--jailer</code> flag with firectl. Note that you must also include the <code>--chroot-base-dir=\"/srv/jailer\"</code> flag, otherwise you get the <code>no such file or directory</code> error as per this issue.</p> <p>You also need to copy or move the firectl, firecracker, and jailer binaries into a bin directory in your $PATH otherwise it complains. I copied them to /usr/local/bin</p> <pre><code>/usr/local/bin/firectl --kernel=/root/release-v0.25.2-x86_64/hello-vmlinux.bin --root-drive=/root/release-v0.25.2-x86_64/hello-rootfs.ext4 --jailer=/usr/local/bin/jailer --exec-file=/usr/local/bin/firecracker --id=testvm4 --chroot-base-dir=\"/srv/jailer\"\n</code></pre> <p>Firectl also doesnt handle cleaning up the /srv/jailer/firecracker/$vm_name chroot directory when you power off the VM. So you need to clean this up manually.</p>"},{"location":"firecracker/#notes-from-running-the-jailer-manually","title":"Notes from running the jailer manually","text":"<pre><code>cp -v rootfs.ext4 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/\ncp -v vmlinux.bin /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/\nls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/\nls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket \nls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/\n</code></pre> <pre><code>curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/boot-source'     -H 'Accept: application/json'             -H 'Content-Type: application/json'       -d \"{\n        \\\"kernel_image_path\\\": \\\"/root/vmlinux.bin\\\",\n        \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\"\n   }\"\nls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin \ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/boot-source'     -H 'Accept: application/json'             -H 'Content-Type: application/json'       -d \"{\n        \\\"kernel_image_path\\\": \\\"/srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin\\\",\n        \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\"\n   }\"\nls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/\nchown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin \nchown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/rootfs.ext4 \ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/boot-source'     -H 'Accept: application/json'             -H 'Content-Type: application/json'       -d \"{\n        \\\"kernel_image_path\\\": \\\"/srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin\\\",\n        \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\"\n   }\"\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/boot-source'     -H 'Accept: application/json'             -H 'Content-Type: application/json'       -d \"{\n        \\\"kernel_image_path\\\": \\\"/root/vmlinux.bin\\\",\n        \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\"\n   }\"\nls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/boot-source'     -H 'Accept: application/json'             -H 'Content-Type: application/json'       -d \"{\n        \\\"kernel_image_path\\\": \\\"/root/vmlinux.bin\\\",\n        \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\"\n   }\"\nls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/\nls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/\nls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/boot-source'     -H 'Accept: application/json'             -H 'Content-Type: application/json'       -d \"{\n        \\\"kernel_image_path\\\": \\\"./vmlinux.bin\\\",\n        \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\"\n   }\"\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/drives/rootfs'   -H 'Accept: application/json'             -H 'Content-Type: application/json'       -d \"{\n        \\\"drive_id\\\": \\\"rootfs\\\",\n        \\\"path_on_host\\\": \\\"./rootfs.ext4\\\",\n        \\\"is_root_device\\\": true,\n        \\\"is_read_only\\\": false\n   }\"\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/actions'         -H  'Accept: application/json'            -H  'Content-Type: application/json'      -d '{\n      \"action_type\": \"InstanceStart\"\n   }'\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X GET 'http://localhost/'         -H  'Accept: application/json'            -H  'Content-Type: application/json'\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/actions'         -H  'Accept: application/json'            -H  'Content-Type: application/json'      -d '{\n      \"action_type\": \"InstanceStop\"\n   }'\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/actions'         -H  'Accept: application/json'            -H  'Content-Type: application/json'      -d '{\n      \"action_type\": \"InstanceStop\"\n   }'\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/actions'         -H  'Accept: application/json'            -H  'Content-Type: application/json'      -d '{\n      \"action_type\": \"SendCtrlAltDel\"\n   }'\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X GET 'http://localhost/'         -H  'Accept: application/json'            -H  'Content-Type: application/json'\nps aux | grep fire\nls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/\nls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/\n</code></pre> <pre><code>mkdir -p /srv/jailer\ncp -v ./release-v1.1.2-x86_64/jailer-v1.1.2-x86_64 /usr/bin/jailer\nls -alh /usr/bin/jailer\nls /var/run/netns\nls /var/run\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --daemonize\naddgroup -G 1111 551e7604-e35c-42b3-b825-416853441234\naddgroup ---gid 1111 551e7604-e35c-42b3-b825-416853441234\naddgroup --gid 1111 551e7604\naddgroup --gid 1111 551e7604-e35c-42b3-b825-416853441234\naddgroup --gid 1111 551e7604\naddgroup --gid 1111 5517604\naddgroup --gid 1111 mygroupname\nadduser\nadduser myusername\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --daemonize --uid 1001 --gid 1111\nls /usr/bin/firecracker\ncp -v ./release-v1.1.2-x86_64/firecracker-v1.1.2-x86_64 /usr/bin/firecracker\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --daemonize --uid 1001 --gid 1111\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize\nrm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize\nrm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/\nmkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/\nchown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/\nls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize\nmkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/\nchown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize\nrm /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/dev/net/tun\nrm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/\nmkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/\nchown -R 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 \nrm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/\nmkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/\nchown -R 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 \n</code></pre> <pre><code>curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/boot-source'     -H 'Accept: application/json'             -H 'Content-Type: application/json'       -d \"{\n        \\\"kernel_image_path\\\": \\\"./vmlinux.bin\\\",\n        \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\"\n   }\"\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/drives/rootfs'   -H 'Accept: application/json'             -H 'Content-Type: application/json'       -d \"{\n        \\\"drive_id\\\": \\\"rootfs\\\",\n        \\\"path_on_host\\\": \\\"./rootfs.ext4\\\",\n        \\\"is_root_device\\\": true,\n        \\\"is_read_only\\\": false\n   }\"\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/network-interfaces/eth0'   -H 'Accept: application/json'   -H 'Content-Type: application/json'   -d '{\n      \"iface_id\": \"eth0\",\n      \"host_dev_name\": \"tap0\"\n    }\n\n'\ncurl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i   -X PUT 'http://localhost/actions'         -H  'Accept: application/json'            -H  'Content-Type: application/json'      -d '{\n      \"action_type\": \"InstanceStart\"\n   }'\nps aux | grep jail\nps aux | grep fire\nip addr | less\nping 10.0.0.1\nping 10.0.0.2\npkill firecracker\n</code></pre> <pre><code>ip tuntap add tap0 mode tap\nip addr\nip addr | grep bond0\nip addr add 10.0.0.1/24 dev tap0\nip link set tap0 up\nsh -c \"echo 1 &gt; /proc/sys/net/ipv4/ip_forward\"\niptables --help\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\niptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\niptables -A FORWARD -i tap0 -o eth0 -j ACCEPT\nps aux | grep fire\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 \nrm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/dev\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 \nrm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 \nrm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run\nrm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/dev\n/usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 \n</code></pre>"},{"location":"gemini/","title":"Gemini Protocol","text":"<p>Project Gemini</p> <pre><code>Gemini is a new, collaboratively designed internet protocol, which explores the space inbetween gopher and the web, striving to address (perceived) limitations of one while avoiding the (undeniable) pitfalls of the other.\n</code></pre> <p>For the server I'm using satellite.</p> <pre><code>git clone https://git.sr.ht/~gsthnz/satellite\ncd satellite\ngo build\nmkdir -p /var/lib/satellite/certs\n</code></pre> <p>Create satellite.toml</p> <pre><code># Address to listen to requests (default: 0.0.0.0:1965)\n#listen = \"0.0.0.0\"\n\n[tls]\n# Directory to save certificates\ndirectory = \"/var/lib/satellite/certs\"\n\n# Multiple domains can be set with the [[domain]] section\n[[domain]]\nname = \"gemini.matrix\"\nroot = \"/srv/gemini/gemini.matrix\"\n</code></pre> <p>For the client I'm using bombadillo</p> <pre><code>git clone https://tildegit.org/sloum/bombadillo\ncd bombadillo\nsudo make install\nbombadillo\n</code></pre> <p>You will need to create a directory with some static files inside. These files should have a file extension of <code>.gmi</code> or <code>.gemini</code>. The content is structured like a subset of markdown:</p> <pre><code># Normal text\nHello World!\n\n# Link\n=&gt; gemini://example.org/ An Example Link\n\n# Preformatted text\n# ```\npreformatted text surrounded by 3 backticks\n# ```\n\n# Headers using #\n# Title\n## Sub Title\n### Sub Sub Title\n\n# Unordered list\n* No\n* Particular\n* Order\n\n# Quote lines\n&gt; This is a good quote\n</code></pre>"},{"location":"go-bazel/","title":"Building Go with Bazel","text":""},{"location":"go-bazel/#handling-go-dependencies","title":"Handling Go Dependencies","text":"<p>During development, you will often use <code>go get</code> to download libraries for import into the program which is useful for development but not so useful when building the finished product. Managing these dependencies over time is a hassle as they change frequently and can sometimes disappear entirely.</p> <p>The <code>dep</code> tool provides a way of automatically scanning your import statements and evaluating all of the dependencies. It create some files <code>Gopkg.toml</code> and <code>Gopkg.lock</code> which contain the location and latest Git SHA of your dependencies.</p> <p><code>dep</code> is installed via:</p> <pre><code>curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh\n</code></pre> <p>Run <code>dep init</code> to create the initial files, then as your develop run <code>dep ensure</code> to update dependencies to the latest version.</p> <p>The <code>dep</code> tool also downloads a copy of all dependencies into a <code>vendor</code> folder at the root of your project. This provides a backup in case a dependency disappears and provides the facility for reproducible builds.</p>"},{"location":"go-bazel/#bazel-gazelle","title":"Bazel / Gazelle","text":"<p>With our dependencies being updated, we would also need to update the WORKSPACE file so that Bazel/Gazelle knows about them as well. Gazelle requires the location and git commit hash in order to pull down the correct dependencies, but this is laborious to update manually.</p> <p>Thankfully, we can run a command to have gazelle pull in all of the dependencies from the <code>Gopkg.lock</code> file and update the WORKSPACE file automatically. Bazel will then pull in all of the dependencies correctly without any manual intervention.</p> <p><code>gazelle update-repos -from_file Gopkg.lock</code></p> <p>As part of ongoing development, you would periodically run</p> <p><code>dep ensure</code> </p> <p>followed by</p> <p><code>gazelle update-repos -from_file Gopkg.lock</code></p> <p>to keep all of the dependencies up to date and generate the new WORKSPACE file.</p>"},{"location":"go-bazel/#packaging-go-applications","title":"Packaging Go Applications","text":"<p>Now that we've built the go application and its dependencies we now need to package it up to distribute across the infrastructure.</p>"},{"location":"go-bazel/#packaging-with-fpm","title":"Packaging with fpm","text":"<p>The below command is an example of what we would want to run:</p> <p><code>fpm -s dir -t freebsd -n ~/go_test --version 1.0.0 --prefix /usr/local/bin go_tests</code></p> <p>But this has a few issues. Rather than putting the finished package into <code>~/go_test</code>, it would be better in a dedicated directory like <code>/var/packages</code> or similar. The version number is hard coded which obviously isn't always going to be correct. You would want to instead have your CI tool set to only run the packaging command when a new tag/release is created, and then have the version number derived from the tag/release number. It also includes the <code>--prefix</code> flag to specify the path to prepend to any files in the package. This is required as when the package is installed/extracted, the files will be extracted to the full path as specified in the package. So in this instance the <code>/usr/local/bin/go_tests</code> file is extracted.</p> <p>For now, I'm getting by with the following command which will overwrite the finished package if it already exists.</p> <p><code>fpm -f -s dir -t freebsd -n ~/go_test --prefix /usr/local/bin go_tests</code></p>"},{"location":"go-bazel/#building-go-programs-using-bazel","title":"Building Go programs using Bazel","text":"<p>Bazel is a build tool created by Google which operates similarly to their internal build tool, Blaze. It is primarily concerned with generating artifacts from compiled languages like C, C++, Go etc. </p> <p><code>pkg install -y bazel</code></p> <p>Bazel requires some files so that it knows what and where to build. As an example, we are going to compile a simple go program with no dependencies (literally print a single string to stdout).</p> <pre><code>// ~/go/src/github.com/omussell/go_tests/main.go\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"test\")\n}\n</code></pre> <p>A file called WORKSPACE should be created at the root of the directory. This is used by bazel to determine source code locations relative to the WORKSPACE file and differentiate other packages in the same directory. Then a BUILD.bazel file should also be created at the root of the directory. </p>"},{"location":"go-bazel/#gazelle","title":"Gazelle","text":"<p>Instead of creating BUILD files by hand, we can use the Gazelle tool to iterate over a go source tree and dynamically generate BUILD files. We can also let bazel itself run gazelle.</p> <p>Note that gazelle doesn't work without bash, and the gazelle.bash file has a hardcoded path to <code>/bin/bash</code> which of course is not available on FreeBSD by default.</p> <pre><code>pkg install -y bash\nln -s /usr/local/bin/bash /bin/bash\n</code></pre> <p>In the WORKSPACE file:</p> <pre><code>http_archive(\n    name = \"io_bazel_rules_go\",\n    url = \"https://github.com/bazelbuild/rules_go/releases/download/0.9.0/rules_go-0.9.0.tar.gz\",\n    sha256 = \"4d8d6244320dd751590f9100cf39fd7a4b75cd901e1f3ffdfd6f048328883695\",\n)\nhttp_archive(\n    name = \"bazel_gazelle\",\n    url = \"https://github.com/bazelbuild/bazel-gazelle/releases/download/0.9/bazel-gazelle-0.9.tar.gz\",\n    sha256 = \"0103991d994db55b3b5d7b06336f8ae355739635e0c2379dea16b8213ea5a223\",\n)\nload(\"@io_bazel_rules_go//go:def.bzl\", \"go_rules_dependencies\", \"go_register_toolchains\")\ngo_rules_dependencies()\ngo_register_toolchains(go_version=\"host\")\nload(\"@bazel_gazelle//:deps.bzl\", \"gazelle_dependencies\")\ngazelle_dependencies()\n</code></pre> <p>In the BUILD.bazel file:</p> <pre><code>load(\"@bazel_gazelle//:def.bzl\", \"gazelle\")\n\ngazelle(\n    name = \"gazelle\",\n    prefix = \"github.com/omussell/go_tests\",\n)\n</code></pre> <p>Then to run:</p> <pre><code>bazel run //:gazelle\nbazel build //:go_tests\n</code></pre> <p>A built binary should be output to the ~/.cache directory. Once a binary has been built once, Bazel will only build again if the source code changes. Otherwise, any subsequent runs just complete successfully extremely quickly.</p> <p>When attempting to use bazel in any capacity like <code>bazel run ...</code> or <code>bazel build ...</code> it would give the following error:</p> <pre><code>ERROR: /root/.cache/bazel/_bazel_root/...285a1776/external/io_bazel_rules_go/\nBUILD.bazel:7:1: every rule of type go_context_data implicitly depends upon the target '@go_sdk//\n:packages.txt', but this target could not be found because of: no such package '@go_sdk//': \nUnsupported operating system: freebsd\nERROR: /root/.cache/bazel/_bazel_root/...1776/external/io_bazel_rules_go/\nBUILD.bazel:7:1: every rule of type go_context_data implicitly depends upon the target '@go_sdk//\n:files', but this target could not be found because of: no such package '@go_sdk//': \nUnsupported operating system: freebsd\nERROR: /root/.cache/bazel/_bazel_root/...5a1776/external/io_bazel_rules_go/\nBUILD.bazel:7:1: every rule of type go_context_data implicitly depends upon the target '@go_sdk//\n:tools', but this target could not be found because of: no such package '@go_sdk//': \nUnsupported operating system: freebsd\nERROR: Analysis of target '//:gazelle' failed; build aborted: no such package '@go_sdk//': \nUnsupported operating system: freebsd\n</code></pre> <p>I think this is caused by bazel attempting to download and build go which isn't necessary as we've already installed via the package anyway. In the WORKSPACE file, change the <code>go_register_toolchains()</code> line to </p> <pre><code>go_register_toolchains(go_version=\"host\")\n</code></pre> <p>as documented at:</p> <pre><code>https://github.com/bazelbuild/rules_go/blob/master/go/toolchains.rst#using-the-installed-go-sdk.\n</code></pre> <p>This will force bazel to use the already installed go tools.</p>"},{"location":"go-bazel/#ci-with-buildbot","title":"CI with Buildbot","text":"<p>Example buildbot config:</p> <pre><code>factory.addStep(steps.Git(repourl='git://github.com/omussell/go_tests.git', mode='incremental'))\nfactory.addStep(steps.ShellCommand(command=[\"go\", \"fix\"],))\nfactory.addStep(steps.ShellCommand(command=[\"go\", \"vet\"],))\nfactory.addStep(steps.ShellCommand(command=[\"go\", \"fmt\"],))\nfactory.addStep(steps.ShellCommand(command=[\"bazel\", \"run\", \"//:gazelle\"],))\nfactory.addStep(steps.ShellCommand(command=[\"bazel\", \"build\", \"//:go_tests\"],))\n</code></pre> <p>I needed to rebuild the buildbot jail because it was borked, and after rebuilding it I was surprised that bazel worked without any more configuration. I just needed to install the git, go and bazel packages and run the buildbot config as described above and it ran through and rebuilt everything from scratch. This is one of the major advantages of keeping the build files (WORKSPACE and BUILD.bazel) alongside the source code. I am sure that if desired, anyone with a bazel setup would be able to build this code as well and the outputs would be identical.</p>"},{"location":"go-bazel/#adding-dependencies","title":"Adding dependencies","text":"<p>In order to have Bazel automatically build dependencies we need to make a some changes to the WORKSPACE file. I've extended the example program to pull in a library that generates fake data and prints a random name when invoked.</p> <pre><code>package main\n\nimport \"github.com/brianvoe/gofakeit\"\nimport \"fmt\"\n\nfunc main() {\n        gofakeit.Seed(0)\n        fmt.Println(gofakeit.Name())\n        //      fmt.Println(\"test\")\n}\n</code></pre> <p>The following needs to be appended to the WORKSPACE file:</p> <pre><code>load(\"@io_bazel_rules_go//go:def.bzl\", \"go_repository\")\n\ngo_repository(\n    name = \"com_github_brianvoe_gofakeit\",\n    importpath = \"github.com/brianvoe/gofakeit\",\n    commit = \"b0b2ecfdf447299dd6bcdef91001692fc349ce4c\",\n)\n</code></pre> <p>The go_repository rule is used when a dependency is required that does not have a BUILD.bzl file in their repo.</p>"},{"location":"go-bazel/#bazel-remote-cache","title":"Bazel Remote Cache","text":"<p>When building with Bazel, by default you are connecting to a local Bazel server which runs the build. If multiple people are running the same builds, you are all independently having to build the whole thing from scratch every time. </p> <p>With a Remote Cache, some other storage service can cache parts of the build and artifacts which can then be reused by multiple people.  This can be a plain HTTP server like NGINX or Google Cloud Storage.</p> <pre><code>mkdir -p /var/cache/nginx\nchmod 777 /var/cache/nginx\n\n# nginx config:\nlocation / {\n    root /var/cache/nginx;\n    dav_methods PUT;\n    create_full_put_path on;\n    client_max_body_size 1G;\n    allow all;\n}\n</code></pre> <p>Then when running the Bazel build, add the <code>--remote_cache=http://$ip:$port</code> flag to the build parameter like <code>bazel build --remote_cache=http://192.168.1.10:80 //...</code></p>"},{"location":"jail-creation/","title":"FreeBSD Jail Creation","text":"<p>Create a template dataset</p> <pre><code>zfs create -o mountpoint=/usr/local/jails zroot/jails\nzfs create -p zroot/jails/template\n</code></pre> <p>Download the base files into a new directory</p> <pre><code>mkdir ~/jails\nfetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/amd64/amd64/11.1-RELEASE/base.txz -o ~/jails\n</code></pre> <p>Extract the base files into the template directory (mountpoint)</p> <pre><code>tar -xf ~/jails/base.txz -C /usr/local/jails/template\n</code></pre> <p>Copy the resolv.conf file from host to template so that we have working DNS resolution</p> <pre><code>cp /etc/resolv.conf /usr/local/jails/template/etc/resolv.conf\n</code></pre> <p>When finished, take a snapshot. Anything after the '@' symbol is the snapshot name. You can make changes to the template at any time, just make sure that you take another snapshot when you are finished and that any subsequently created jails use the new snapshot.</p> <pre><code>zfs snapshot zroot/jails/template@1\n</code></pre> <p>New jails can then be created by cloning the snapshot of the template dataset</p> <pre><code>zfs clone zroot/jails/template@1 zroot/jails/testjail\n</code></pre> <p>Add the jails configuration to /etc/jail.conf</p> <pre><code># Global settings applied to all jails\n\ninterface = \"re0\";\nhost.hostname = \"$name\";\nip4.addr = 192.168.1.$ip;\npath = \"/usr/local/jails/$name\";\n\nexec.start = \"/bin/sh /etc/rc\";\nexec.stop = \"/bin/sh /etc/rc.shutdown\";\nexec.clean;\nmount.devfs;\n\n# Jail Definitions\ntestjail {\n    $ip = 15;\n}\n</code></pre> <p>Run the jail</p> <pre><code>jail -c testjail\n</code></pre> <p>View running jails</p> <pre><code>jls\n</code></pre> <p>Login to the jail</p> <pre><code>jexec testjail sh\n</code></pre>"},{"location":"lean/","title":"Lean","text":"<p>A collection of notes about the Toyota way, Lean, Theory of Constraints etc. copied from Wikipedia and other sites</p>"},{"location":"lean/#5s","title":"5S","text":"<pre><code>Seiri       Sort\nSeiton      Set in order\nSeso        Shine\nSeiketsu    Standardise\nShitsuke    Sustain\n</code></pre>"},{"location":"lean/#toyota-production-system","title":"Toyota Production System","text":""},{"location":"lean/#goal","title":"Goal","text":"<p>Design out overburden (muri) and inconsistency (mura), and to eliminate waste (muda).</p>"},{"location":"lean/#types-of-waste","title":"Types of waste","text":"<ul> <li>Overproduction</li> <li>Time on hand (waiting)</li> <li>Transportation</li> <li>Processing itself</li> <li>Excess inventory</li> <li>Movement</li> <li>Making defective products</li> <li>Underutilised workers</li> </ul>"},{"location":"lean/#muda","title":"Muda","text":"<p>Waste</p>"},{"location":"lean/#transport","title":"Transport","text":"<p>Moving products that are not actually required to perform processing</p>"},{"location":"lean/#inventory","title":"Inventory","text":"<p>All components, work in process and finished product not being processed</p>"},{"location":"lean/#motion","title":"Motion","text":"<p>People or equipment moving or walking more than is required to perform the processing</p>"},{"location":"lean/#waiting","title":"Waiting","text":"<p>Waiting for the next production step</p>"},{"location":"lean/#overproduction","title":"Overproduction","text":"<p>Production ahead of demand</p>"},{"location":"lean/#over-processing","title":"Over processing","text":"<p>Resulting from poor tool or product design creating activity</p>"},{"location":"lean/#defects","title":"Defects","text":"<p>The effort involved in inspecting for and fixing defects.</p>"},{"location":"lean/#mura","title":"Mura","text":"<p>Unevenness</p> <p>Is avoided through just in time systems which are based on keeping little or no inventory. These systems supply the production process with the right part, at the right time, in the right amount, using first-in, first-out (FIFO) component flow. JIT systems create a pull system in which each sub-process withdraws its needs from the preceding sub-processes, and ultimately from an outside supplier. When a preceding process does not receive a request or withdrawal it does not make more parts.</p>"},{"location":"lean/#muri","title":"Muri","text":"<p>Unreasonableness</p> <p>Muri can be avoided through standardized work. To achieve this a standard condition or output must be efined to assure effective judgment of quality.</p>"},{"location":"lean/#concept","title":"Concept","text":""},{"location":"lean/#just-in-time","title":"Just-in-time","text":"<p>Make only what is needed, only when it is needed and only in the amount that is needed</p>"},{"location":"lean/#jidoka","title":"Jidoka","text":"<p>Autonomation, automation with a human touch</p> <p>If an abnormal situation arises, the machine stops and the worker will stop the production line.</p> <p>Autonomation aims to prevent the production of defective products, eliminate overproduction and focus attention on understanding the problems.</p>"},{"location":"lean/#poka-yoke","title":"Poka Yoke","text":"<p>Any mechanism in a process that helps an equipment operator avoid (yokeru) mistakes (poka) and defects by preventing, correcting, or drawing attention to human errors as they occur</p>"},{"location":"lean/#continuous-improvement","title":"Continuous Improvement","text":"<p>We form a long term vision, then iteratively work towards it</p>"},{"location":"lean/#kaizen","title":"Kaizen","text":"<p>We improve our business operations continuously, always driving for innovation and evolution</p>"},{"location":"lean/#genchi-genbutsu","title":"Genchi Genbutsu","text":"<p>Go to the source to find the facts to make correct decisions</p>"},{"location":"lean/#heijunka","title":"Heijunka","text":"<p>Production levelling</p> <p>Reduce unevenness which in turn reduces waste.</p> <p>The goal is to produce intermediate goods at a constant rate so that further processing may also be carried out at a constant and predictable rate.</p>"},{"location":"lean/#the-right-process-will-produce-the-right-results","title":"The right process will produce the right results","text":"<ul> <li>Create continuous process flow to bring problems to the surface</li> <li>Use the 'pull' system to avoid overproduction (Kanban)</li> <li>Level out the workload (Heijunka)</li> <li>Build a culture of stopping to fix problems, to get quality right from the start</li> <li>Standardised tasks are the foundation for continuous improvement and employee empowerment</li> <li>Use visual control so no problems are hidden</li> <li>Use only reliable, thoroughly tested technology that serves your people and processes</li> </ul>"},{"location":"lean/#continuously-solving-root-problems-drives-organisational-learning","title":"Continuously solving root problems drives organisational learning","text":"<ul> <li>Go and see for yourself to thoroughly understand the situation (Genchi Genbutsu)</li> <li>Make decisions slowly by consensus, thoroughly considering all options. Implement decisions rapidly (Nemawashi)</li> <li>Become a learning organisation through relentless reflection (Hansei) and continuous improvement (Kaizen)</li> </ul>"},{"location":"lean/#andon","title":"Andon","text":"<p>The worker has the ability to stop production when a defect is found, and immediately call for assistance.</p> <p>Work is stopped until a solution is found.</p> <p>Stack light / Traffic light - Visual indicator of a machine state or process event</p>"},{"location":"lean/#nemawashi","title":"Nemawashi","text":"<p>An informal process of quietly laying the foundation for some proposed change or project, by talking to the people concerned, and gathering support and feedback. It is considered an important element in any major change, before any formal steps are taken and enables changes to be carried out with the consent of all sides.</p>"},{"location":"lean/#obeya","title":"Obeya","text":"<p>Large room</p> <p>During the product and process development, all individuals involved in managerial planning meet in a great room to speed communicationand decision making.</p>"},{"location":"lean/#takt-time","title":"Takt Time","text":"<p>Describes the required product assembly duration that is needed to match the demand. Often confused with cycle time, takt time is a tool used to design work and it measures the average time interval between the start of production of one unit and the start of production of the next unit when items are produced sequentially.</p>"},{"location":"lean/#theory-of-constraints","title":"Theory of Constraints","text":"<p>Organisations can be measured and controlled by variations on three measures:</p> <ul> <li>Inventory - The money that the system has invested in purchasing things which it intends to sell</li> <li>Operational expense - The money the system spends in order to turn inventory into throughput</li> <li>Throughput - The rate at which the system generates money through sales</li> </ul>"},{"location":"lean/#constraints","title":"Constraints","text":"<p>A constraint is anything that prevents the system from achieving its goal. There is always at least one, but at most only a few, at any given time.</p> <p>If a constraints throughput capacity is elevated to the point where it is no longer the systems limiting factor, this is said to 'break' the constraint.</p>"},{"location":"lean/#buffers","title":"Buffers","text":"<p>Buffers are placed before the governing constraint, thus ensuring that the constraint is never starved.</p> <p>Buffers are also placed behind the constraint to prevent downstream failure from blocking the constraints output.</p> <p>Buffers used in this way protect the constraint from variations in the rest of the system and should allow for normal variation of processing time and the occasional upset before and behind the constraint.</p> <p>With one constraint in the system, all other parts of the system must have sufficient capacity to keep up with the work at the constraint and to catch up if time was lost.</p>"},{"location":"lean/#plant-types-vati","title":"Plant Types (VATI)","text":"<p>The plant types specify the general flow of materials through a system</p>"},{"location":"lean/#v","title":"V","text":"<p>Flow is one to many, such as a plant that takes one raw material and can make many final products.</p> <p>The primary problem in V plants is 'robbing', where on operation (A) immediately after a diverging point 'steals' materials meant for the other operation (B). Once the material has been processed by A, it cannot come back and be run through B without significant rework.</p>"},{"location":"lean/#a","title":"A","text":"<p>Flow is many to one, such as where many sub-assemblies converge for a final assembly.</p> <p>The problem in A plants is synchronising the converging lines so that each supplies the final assembly point at the right time.</p>"},{"location":"lean/#t","title":"T","text":"<p>Flow is many to many, similar to I plant (or has multiple lines), which then splits into many assemblies.</p> <p>T plants suffer from both synchronisation problems of A plants and the robbing problems of V plants.</p>"},{"location":"lean/#i","title":"I","text":"<p>Material flows in a sequence of events in a straight line. The constraint is the slowest operation.</p>"},{"location":"lxc/","title":"LXC/LXD Containers","text":"<p>You should have either a blank disk or an existing zpool for storage. Run <code>lxd init</code>, answer the questions with Yes for the most part. Enter either the disk name like <code>/dev/sdb</code> or the zpool name <code>tank</code> when prompted.</p> <p>Once complete, you can start up an Alpine container with </p> <pre><code>lxc launch images:alpine/3.12 alpinecontainer\n</code></pre> <p>or a Ubuntu container with </p> <pre><code>lxc launch ubuntu:20.04 ubuntucontainer\n</code></pre> <p>You can then connect to the container with </p> <pre><code>lxc exec alpinecontainer -- /bin/sh\n</code></pre> <p>or </p> <pre><code>lxc exec ubuntucontainer -- /bin/bash\n</code></pre>"},{"location":"microserver/","title":"NAS on HP Microserver Gen8","text":""},{"location":"microserver/#hardware-specs","title":"Hardware specs","text":"<p>HP ProLiant G8 Microserver G1610T</p> <ul> <li>Intel Celeron G1610T (dual core 2.3 GHz)     </li> <li>16GB RAM           </li> <li>2 x 250GB SSD</li> <li>2 x 3TB HDD</li> </ul>"},{"location":"microserver/#summary","title":"Summary","text":"<p>I previously ran FreeNAS on this Microserver, but that was installed about 6 years ago so its very out of date. I want to use this as a NAS, but Im not too bothered about running a specific NAS OS like FreeNAS/TrueNAS etc. So my plan is to install Ubuntu 20.04 (current latest LTS) onto a USB disk, then have the disks set up in zpools with ZFS.</p>"},{"location":"microserver/#setup","title":"Setup","text":"<p>Whenever you search the internet for installing Ubuntu onto a USB disk it assumes you want to use it as a LiveCD from which to install Ubuntu onto the HDDs. I initially tried installing onto a USB stick by using two sticks, one for the initial boot which is placed in the USB jack inside the case, then another blank one inserted in the USB jack on the front of the case.</p> <p>However for whatever reason, the subsequent USB stick didnt boot. I think just a dodgy stick.</p> <p>So instead I did the same thing of booting from a USB stick inside the case, but then inserted a micro-sd card into the slot inside the case. I then selected that SD card as the disk to install to. </p> <p>In order to boot from this SD card, you need to press F9 during boot to enter the system setup. Then, I cant remember which specific option, but one of them has a list of options for booting from USB sticks which says like \"Boot from internal USB drive first\", \"Boot from internal SD card first\". You need to select the \"Boot from internal SD first\" option.</p> <p>Then continue boot, and it should boot correctly.</p>"},{"location":"microserver/#zfs","title":"ZFS","text":"<p>You need to install the <code>zfsutils-linux</code> package to manage zpools.</p> <p>I set up the disks so that the two SSDs were in one pool, just striped, no mirror. Then the two HDDs were in another pool, mirrored. This results in two zpools, one with 500GB and no redundancy plus one with 3TB and redundancy.</p> <pre><code># Amend device names as appropriate\n\n# SSD zpool\nzpool create SSD_storage /dev/sdb /dev/sdc\n\n# HDD zpool\nzpool create HDD_storage mirror /dev/sdd /dev/sde\n</code></pre>"},{"location":"new-prog-lang/","title":"Finding a New Programming Language","text":"<p>In my journey at work of learning how to program in Python, I've become increasingly annoyed by some of its behaviours.</p> <p>Initially creating a new project is difficult. Build a virtualenv, activate it, pip install the requirements. Each of these has their own problems. It also ends up making it hard to deploy projects because you need to do these steps wherever you want to use your project.</p> <p>The performance is poor because its interpreted. If you want to make things faster, you can link to C code, but then you have to write C code.</p> <p>The package ecosystem is a mess. Packages frequently break or change their dependencies.</p> <p>New updates happen frequently and bring backwards incompatible changes with them. So you end up with some packages that won't work on newer Python versions, and some packages that only work on new Python versions. You end up stuck in limbo.</p> <p>Popular libraries like requests and flask are mature but the way you use the libraries are similar to Python 2 era code. Newer libraries like FastAPI are nicer to use, but they arent stable and for some reason use async. So now every project has to decide between mature and old code, or immature and async code.</p> <p>Types hints are a pain. They arent evaluated unless you use a static checker like mypy or a validator like pydantic. You might put loads of effort into maintaining types but ultimately Python is dynamically typed and mypy will miss things so the types are wrong.</p> <p>Python seems to work fine for small or short lived projects. But if you want a project to last at least a couple of years, it ends up being painful.</p> <p>My requirements for a new language are:</p> <ul> <li>Compiled - Interpreted is inherently slower</li> <li>Statically typed - Dynamic typing is great but makes it harder in the long run</li> <li>Easy to install packages and pin their version</li> <li>Easy to deploy</li> <li>Good ecosystem - Web servers, ORM, Database connectors</li> </ul> <p>So far I have tried Go, Rust and even Ada. </p> <p>Somehow, I came across Nim, and it fits everything that I want. The syntax is similar enough to Python that it doesnt feel like a chore to learn like the other languages. It also compiles down to a single binary making deployment easy. It uses a C compiler so the resulting binary can be small and compile anywhere that supports C.</p> <p>The problem is, the ecosystem isnt there yet. Even basic things like, how to write tests, isnt documented clearly. I think it just needs some more popularity which would sort out the low hanging fruit.</p> <p>For now, I've decided to try Go. I dont like that its got policitical messages on the websites. I also dont like that they enforce CoC to permaban people.</p> <p>But I dont have much choice. Otherwise I'd have to use something like C# or Java, backed by Microsoft and Oracle, which are just as bad. At least Go compiles quickly.</p> <p>Go has a lot of stuff baked into the stdlib like DNS, HTTP, crypto, file operations etc. which I think should be a standard nowadays. You can get very far without ever having to import other packages. I tried Go while it was still young and endured the problems with importing dependencies.</p> <p>I set up Bazel with Gazelle before it was well documented. I used Hugo before it was popular and had lots of themes.</p> <p>So I think if I were starting a project now I would: - Use Go - Have Bazel set up on a remote server and use remote caching to perform quick builds during the develop/debug phase - Build the final app in Docker - Deploy to Kubernetes</p>"},{"location":"nginx-chacha20/","title":"Compiling NGINX with ChaCha20 support","text":"<p>Make a working directory</p> <pre><code>mkdir ~/nginx\ncd ~/nginx\n</code></pre> <p>Install some dependencies</p> <pre><code>pkg install -y ca_root_nss pcre perl5\n</code></pre> <p>Pull the source files</p> <pre><code>fetch https://nginx.org/download/nginx-1.13.0.tar.gz\nfetch https://www.openssl.org/source/openssl-1.1.0e.tar.gz\n</code></pre> <p>Extract the tarballs</p> <pre><code>tar -xzvf nginx-1.13.0.tar.gz\ntar -xzvf openssl-1.1.0e.tar.gz\nrm *.tar.gz\n</code></pre> <p>Compile openssl</p> <pre><code>cd ~/nginx/openssl-1.1.0e.tar.gz\n./config\nmake\nmake install\n</code></pre> <p>The compiled OpenSSL binary should be located in /usr/local/bin by default, unless the prefixdir variable has been set</p> <pre><code>/usr/local/bin/openssl version\n# Should output OpenSSL 1.1.0e\n</code></pre> <p>Compile NGINX</p> <pre><code>#!/bin/sh\ncd ~/nginx/nginx-1.13.0/\n#make clean\n\n./configure \\\n    --with-http_ssl_module \\\n    --with-http_gzip_static_module \\\n    --with-file-aio \\\n    --with-ld-opt=\"-L /usr/local/lib\" \\\n\n    --without-http_browser_module \\\n    --without-http_fastcgi_module \\\n    --without-http_geo_module \\\n    --without-http_map_module \\\n    --without-http_proxy_module \\\n    --without-http_memcached_module \\\n    --without-http_ssi_module \\\n    --without-http_userid_module \\\n    --without-http_split_clients_module \\\n    --without-http_uwsgi_module \\\n    --without-http_scgi_module \\\n    --without-http_limit_conn_module \\\n    --without-http_referer_module \\\n    --without-http_http-cache \\\n    --without_upstream_ip_hash_module \\\n    --without-mail_pop3_module \\\n    --without-mail-imap_module \\\n    --without-mail_smtp_module\n\n    --with-openssl=~/nginx/openssl-1.1.0e/\n\nmake\nmake install\n</code></pre> <p>After running the compile script, NGINX should be installed in /usr/local/nginx</p> <p>Start the service</p> <pre><code>/usr/local/nginx/sbin/nginx\n</code></pre> <p>If there are no issues, update the config file as appropriate in <code>/usr/local/nginx/conf/nginx.conf</code></p> <p>Reload NGINX to apply the new config</p> <pre><code>/usr/local/nginx/sbin/nginx -s reload\n</code></pre> <p>Generate a self-signed certificate</p> <p>Current NGINX config</p> <pre><code>worker_processes  1;\n\nevents {\n    worker_connections  1024;\n}\n\n\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n    sendfile        on;\n    keepalive_timeout  65;\n\n    server {\n        listen       80;\n        server_name  localhost;\n        location / {\n            root   /usr/local/www/;\n            index  index.html index.htm;\n        }\n\n        error_page   500 502 503 504  /50x.html;\n        location = /50x.html {\n            root   html;\n        }\n\n    }\n\n    server {\n        listen       443 ssl;\n        server_name  localhost;\n\n    ssl on;\n        #ssl_certificate      /root/nginx/server.pem;\n        #ssl_certificate_key  /root/nginx/private.pem;\n    ssl_certificate /usr/local/www/nginx-selfsigned.crt;\n    ssl_certificate_key /usr/local/www/nginx-selfsigned.key;\n    ssl_ciphers \"ECDHE-RSA-CHACHA20-POLY1305\";\n        ssl_prefer_server_ciphers  on;\n    ssl_protocols TLSv1.2;\n    ssl_ecdh_curve X25519;\n\n    location / {\n            root   /usr/local/www/;\n            index  index.html index.htm;\n        }\n    }\n\n}\n</code></pre>"},{"location":"nginx/","title":"NGINX","text":""},{"location":"nginx/#tls-13-0-rtt-with-nginx","title":"TLS 1.3 0-RTT with NGINX","text":"<p>NGINX Docs Early data var</p> <pre><code>ssl_early_data on;\nproxy_set_header Early-Data $ssl_early_data;\nlimit_except GET {\n    deny  all;\n}\n</code></pre> <p>0-RTT is vulnerable to replay attacks, so we should only use this with requests using the GET method. If passing the request to a backend, you can set a header with <code>proxy_set_header Early-Data $ssl_early_data;</code>. The value of the $ssl_early_data variable is \"1\" if early data is used, otherwise \"\". This header is passed to the upstream, so it can be used by the upstream application to determine the response.</p>"},{"location":"nginx/#only-allow-certain-http-methods-with-nginx","title":"Only allow certain HTTP methods with NGINX","text":"<p>NGNX Docs</p> <pre><code>limit_except GET {\n    deny  all;\n}\n</code></pre> <p>Only allows GET requests through, denies all other methods, with the exception of HEAD because if GET is allowed HEAD is too.</p>"},{"location":"nginx/#dynamic-certificate-loading-with-nginx","title":"Dynamic Certificate loading with NGINX","text":"<p>NGINX Announcement NGINX Docs</p> <p>If you have a lot of NGINX servers/vhosts all served from the same box, you probably want to secure them with TLS. Normally this would mean a lot of duplicate configuration to specify which certificate is needed for each server_name. With Dynamic Certificate Loading, you can use a NGINX variable as part of the certificate name. So if you have certificate/key files named after the server name, you can load them dynamically with NGINX.</p> <pre><code>server_name  omuss.net omuss-test.net;\n\nssl_certificate      /usr/local/etc/nginx/ssl/$ssl_server_name.crt;\nssl_certificate_key  /usr/local/etc/nginx/ssl/$ssl_server_name.key;\n</code></pre> <p>With certificate and key files named appropriately:</p> <pre><code>/usr/local/etc/nginx/ssl/omuss.net.crt\n/usr/local/etc/nginx/ssl/omuss.net.key\n/usr/local/etc/nginx/ssl/omuss-test.net.crt\n/usr/local/etc/nginx/ssl/omuss-test.net.key\n</code></pre> <p>Note that certificates are lazy loaded, as in they are only loaded when a request comes in. So all certificates aren't loaded into memory, which means less resource usage, but there is some overhead for the TLS negotiation because NGINX has to load the certificate from disk. TLS session caching may help alleviate this though.</p> <p>You would probably want the certificates stored on a fast disk to eliminate I/O overhead.</p>"},{"location":"nginx/#brotli-compression-with-nginx","title":"Brotli Compression with NGINX","text":"<p>Brotli can be used as an alternative to GZIP. It can give better compression in some cases.</p> <p>NGINX Brotli Docs Module Docs</p> <p>The normal <code>nginx</code> package does not include the brotli module. You can either compile NGINX yourself and include the Brotli module, or otherwise install the <code>nginx-full</code> package (though the package is big because of lots of dependencies and includes lots of other modules).</p> <p>Once you have a NGINX binary with the Brotli module included, you need to load the module in the NGINX configuration:</p> <pre><code>load_module /usr/local/libexec/nginx/ngx_http_brotli_static_module.so;\nload_module /usr/local/libexec/nginx/ngx_http_brotli_filter_module.so;\n</code></pre> <p>Also an important note, you MUST use HTTPS for Brotli to work. So make sure you set a server block to use HTTPS and set up a certificate etc.</p> <p>Now you have two options, compress you static files manually and put them where NGINX can find them, or let NGINX compress them on-the-fly. </p>"},{"location":"nginx/#static","title":"Static","text":"<p>With <code>brotli_static</code> set to <code>on</code> or <code>always</code>, the files must already be compressed. This can be done by installing the <code>brotli</code> package on FreeBSD, or otherwise you can do it quick and dirty with python like:</p> <pre><code># pip install brotli\n\nimport brotli\nwith open('index.html', 'rb') as f:\n    with open('index.html.br', 'wb') as brotted:\n        brotted.write(brotli.compress(f.read()))\n</code></pre> <p>Note that brotli prefers bytestrings.</p> <p>With the <code>brotli_static</code> option turned on, I found that using <code>index.html.br</code> didn't work, but if I set the filename to <code>index.html</code> but with Brotli-fied contents, it loaded correctly.</p> <p>You should also make sure to set <code>add_header Content-Encoding \"br\";</code> so that the browser knows that it is Brotli encoded.</p>"},{"location":"nginx/#dynamic","title":"Dynamic","text":"<p>Otherwise, set <code>brotli on;</code> and it will compress file on-the-fly.</p>"},{"location":"nginx/#nginx-tcpudp-proxy","title":"NGINX TCP/UDP proxy","text":"<p>NGINX needs to be compiled with the --with-stream option. It can't be dynamic, which is the default. In the config file you need to add:</p> <pre><code>load_module /usr/local/libexec/nginx/ngx_stream_module.so;\n</code></pre> <p>Then in the config file:</p> <pre><code>stream {\n\n  server {\n\n    listen 80;\n    proxy_pass 192.168.1.15:80;\n\n  }\n\n  server {\n\n    # Override the default stream type of TCP with UDP\n    listen 53;\n    proxy_pass 192.168.1.15:53 udp;\n\n  }\n\n}\n</code></pre>"},{"location":"nsd-unbound/","title":"NSD and Unbound config","text":"<p>Set up the unbound/nsd-control</p> <pre><code>local-unbound-setup\nnsd-control-setup\n</code></pre> <p>Enable NSD and Unbound to start in <code>/etc/rc.conf</code></p> <pre><code>sysrc nsd_enable=\"YES\"\nsysrc local_unbound_enable=\"YES\"\n</code></pre> <p>Set a different listening port for NSD in <code>/usr/local/etc/nsd.conf</code></p> <pre><code>server:\n  port: 5353\n</code></pre> <p>Create an inital zone file <code>/usr/local/etc/nsd/home.lan.zone</code></p> <pre><code>$ORIGIN home.lan. ;\n$TTL 86400 ;\n\n@ IN SOA ns1.home.lan. admin.home.lan. (\n        2017080619 ;\n        28800 ;\n        7200 ;\n        864000 ;\n        86400 ;\n        )\n\n        NS ns1.home.lan.\n\nns1 IN A 192.168.1.15\njail IN A 192.168.1.15\n</code></pre> <p>Create the reverse lookup zone file <code>/usr/local/etc/nsd/home.lan.reverse</code></p> <pre><code>$ORIGIN home.lan.\n$TTL 86400\n\n0.1.168.192.in-addr.arpa. IN SOA ns1.home.lan. admin.home.lan. (\n        2017080619\n        28800\n        7200\n        864000\n        86400\n        )\n\n        NS ns1.home.lan.\n\n15.1.168.192.in-addr.arpa. IN PTR jail\n15.1.168.192.in-addr.arpa. IN PTR ns1\n</code></pre>"},{"location":"nsd-unbound/#opendnssec","title":"OpenDNSSEC","text":"<p>Install the required packages</p> <pre><code>pkg install -y opendnssec softhsm\n</code></pre> <p>Set the softhsm database location in <code>/usr/local/etc/softhsm.conf</code></p> <pre><code>0:/var/lib/softhsm/slot0.db\n</code></pre> <p>Initialise the token database:</p> <pre><code>softhsm --init-token --slot 0 --label \"OpenDNSSEC\"\nEnter the PIN for the SO and then the USER.\n</code></pre> <p>Make sure opendnssec has permission to access the token database</p> <pre><code>chown opendnssec /var/lib/softhsm/slot0.db\nchgrp opendnssec /var/lib/softhsm/slot0.db\n</code></pre> <p>Set some options for OpenDNSSEC in <code>/usr/local/etc/opendnssec/conf.xml</code></p> <pre><code>&lt;Repository name=\"SoftHSM\"&gt;\n        &lt;Module&gt;/usr/local/lib/softhsm/libsofthsm.so&lt;/Module&gt;\n        &lt;TokenLabel&gt;OpenDNSSEC&lt;/TokenLabel&gt;\n        &lt;PIN&gt;1234&lt;/PIN&gt;\n        &lt;SkipPublicKey/&gt;\n&lt;/Repository&gt;\n</code></pre> <p>Edit <code>/usr/local/etc/opendnssec/kasp.xml</code>. Change unixtime to datecounter in the Serial parameter. This allows us to use YYYYMMDDXX format for the SOA SERIAL values.</p> <pre><code>&lt;Zone&gt;\n        &lt;PropagationDelay&gt;PT300S&lt;/PropagationDelay&gt;\n        &lt;SOA&gt;\n                &lt;TTL&gt;PT300S&lt;/TTL&gt;\n                &lt;Minimum&gt;PT300S&lt;/Minimum&gt;\n                &lt;Serial&gt;datecounter&lt;/Serial&gt;\n        &lt;/SOA&gt;\n&lt;/Zone&gt;\n</code></pre>"},{"location":"pgsql-repl/","title":"PostgreSQL 10.1 with replication","text":"<pre><code>pkg install -y postgresql10-server postgresql10-client\nsysrc postgresql_enable=YES\nservice postgresql initdb\nservice postgresql start\n</code></pre>"},{"location":"pgsql-repl/#postgresql-101-scram-authentication","title":"PostgreSQL 10.1 SCRAM Authentication","text":"<pre><code>su - postgres\npsql\nset password_encryption = 'scram-sha-256';\ncreate role app_db with password 'foo';\nselect substring(rolpassword, 1, 14) from pg_authid where rolname = 'app_db';\n</code></pre>"},{"location":"pgsql-repl/#postgresql-101-using-repmgr-for-database-replication-wal-g-for-wal-archiving-and-minio-for-s3-compatible-storage","title":"PostgreSQL 10.1 using repmgr for database replication, WAL-G for WAL archiving, and minio for S3 compatible storage","text":"<p>For this, I created two bhyve VMs to host postgresql and a jail on the host for minio</p> <p>Make sure postgresql is running</p> <p>Carry out the following steps on both primary and replicas</p> <p>The current packaged version of repmgr is 3.3.1 which isn't the latest. The latest is 4.0.1, so we need to compile it ourself, and put files into the correct locations</p> <pre><code>fetch https://repmgr.org/download/repmgr-4.0.1.tar.gz\ntar -zvxf repmgr-4.0.1.tar.gz\n./configure\npkg install -y gmake\ngmake\n</code></pre> <p>Copy the repmgr files to their correct locations</p> <pre><code>cp -v repmgr /var/db/postgres\ncp -v repmgr--4.0.sql /usr/local/share/postgresql/extension/\ncp -v repmgr.control /usr/local/share/postgresql/extension\n</code></pre> <pre><code>vim /var/db/postgrs/data10/postgresql.conf \n</code></pre> <p>Add lines: </p> <pre><code>include_dir = 'postgresql.conf.d'\nlisten_addresses = '\\*'\n</code></pre> <pre><code>vim /var/db/postgres/data10/postgresql.conf.d/postgresql.replication.conf\n</code></pre> <p>Add lines:</p> <pre><code>max_wal_senders = 10\nwal_level = 'replica'\nwal_keep_segments = 5000\nhot_standby = on\narchive_mode = on\narchive_command = 'wal-g stuff here'\n</code></pre> <p>vim /var/db/postgres/data10/pg_hba.conf</p> <p>Add lines: Please note, for testing purposes, these rules are wide open and allow everything. Dont do this in production, use a specific role with a password and restrict to a specific address</p> <pre><code>local   all     all         trust\nhost    all     all 0.0.0.0/0   trust\nhost    replication all 0.0.0.0/0   trust\n</code></pre> <p>vim /usr/local/etc/repmgr.conf</p> <p>Add lines:</p> <pre><code>node_id=1 # arbitrary number, each node needs to be unique\nnode_name=postgres-db1 # this nodes hostname\nconninfo='host=192.168.1.10 user=repmgr dbname=repmgr' # the host value should be a hostname if DNS is working\n</code></pre> <p>On the primary</p> <pre><code>su - postgres\ncreateuser -s repmgr\ncreatedb repmgr -O repmgr\n\nrepmgr -f /usr/local/etc/repmgr.conf primary register\nrepmgr -f /usr/local/etc/repmgr.conf cluster show\n</code></pre> <p>On a standby</p> <pre><code>su - postgres\npsql 'host=node1 user=repmgr dbname=repmgr'\n</code></pre> <p>To clone the primary, the data directory on the standby node must exist but be empty</p> <pre><code>rm -rf /var/db/postgres/data10/\nmkdir -p /var/db/postgres/data10\nchown postgres:postgres /var/db/postgres/data10\n</code></pre> <p>Dry run first to check for problems</p> <p><code>repmgr -h node1 -U repmgr -d repmgr -f /usr/local/etc/repmgr.conf standby clone --dry-run</code></p> <p>If its ok, run it</p> <p><code>repmgr -h node1 -U repmgr -d repmgr -f /usr/local/etc/repmgr.conf standby clone</code></p> <p>On the primary</p> <pre><code>su - postgres\npsql -d repmgr\nselect * from pg_stat_replication;\n</code></pre> <p>On the standby</p> <pre><code>repmgr -f /usr/local/etc/repmgr.conf standby register\nrepmgr -f /usr/local/etc/repmgr.conf cluster show\n</code></pre> <p>Install minio</p> <pre><code>pkg install -y minio\nsysrc minio_enable=YES\nsysrc minio_disks=/home/user/test\nmkdir -p /home/user/test\nchown minio:minio /home/user/test\nservice minio start\n# The access keys are in /usr/local/etc/minio/config.json\n# You can change them in this file and restart the service to take effect\n</code></pre> <p>On the primary WAL-G</p> <pre><code>pkg install -y go\nmkdir -p /root/go\nsetenv GOPATH /root/go\ncd go\ngo get github.com/wal-g/wal-g\ncd src/github.com/wal-g/wal-g\nmake all\nmake install\ncp /root/go/bin/wal-g /usr/local/bin\n</code></pre> <p>WAL-G requires certain environment variables to be set. This can be done using envdir, part of the daemontools package</p> <p>pkg install -y daemontools</p> <p>Setup is now complete. </p> <p>For operations, a base backup needs to be taken on a regular basis probably via a cron job, running the following command as postgres user</p> <p><code>wal-g backup-push /var/db/postgres/data10</code></p> <p>Then the archive_command in the postgresql.replication.conf should be set to the wal-push command</p> <p><code>wal-g wal-push /var/db/postgres/data10</code></p> <p>To restore, backup-fetch and wal-fetch can be used to pull the latest base backup and the necessary wal logs to recover to the latest transaction</p>"},{"location":"pi-bitwarden/","title":"Running bitwarden_rs on a Raspberry Pi 4","text":""},{"location":"pi-bitwarden/#summary","title":"Summary","text":"<p>We will be setting up bitwarden_rs without Docker, by compiling it manually and then running as a service. In this example we are using SQLite, but you can change this to MySQL or PostgreSQL if you prefer.</p>"},{"location":"pi-bitwarden/#install-dependencies","title":"Install dependencies","text":"<pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh      # Answer Y when prompted\napt install -y build-essential git pkg-config libssl-dev libsqlite3-dev\n</code></pre>"},{"location":"pi-bitwarden/#clone-the-repo","title":"Clone the repo","text":"<pre><code>git clone https://github.com/dani-garcia/bitwarden_rs.git\ncd bitwarden_rs\n</code></pre>"},{"location":"pi-bitwarden/#compile","title":"Compile","text":"<pre><code>cargo build --features sqlite --release\n</code></pre>"},{"location":"pi-bitwarden/#admin","title":"Admin","text":"<p>After compilation, the built binary will be <code>./target/release/bitwarden_rs</code>. This should be moved to <code>/usr/bin</code> with <code>mv ./target/release/bitwarden_rs /usr/bin/bitwarden_rs</code></p> <p>The data directory needs to be created with <code>mkdir -p /var/lib/bitwarden_rs/data</code>. This is where the bitwarden keys and database are stored.</p> <p>Create a user account with <code>adduser bitwarden_rs</code>. Make sure the ownership of everything in <code>/var/lib/bitwarden_rs</code> is set to the <code>bitwarden_rs</code> user.</p>"},{"location":"pi-bitwarden/#frontend","title":"Frontend","text":"<p>Download the already built assets: </p> <pre><code>cd /var/lib/bitwarden_rs\n# Amend the version as appropriate\nwget https://github.com/dani-garcia/bw_web_builds/releases/download/v2.17.1/bw_web_v2.17.1.tar.gz\n</code></pre> <p>Extract them</p> <pre><code>tar -xvf bw_web_v2.17.1.tar.gz\n</code></pre>"},{"location":"pi-bitwarden/#run","title":"Run","text":"<p>Create the systemd service file. Copy the file from the wiki. </p>"},{"location":"puppet-bolt/","title":"Puppet Bolt","text":"<p>Install Bolt with:</p> <pre><code>wget https://apt.puppet.com/puppet-tools-release-focal.deb\nsudo dpkg -i puppet-tools-release-focal.deb\nsudo apt-get update \nsudo apt-get install puppet-bolt\n</code></pre> <p>If you want to create modules but dont want to publish them to the Puppet Forge, you can just add the directory to the module path.</p>"},{"location":"puppet-jails-deploy/","title":"Blue/Green Deployments with Puppet, NGINX and FreeBSD Jails","text":"<p>At $WORK, we were going through a period of rapid growth and were planning on creating many more web apps. The apps are mainly Django/Python with a few NodeJS apps too. Previously they had been deployed on Debian servers and the deployment process was to SSH onto each app server and run git pull, then restart the processes.</p> <p>This solution works fine for apps with little traffic, but since we were growing rapidly, we were finding that this architecture wouldnt scale as much as we would like.</p> <p>At the time we were deciding on a new architecture (2017), the current modern solutions like Docker, Kubernetes and LXC/LXD were still very much in their infancy and not ready for production.</p> <p>In addition, one of the main gripes with Python apps is that they frequently link to C libraries. So when a new deployment contains a package updated via pip, what can happen is that it requires a new version of an OS package for a C library. Now you not only need to deploy the app, you also need to update the OS too.</p>"},{"location":"puppet-jails-deploy/#proposed-architecture","title":"Proposed Architecture","text":"<p>The solution we came up with was to deploy the apps inside FreeBSD jails, control the deployments via Puppet, and handle routing of requests from the load balancer to the correct jail using NGINX.</p>"},{"location":"puppet-jails-deploy/#freebsd-jails","title":"FreeBSD Jails","text":"<p>Jails work in a very similar way to LXC/LXD or like Docker but without immutable images. They are containers which share the host kernel but have their own copy of the base OS which runs isolated processes. Each jail can be allocated its own IP address and RW filesystem.</p> <p>To create a new jail, a filesystem containing the FreeBSD base system is created, the jail.conf files created and then the jail started. A jail runs the same init process tree as it would on bare metal or a VM. Once a jail is running, you can jexec inside the jail to run further commands.</p>"},{"location":"puppet-jails-deploy/#puppet","title":"Puppet","text":"<p>Puppet is a configuration management system. It lets you decide the desired state for a system and it will idempotently change the system to achieve the desired state.</p> <p>For example, if you wanted to install the NGINX package, make sure the config file was set up correctly and then start the service, you could write Puppet code like:</p> <pre><code>package { 'nginx':\n  ensure =&gt; 'installed',\n}\n\nfile { '/etc/nginx/nginx.conf':\n  ensure  =&gt; 'file',\n  content =&gt; template('profiles/nginx/nginx.conf.erb'),\n}\n\nservice { 'nginx':\n  ensure =&gt; 'running',\n  enable =&gt; true,\n}\n</code></pre> <p>That code is very generic and Puppet will work across multiple OSes and filesystems. You dont have to worry about OS specific actions, Puppet just does it all for you.</p>"},{"location":"puppet-jails-deploy/#nginx","title":"NGINX","text":"<p>With NGINX we can use it both to serve static assets like a web server and as a reverse proxy to the applications. The config can be set to route requests to specific domains or URLs to specific backends. In this case, the backends would be the IP addresses of the jails.</p>"},{"location":"puppet-jails-deploy/#putting-it-all-together","title":"Putting it all together","text":"<p>I created a basic Puppet type and provider which can check for the presence or absence of jails. This was used in a module which performed the actions to set up the jails.</p> <p>The jail filesystem would run on ZFS which meant we could create a template dataset and then new jails were just clones of that dataset.</p> <p>When Puppet ran on the host system, it would perform the following actions in order to create a new jail:</p> <ul> <li>Download the FreeBSD base files tar.xz</li> <li>Create the template ZFS dataset and extract the base files into it</li> <li>Create some standard files like resolv.conf and install standard packages like Git and Puppet</li> <li>Create a ZFS snapshot of the template dataset</li> <li>Clone the template snapshot for the new jail</li> <li>Amend the jail.conf to include config for the new jail</li> <li>Start the new jail</li> <li>Run Puppet inside the jail to provision the app</li> <li>Subsequent Puppet runs on the host would amend the NGINX config to route traffic to the new jails IP address.</li> </ul> <p>When creating a new jail, we designated it as the \"test\" jail which had a specific domain or URL in the NGINX config. So all live traffic would continue to be served by the \"live\" jail, but any requests to a specific \"test\" domain/URL would instead be routed to the \"test\" jail. This meant that the QA department could run their tests in the \"test\" jail without it affect the \"live\" jail.</p> <p>The switchover was accomplished by changing the NGINX config and running nginx -s reload which gracefully reloads the NGINX processes with the new config without dropping any in flight requests.</p>"},{"location":"puppet-jails-deploy/#end-result","title":"End Result","text":"<p>This setup was running fine for years and allowed us to scale well. I wouldn't recommend going this route for future deployments. We spent a lot of time fighting FreeBSD because the support for third-party packages was so poor. We frequently had to reinvent the wheel to get things working.</p> <p>My recommendations would be:</p> <ul> <li>If you want to self-host and are a sufficiently large organisation, use Kubernetes.</li> <li>If you want to self-host and arent that big, use LXD and pair it with Ansbile or Saltstack.</li> <li>If you dont want to self-host, use something like Heroku or Fly.io</li> </ul>"},{"location":"python/","title":"Modern Python","text":""},{"location":"python/#summary","title":"Summary","text":"<p>The following tools and libraries are known to work well with modern python. They are modular, so you can pick and choose the components you want based on your need.</p> <p>A minimal project might have:</p> <ul> <li>Dependencies managed with <code>uv</code></li> <li>Formatting with <code>ruff</code></li> <li>Typing/validation with <code>attrs</code></li> <li>HTTP server with <code>litestar</code> running with <code>uvicorn</code></li> <li>HTTP requests with <code>httpx</code></li> <li>Documentation with <code>mkdocs</code></li> <li>Tests with <code>pytest</code></li> </ul>"},{"location":"python/#http","title":"HTTP","text":""},{"location":"python/#client","title":"Client","text":"<ul> <li>HTTPX</li> </ul>"},{"location":"python/#server","title":"Server","text":"<ul> <li>Litestar</li> <li>Uvicorn</li> </ul>"},{"location":"python/#databases","title":"Databases","text":"<ul> <li>Advanced Alchemy</li> <li>Alembic</li> </ul>"},{"location":"python/#async-task-processes","title":"Async Task Processes","text":"<ul> <li>Celery</li> </ul>"},{"location":"python/#cryptography","title":"Cryptography","text":"<ul> <li>pynacl</li> <li>secrets</li> </ul>"},{"location":"python/#dev-tools","title":"Dev Tools","text":""},{"location":"python/#project-management","title":"Project management","text":"<ul> <li>uv</li> </ul>"},{"location":"python/#formattingstyling","title":"Formatting/Styling","text":"<ul> <li>ruff</li> </ul>"},{"location":"python/#typing-and-validation","title":"Typing and Validation","text":"<ul> <li>attrs</li> <li> <p>cattrs</p> </li> <li> <p>Pydantic</p> </li> <li>Mypy</li> </ul>"},{"location":"python/#logging","title":"Logging","text":"<ul> <li>structlog</li> </ul>"},{"location":"python/#tests","title":"Tests","text":"<ul> <li>pytest</li> <li>playwright</li> </ul>"},{"location":"python/#documentation","title":"Documentation","text":"<ul> <li>mkdocs</li> <li>mkdocs-material</li> <li>mkdocstrings</li> </ul>"},{"location":"python/#code-complexity","title":"Code Complexity","text":"<ul> <li>lizard</li> <li>radon</li> </ul>"},{"location":"python/#lizard","title":"Lizard","text":"<p><code>lizard -x'*/tests/*' -l python -w src</code></p>"},{"location":"python/#radon","title":"Radon","text":"<pre><code>radon cc --min B --average --total-average src\nradon mi --min B src\n</code></pre>"},{"location":"quick-k8s/","title":"Quick Multi-Node Kubernetes Cluster","text":""},{"location":"quick-k8s/#multipass","title":"Multipass","text":"<p>Multipass lets you easily spin up Ubuntu VMs on a workstation. </p> <pre><code># Install\nsnap install multipass --classic\n</code></pre> <p>Then to create a new instance, just run <code>multipass launch</code>. It will create a new instance based on an Ubuntu LTS image. </p> <p>To access the instance, just run <code>multipass shell $name</code>. You then have full access to the instance. </p> <p>The instances can also be bootstrapped via cloud-init in the same way that instances on cloud providers are.</p>"},{"location":"quick-k8s/#microk8s","title":"Microk8s","text":"<p>Microk8s is a small Kubernetes distribution designed for appliances. </p> <pre><code># Install\nsudo snap install microk8s --classic --channel=1.16/stable\nsudo usermod -a -G microk8s $USER\nsu - $USER\n</code></pre>"},{"location":"quick-k8s/#cluster","title":"Cluster","text":"<p>So with two Multipass instances launched, and Microk8s installed on each, we can now join them together to form a cluster by running <code>microk8s.add-node</code> on the proposed master and then the requisite <code>microk8s.join</code> command on the other node. </p>"},{"location":"rqlite/","title":"RQLite","text":"<p>SQLite, distributed over many nodes with consensus achieved with the Raft protocol.</p> <pre><code>go get github.com/rqlite/rqlite\ncd ~/go/src/github.com/rqlite/rqlite/cmd/rqlite\ngo get -t -d -v ./...\ngo build\n# You now have the rqlite binary\ncd ~/go/src/github.com/rqlite/rqlite/cmd/rqlited\ngo build\n# You now have the rqlited binary\n</code></pre> <p>Set up the first cluster node:</p> <pre><code>./rqlited ~/node.1\n</code></pre> <p>Then subsequent cluster nodes:</p> <pre><code>rqlited -http-addr localhost:4003 -raft-addr localhost:4004 -join http://localhost:4001 ~/node.2\n</code></pre> <p>Presumably you'd have the HTTP address and Raft address to be the same port on different servers, and you'd join to the same master node.</p>"},{"location":"rrr/","title":"Robots, Reactors and Rockets","text":""},{"location":"saltstack/","title":"Saltstack install and config","text":"<p>Install the salt package</p> <pre><code>pkg install -y py36-salt\n</code></pre> <p>Copy the sample files to create the master and/or minion configuration files</p> <pre><code>cp -v /usr/local/etc/salt/master{.sample,\"\"}\ncp -v /usr/local/etc/salt/minion{.sample,\"\"}\n</code></pre> <p>Set the master/minion services to start on boot</p> <pre><code>sysrc salt_master_enable=\"YES\"\nsysrc salt_minion_enable=\"YES\"\n</code></pre> <p>Salt expects state files to exist in the /srv/salt or /etc/salt directories which don't exist by default on FreeBSD so make symlinks instead:</p> <pre><code>ln -s /usr/local/etc/salt /etc/salt\nln -s /usr/local/etc/salt /srv/salt\n</code></pre> <p>Start the services</p> <pre><code>service salt_master onestart\nservice salt_minion onestart\n</code></pre> <p>Accept minion keys sent to the master</p> <pre><code>salt-key -A\n# Press y to accept\n</code></pre> <p>Create a test state file</p> <pre><code>vi /usr/local/etc/salt/states/examples.sls\n</code></pre> <pre><code>---\n\ninstall_packages:\n  pkg.installed:\n    - pkgs:\n      - vim-lite\n</code></pre> <p>Then apply the examples state</p> <pre><code>salt '*' state.apply examples\n</code></pre>"},{"location":"saltstack/#salt-formulas","title":"Salt Formulas","text":"<p>Install the GitFS backend, this allows you to serve files from git repos.</p> <pre><code>pkg install -y git py36-gitpython\n</code></pre> <p>Edit the <code>/usr/local/etc/salt/master</code> configuration file:</p> <pre><code>fileserver_backend:\n  - git\n  - roots\ngitfs_remotes:\n  - https://github.com/saltstack-formulas/lynis-formula\n</code></pre> <p>Restart the master. If master and minion are the same node, restart the minion service as well.</p> <pre><code>service salt_master onerestart\n</code></pre> <p>The formulas can then be used in the state file</p> <pre><code>include:\n  - lynis\n</code></pre>"},{"location":"saltstack/#salt-equivalent-to-r10k-and-using-git-as-a-pillar-source","title":"Salt equivalent to R10K and using git as a pillar source","text":"<p>If the git server is also a minion, you can use Reactor to signal to the master to update the fileserver on each git push:</p> <pre><code>https://docs.saltstack.com/en/latest/topics/tutorials/gitfs.html#refreshing-gitfs-upon-push\n</code></pre> <p>You can also use git as a pillar source (host your specific config data in version control)</p> <pre><code>https://docs.saltstack.com/en/latest/topics/tutorials/gitfs.html#using-git-as-an-external-pillar-source\n</code></pre>"},{"location":"saltstack/#installing-raet","title":"Installing RAET","text":"<p>RAET support isn't enabled in the default package. If you install py27-salt and run <code>pkg info py27-salt</code> you can see in the options <code>RAET: off</code>. In order to use RAET, you need to build the py27-salt port.</p> <p>Compile the port</p> <pre><code>pkg remove -y py27-salt\nportsnap fetch extract\ncd /usr/ports/sysutil/py-salt\nmake config\n# Press space to select RAET\nmake install\n</code></pre> <p>Edit <code>/srv/salt/master</code> and <code>/srv/salt/minion</code> and add</p> <pre><code>transport: raet\n</code></pre> <p>Then restart the services</p> <pre><code>service salt_master restart\nservice salt_minion restart\n</code></pre> <p>You will need to accept keys again</p> <pre><code>salt-key \nsalt-key -A\n</code></pre>"},{"location":"saltstack/#salt-equivalent-of-hiera-eyaml","title":"Salt equivalent of hiera-eyaml","text":"<p>Salt.runners.nacl</p> <p>Similar to hiera-eyaml, it is used for encrypting data stored in pillar:</p> <pre><code>https://docs.saltstack.com/en/latest/ref/runners/all/salt.runners.nacl.html\n</code></pre>"},{"location":"serverless/","title":"Serverless with Knative running in gVisor sandbox on Minikube","text":"<ul> <li>Minikube - A Kubernetes distribution which starts a single-node cluster</li> <li>gVisor - A user-space kernel, written in Go, that implements a substantial portion of the Linux system call interface.</li> <li>Knative - Run serverless services on Kubernetes</li> </ul> <p>Install Minikube as described in the documentation.</p> <p>Install gVisor as per the docs:</p> <pre><code>minikube start --container-runtime=containerd  \\\n    --docker-opt containerd=/var/run/containerd/containerd.sock\nminikube addons enable gvisor\nkubectl get pod,runtimeclass gvisor -n kube-system\n</code></pre>"},{"location":"signify/","title":"Signify","text":"<p>Sign and verify files</p> <p>Generate keys without password (remove -n flag to ask for a password)</p> <pre><code>signify-openbsd -G -p keyname.pub -s keyname.sec -n\n</code></pre> <p>Sign a file</p> <pre><code>signify-openbsd -S -s keyname.sec -m $file_to_sign -x $signature_file\n</code></pre> <p>Verify a file</p> <pre><code>signify-openbsd -V -p keyname.pub -m $file_to_verify -x $signature_file\n</code></pre>"},{"location":"homelab/","title":"Homelab","text":"<p>The homelab has evolved over time as I tried learning new things to either support my current work or build towards finding new work.</p> <p>It started out with basic Linux like Debian/Ubuntu before moving to FreeBSD. That meant learning about managing services using Jails and storing data on disks with ZFS. That was then used at my next workplace.</p> <p>After that it was setting up a hypervisor for running VMs and spinning up Kubernetes clusters. Running different CI systems for building images and deploying containers.</p> <p>Its currently defunct and my intention is to have the hardware set up properly either in a small rack or some other kind of furniture as a more permanent solution. Its mostly been a pile of computers on the floor. I've always managed the networking by using an 8 port dumb switch. Now there will be a router, switches and fiber networking.</p> <p>In addition to the networking there is also a bunch of raspberry pis which will be running all the time since they have low power usage. They will be for running services like DNS, web servers etc. when it would be preferred for them to always be accessible. Then for any heavier compute or storage testing there are the beefier intel based HP microservers which can run VMs and have lots of disk space and will be turned on as needed.</p>"},{"location":"homelab/hardware/","title":"Hardware","text":"<p>The homelab has evolved over time as I tried learning new things to either support my current work or build towards finding new work.</p> <p>It started out with basic Linux like Debian/Ubuntu before moving to FreeBSD. That meant learning about managing services using Jails and storing data on disks with ZFS. That was then used at my next workplace.</p> <p>After that it was setting up a hypervisor for running VMs and spinning up Kubernetes clusters. Running different CI systems for building images and deploying containers.</p> <p>Its currently defunct and my intention is to have the hardware set up properly either in a small rack or some other kind of furniture as a more permanent solution. Its mostly been a pile of computers on the floor. I've always managed the networking by using an 8 port dumb switch. Now there will be a router, switches and fiber networking.</p> <p>In addition to the networking there is also a bunch of raspberry pis which will be running all the time since they have low power usage. They will be for running services like DNS, web servers etc. when it would be preferred for them to always be accessible. Then for any heavier compute or storage testing there are the beefier intel based HP microservers which can run VMs and have lots of disk space and will be turned on as needed.</p>"},{"location":"homelab/hardware/#gigabyte-brix-pro-gb-bxi7-4770r","title":"Gigabyte Brix Pro GB-BXI7-4770R","text":"<ul> <li>Intel Core i7-4770R (quad core 3.2GHz)</li> <li>16GB RAM</li> <li>250GB mSATA SSD</li> <li>250GB 2.5 inch SSD</li> </ul>"},{"location":"homelab/hardware/#hp-proliantgen10-plus-microserver","title":"HP ProLiantGen10 Plus Microserver","text":"<ul> <li>Intel Xeon E-2224 4 Core Processor</li> <li>16GB RAM</li> <li>2 x 250GB SSD</li> </ul>"},{"location":"homelab/hardware/#hp-proliant-g8-microserver-g1610t","title":"HP ProLiant G8 Microserver G1610T","text":"<ul> <li>Intel Celeron G1610T (dual core 2.3 GHz)</li> <li>16GB RAM</li> <li>2 x 3TB HDD</li> <li>2 x 4TB HDD</li> </ul>"},{"location":"homelab/hardware/#raspberry-pi-2-model-b","title":"Raspberry Pi 2 Model B","text":"<ul> <li>Quad core</li> <li>1GB RAM</li> <li>8GB MicroSD</li> </ul>"},{"location":"homelab/hardware/#raspberry-pi-4-model-b","title":"Raspberry\u00a0Pi\u00a04 Model B","text":"<ul> <li>Quad core</li> <li>4GB RAM</li> <li>16GB MicroSD</li> </ul>"},{"location":"homelab/hardware/#3-x-raspberry-pi-5","title":"3 x Raspberry Pi 5","text":"<ul> <li>Quad core</li> <li>8GB RAM</li> <li>32GB MicroSD</li> <li>256GB NVME</li> </ul>"},{"location":"homelab/hardware/#1-x-raspberry-pi-5","title":"1 x Raspberry Pi 5","text":"<ul> <li>Quad core</li> <li>8GB RAM</li> <li>32GB MicroSD</li> <li>Hailo 13 TOPS</li> </ul>"},{"location":"homelab/hardware/#1-x-raspberry-pi-5_1","title":"1 x Raspberry Pi 5","text":"<ul> <li>Quad core</li> <li>8GB RAM</li> <li>32GB MicroSD</li> <li>Dual NVME HAT</li> <li>2 x 512GB NVME</li> </ul>"},{"location":"homelab/hardware/#networking","title":"Networking","text":"<ul> <li>MikroTik 5009UG+ Compact Rackmount 9 Port Router</li> <li>MikroTik CSS318 Cloud Smart Switch - CSS318-16G-2S+IN</li> <li>Mikrotik Rack-holder 19inch 10U Adjustable Desktop Rack</li> <li>MikroTik SFP+ Direct Attach Active Optics Cable 5m - S+AO0005</li> </ul>"}]}