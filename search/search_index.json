{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Oliver Mussell \u00b6 Me \u00b6 Working as a Systems Administrator since 2012 in many industries, ranging from Medical, Energy Generation, Environmental, Government and Education. Most recent previous job was maintaining ~5,000 websites for UK schools along with various services to allow schools to communicate with parents. Currently working as an SRE at Crossref . Interests \u00b6 Video games, cycling, cryptography Contact \u00b6 You can reach me via email at: my first name dot last name @ hotmail.co.uk Projects \u00b6 quire - Generate a One Time Pad notebook for unbreakable encryption bun - Simple YAML Based Puppet ENC dotfiles - Workstation config files","title":"Home"},{"location":"#oliver-mussell","text":"","title":"Oliver Mussell"},{"location":"#me","text":"Working as a Systems Administrator since 2012 in many industries, ranging from Medical, Energy Generation, Environmental, Government and Education. Most recent previous job was maintaining ~5,000 websites for UK schools along with various services to allow schools to communicate with parents. Currently working as an SRE at Crossref .","title":"Me"},{"location":"#interests","text":"Video games, cycling, cryptography","title":"Interests"},{"location":"#contact","text":"You can reach me via email at: my first name dot last name @ hotmail.co.uk","title":"Contact"},{"location":"#projects","text":"quire - Generate a One Time Pad notebook for unbreakable encryption bun - Simple YAML Based Puppet ENC dotfiles - Workstation config files","title":"Projects"},{"location":"basecamp/","text":"Basecamp Books \u00b6 Shape Up \u00b6 Shaped versus unshaped work Setting appetites instead of estimates Designing at the right level of abstraction Concepting with breadboards and fat marker sketches Making bets with a capped downside (the circuit breaker) and honouring them with uninterrupted time Choosing the right cycle length (six weeks) A cool-down period between cycles Breaking projects apart into scopes Downhill versus uphill work and communicating about unknowns Scope hammering to separate must-haves from nice-to-haves","title":"Basecamp books"},{"location":"basecamp/#basecamp-books","text":"","title":"Basecamp Books"},{"location":"basecamp/#shape-up","text":"Shaped versus unshaped work Setting appetites instead of estimates Designing at the right level of abstraction Concepting with breadboards and fat marker sketches Making bets with a capped downside (the circuit breaker) and honouring them with uninterrupted time Choosing the right cycle length (six weeks) A cool-down period between cycles Breaking projects apart into scopes Downhill versus uphill work and communicating about unknowns Scope hammering to separate must-haves from nice-to-haves","title":"Shape Up"},{"location":"bhyve-vm-creation/","text":"Bhyve VM Creation \u00b6 Bhyve Initial Setup \u00b6 Enable the tap interface in /etc/sysctl.conf and load it on the currently running system net.link.tap.up_on_open=1 sysctl -f /etc/sysctl.conf Enable bhyve, serial console and bridge/tap interface kernel modules in /boot/loader.conf . Reboot to apply changes or use kldload. vmm_load=\"YES\" nmdm_load=\"YES\" if_bridge_load=\"YES\" if_tap_load=\"YES\" Set up the network interfaces in /etc/rc.conf cloned_interfaces=\"bridge0 tap0\" ifconfig_bridge0=\"addm re0 addm tap0\" Create a ZFS volume zfs create -V16G -o volmode=dev zroot/testvm Download the installation image fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/ISO-IMAGES/11.1/FreeBSD-11.1-RELEASE-amd64-disc1.iso Start the VM sh /usr/share/examples/bhyve/vmrun.sh -c 1 -m 512M -t tap0 -d /dev/zvol/zroot/testvm -i -I FreeBSD-11.1-RELEASE-amd64-disc1.iso testvm Install as normal, following the menu options New VM Creation Script \u00b6 #! /bin/sh read -p \"Enter hostname: \" hostname zfs create -V16G -o volmode=dev zroot/$hostname sh /usr/share/examples/bhyve/vmrun.sh -c 1 -m 512M -t tap0 -d /dev/zvol/zroot/$hostname -i -I ~/FreeBSD-11.1-RELEASE-amd64-disc1.iso $hostname Creating a Linux guest \u00b6 Create a file for the hard disk truncate -s 16G linux.img Create the file to map the virtual devices for kernel load ~/device.map (hd0) /root/linux.img (cd0) /root/linux.iso Load the kernel grub-bhyve -m ~/device.map -r cd0 -M 1024M linuxguest Grub should start, choose install as normal Start the VM bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,tap0 -s 3:0,virtio-blk,/root/linux.img -l com1,/dev/nmdm0A -c 1 -m 512M linuxguest Access through the serial console cu -l /dev/nmdm0B pfSense in a VM \u00b6 Download the pfSense disk image from the website using fetch fetch https://frafiles.pfsense.org/mirror/downloads/pfSense-CE-2.3.1-RELEASE-2g-amd64-nanobsd.img.gz -o ~/pfSense.img.gz Create the storage zfs create -V2G -o volmode=dev zroot/pfsense Unzip the file, and redirect output to the storage via dd gzip -dc pfSense.img.gz | dd of=/dev/zvol/zroot/pfsense obs=64k Load the kernel and start the boot process bhyveload -c /dev/nmdm0A -d /dev/zvol/zroot/pfsense -m 256MB pfsense Start the VM /usr/sbin/bhyve -c 1 -m 256 -A -H -P -s 0:0,hostbridge -s 1:0,virtio-net,tap0 -s 3:0,ahci-hd,/dev/zvol/zroot/pfsense -s 4:1,lpc -l com1,/dev/nmdm0A pfsense Connect to the VM via the serial connection with nmdm cu -l /dev/nmdm0B Perform initial configuration through the shell to assign the network interfaces Once done, use the IP address to access through the web console When finished, you can shutdown/reboot To de-allocate the resources, you need to destroy the VM bhyvectl --destroy --vm=pfsense Multiple VMs using bhyve \u00b6 To allow networking on multiple vms, there should be a tap assigned to each vm, connected to the same bridge. cloned_interfaces=\"bridge0 tap0 tap1 tap2\" ifconfig_bridge0=\"addm re0 addm tap0 addm tap1 addm tap2\" Then when you provision vms, assign one of the tap interfaces to them. vm-bhyve \u00b6 A better way for managing a bhyve hypervisor. Follow the instructions on the repo. When adding the switch to a network interface, it doesn't work with re0. tap1 works, but then internet doesnt work in the VMs. Needs sorting. zfs bsd-cloud-init should be tested, it sets hostname based on openstack image name. otherwise, if we figure out how to make a template VM, you could set the hostname as part of transferring over the rc.conf file create template VM, start it, zfs send/recv?","title":"Bhyve VM Creation"},{"location":"bhyve-vm-creation/#bhyve-vm-creation","text":"","title":"Bhyve VM Creation"},{"location":"bhyve-vm-creation/#bhyve-initial-setup","text":"Enable the tap interface in /etc/sysctl.conf and load it on the currently running system net.link.tap.up_on_open=1 sysctl -f /etc/sysctl.conf Enable bhyve, serial console and bridge/tap interface kernel modules in /boot/loader.conf . Reboot to apply changes or use kldload. vmm_load=\"YES\" nmdm_load=\"YES\" if_bridge_load=\"YES\" if_tap_load=\"YES\" Set up the network interfaces in /etc/rc.conf cloned_interfaces=\"bridge0 tap0\" ifconfig_bridge0=\"addm re0 addm tap0\" Create a ZFS volume zfs create -V16G -o volmode=dev zroot/testvm Download the installation image fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/ISO-IMAGES/11.1/FreeBSD-11.1-RELEASE-amd64-disc1.iso Start the VM sh /usr/share/examples/bhyve/vmrun.sh -c 1 -m 512M -t tap0 -d /dev/zvol/zroot/testvm -i -I FreeBSD-11.1-RELEASE-amd64-disc1.iso testvm Install as normal, following the menu options","title":"Bhyve Initial Setup"},{"location":"bhyve-vm-creation/#new-vm-creation-script","text":"#! /bin/sh read -p \"Enter hostname: \" hostname zfs create -V16G -o volmode=dev zroot/$hostname sh /usr/share/examples/bhyve/vmrun.sh -c 1 -m 512M -t tap0 -d /dev/zvol/zroot/$hostname -i -I ~/FreeBSD-11.1-RELEASE-amd64-disc1.iso $hostname","title":"New VM Creation Script"},{"location":"bhyve-vm-creation/#creating-a-linux-guest","text":"Create a file for the hard disk truncate -s 16G linux.img Create the file to map the virtual devices for kernel load ~/device.map (hd0) /root/linux.img (cd0) /root/linux.iso Load the kernel grub-bhyve -m ~/device.map -r cd0 -M 1024M linuxguest Grub should start, choose install as normal Start the VM bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,tap0 -s 3:0,virtio-blk,/root/linux.img -l com1,/dev/nmdm0A -c 1 -m 512M linuxguest Access through the serial console cu -l /dev/nmdm0B","title":"Creating a Linux guest"},{"location":"bhyve-vm-creation/#pfsense-in-a-vm","text":"Download the pfSense disk image from the website using fetch fetch https://frafiles.pfsense.org/mirror/downloads/pfSense-CE-2.3.1-RELEASE-2g-amd64-nanobsd.img.gz -o ~/pfSense.img.gz Create the storage zfs create -V2G -o volmode=dev zroot/pfsense Unzip the file, and redirect output to the storage via dd gzip -dc pfSense.img.gz | dd of=/dev/zvol/zroot/pfsense obs=64k Load the kernel and start the boot process bhyveload -c /dev/nmdm0A -d /dev/zvol/zroot/pfsense -m 256MB pfsense Start the VM /usr/sbin/bhyve -c 1 -m 256 -A -H -P -s 0:0,hostbridge -s 1:0,virtio-net,tap0 -s 3:0,ahci-hd,/dev/zvol/zroot/pfsense -s 4:1,lpc -l com1,/dev/nmdm0A pfsense Connect to the VM via the serial connection with nmdm cu -l /dev/nmdm0B Perform initial configuration through the shell to assign the network interfaces Once done, use the IP address to access through the web console When finished, you can shutdown/reboot To de-allocate the resources, you need to destroy the VM bhyvectl --destroy --vm=pfsense","title":"pfSense in a VM"},{"location":"bhyve-vm-creation/#multiple-vms-using-bhyve","text":"To allow networking on multiple vms, there should be a tap assigned to each vm, connected to the same bridge. cloned_interfaces=\"bridge0 tap0 tap1 tap2\" ifconfig_bridge0=\"addm re0 addm tap0 addm tap1 addm tap2\" Then when you provision vms, assign one of the tap interfaces to them.","title":"Multiple VMs using bhyve"},{"location":"bhyve-vm-creation/#vm-bhyve","text":"A better way for managing a bhyve hypervisor. Follow the instructions on the repo. When adding the switch to a network interface, it doesn't work with re0. tap1 works, but then internet doesnt work in the VMs. Needs sorting. zfs bsd-cloud-init should be tested, it sets hostname based on openstack image name. otherwise, if we figure out how to make a template VM, you could set the hostname as part of transferring over the rc.conf file create template VM, start it, zfs send/recv?","title":"vm-bhyve"},{"location":"bonded-nic/","text":"Creating Bonded NICs on Ubuntu 20.04 \u00b6 Summary \u00b6 On the HP MicroServer Gen 8 and MicroServer Gen 10+ there are four ethernet ports. I wanted to group these together into a bonded NIC so that ethernet traffic could run over all four to increase the throughput. There are other names for bonding, like teaming or link aggregation/LACP. They all mean the same thing, multiple network ports joined together to serve traffic. Setup \u00b6 Install the dependency: apt install ifenslave Load the kernel module: # Check if already loaded: lsmod | grep bonding # If no output then: modprobe bonding This only loads the bonding kernel module while the system is running, it would be lost on reboot. Add it to the modules file to load it at boot as well: vim /etc/modules # Add the following line to the file: bonding Find the network interfaces. You can do this in a few different ways, an easy way is just: ip addr Which lists all of your network interfaces. lo is for loopback, then the others will be your ethernet interfaces. This can differ between different NIC manufacturers. On this machine, its returning names like eno1 , eno2 etc. Sometimes it is like enp2s0 or enp3s0 instead. Edit the netplan config file to add the config. You will need to know the IP address of your gateway/router and the IP addresses for the DNS nameservers. For me, the router IP is 192.168.0.1 and the DNS servers are 192.168.0.15 (Rpi) and 1.1.1.1 (Cloudflare). Also, I've set the bonded NIC to use DHCP to configure its IP address. I'm also using all available eno* ethernet ports in the bond. If you want to use a specific set of ports instead, check out the netplan documentation vim /etc/netplan/00-installer-config.yaml network: version: 2 ethernets: eports: match: name: eno* optional: true bonds: bond0: interfaces: [eports] dhcp4: true gateway4: 192.168.0.1 nameservers: addresses: [192.168.0.15, 1.1.1.1] parameters: mode: 802.3ad lacp-rate: fast mii-monitor-interval: 100 Apply the changes: netplan apply If you lose your SSH connection, something went wrong, or DHCP has just decided to give it a different IP address than what you used to connect. Its a good idea to have out of band management or a spare keyboard+monitor plugged in if the network stops working. End result \u00b6 If you run ip addr again, you can now see a new network interface has been created called bond0 , which is the master. Then the eno* interfaces have been added as slaves of bond0 . root@dixie:~# ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno1: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond0 state UP group default qlen 1000 link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff 3: eno2: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond0 state UP group default qlen 1000 link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff 4: eno3: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond0 state UP group default qlen 1000 link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff 5: eno4: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond0 state UP group default qlen 1000 link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff 6: bond0: <BROADCAST,MULTICAST,MASTER,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff inet 192.168.0.21/24 brd 192.168.0.255 scope global dynamic bond0 valid_lft 84976sec preferred_lft 84976sec inet6 fe80::dcd5:c6ff:feda:5ba0/64 scope link valid_lft forever preferred_lft forever","title":"Bonded NICs on Ubuntu"},{"location":"bonded-nic/#creating-bonded-nics-on-ubuntu-2004","text":"","title":"Creating Bonded NICs on Ubuntu 20.04"},{"location":"bonded-nic/#summary","text":"On the HP MicroServer Gen 8 and MicroServer Gen 10+ there are four ethernet ports. I wanted to group these together into a bonded NIC so that ethernet traffic could run over all four to increase the throughput. There are other names for bonding, like teaming or link aggregation/LACP. They all mean the same thing, multiple network ports joined together to serve traffic.","title":"Summary"},{"location":"bonded-nic/#setup","text":"Install the dependency: apt install ifenslave Load the kernel module: # Check if already loaded: lsmod | grep bonding # If no output then: modprobe bonding This only loads the bonding kernel module while the system is running, it would be lost on reboot. Add it to the modules file to load it at boot as well: vim /etc/modules # Add the following line to the file: bonding Find the network interfaces. You can do this in a few different ways, an easy way is just: ip addr Which lists all of your network interfaces. lo is for loopback, then the others will be your ethernet interfaces. This can differ between different NIC manufacturers. On this machine, its returning names like eno1 , eno2 etc. Sometimes it is like enp2s0 or enp3s0 instead. Edit the netplan config file to add the config. You will need to know the IP address of your gateway/router and the IP addresses for the DNS nameservers. For me, the router IP is 192.168.0.1 and the DNS servers are 192.168.0.15 (Rpi) and 1.1.1.1 (Cloudflare). Also, I've set the bonded NIC to use DHCP to configure its IP address. I'm also using all available eno* ethernet ports in the bond. If you want to use a specific set of ports instead, check out the netplan documentation vim /etc/netplan/00-installer-config.yaml network: version: 2 ethernets: eports: match: name: eno* optional: true bonds: bond0: interfaces: [eports] dhcp4: true gateway4: 192.168.0.1 nameservers: addresses: [192.168.0.15, 1.1.1.1] parameters: mode: 802.3ad lacp-rate: fast mii-monitor-interval: 100 Apply the changes: netplan apply If you lose your SSH connection, something went wrong, or DHCP has just decided to give it a different IP address than what you used to connect. Its a good idea to have out of band management or a spare keyboard+monitor plugged in if the network stops working.","title":"Setup"},{"location":"bonded-nic/#end-result","text":"If you run ip addr again, you can now see a new network interface has been created called bond0 , which is the master. Then the eno* interfaces have been added as slaves of bond0 . root@dixie:~# ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno1: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond0 state UP group default qlen 1000 link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff 3: eno2: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond0 state UP group default qlen 1000 link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff 4: eno3: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond0 state UP group default qlen 1000 link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff 5: eno4: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond0 state UP group default qlen 1000 link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff 6: bond0: <BROADCAST,MULTICAST,MASTER,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether de:d5:c6:da:5b:a0 brd ff:ff:ff:ff:ff:ff inet 192.168.0.21/24 brd 192.168.0.255 scope global dynamic bond0 valid_lft 84976sec preferred_lft 84976sec inet6 fe80::dcd5:c6ff:feda:5ba0/64 scope link valid_lft forever preferred_lft forever","title":"End result"},{"location":"fbsd-update-cache/","text":"Caching freebsd-update and pkg files \u00b6 Change the domains as appropriate. The proxy_store location is where the cached files will be placed. This directory needs to be accessible by the user that NGINX is running as (defaults to www). NGINX config: # pkg server { listen *:80; server_name pkg.mydomain.local; access_log /var/log/nginx/pkg.access.log; error_log /var/log/nginx/pkg.error.log; location / { root /var/cache/packages/freebsd; try_files $uri @pkg_cache; } location @pkg_cache { proxy_pass https://pkg.freebsd.org; proxy_set_header Host $host; proxy_cache_lock on; proxy_cache_lock_timeout 20s; proxy_cache_revalidate on; proxy_cache_valid 200 301 302 30d; proxy_store /var/cache/packages/freebsd/$request_uri; } } # freebsd-update server { listen *:80; server_name freebsd-update.mydomain.local; access_log /var/log/nginx/freebsd_update.access.log; error_log /var/log/nginx/freebsd_update.error.log; location / { root /var/cache/freebsd-update; try_files $uri @freebsd_update_cache; } location @freebsd_update_cache { proxy_pass http://update.freebsd.org; proxy_set_header Host update.freebsd.org; proxy_cache_lock on; proxy_cache_lock_timeout 20s; proxy_cache_revalidate on; proxy_cache_valid 200 301 302 30d; proxy_store /var/cache/freebsd-update/$request_uri; } } Client config: Create /usr/local/etc/pkg/repos/FreeBSD.conf with this content: FreeBSD: { enabled: NO } MyRepo: { url: \"pkg+http://pkg.mydomain.local/${ABI}/latest\", enabled: true, signature_type: \"fingerprints\", fingerprints: \"/usr/share/keys/pkg\", mirror_type: \"srv\" } Edit /etc/freebsd-update.conf , change ServerName value to freebsd-update.mydomain.local .","title":"Caching freebsd-update and pkg files"},{"location":"fbsd-update-cache/#caching-freebsd-update-and-pkg-files","text":"Change the domains as appropriate. The proxy_store location is where the cached files will be placed. This directory needs to be accessible by the user that NGINX is running as (defaults to www). NGINX config: # pkg server { listen *:80; server_name pkg.mydomain.local; access_log /var/log/nginx/pkg.access.log; error_log /var/log/nginx/pkg.error.log; location / { root /var/cache/packages/freebsd; try_files $uri @pkg_cache; } location @pkg_cache { proxy_pass https://pkg.freebsd.org; proxy_set_header Host $host; proxy_cache_lock on; proxy_cache_lock_timeout 20s; proxy_cache_revalidate on; proxy_cache_valid 200 301 302 30d; proxy_store /var/cache/packages/freebsd/$request_uri; } } # freebsd-update server { listen *:80; server_name freebsd-update.mydomain.local; access_log /var/log/nginx/freebsd_update.access.log; error_log /var/log/nginx/freebsd_update.error.log; location / { root /var/cache/freebsd-update; try_files $uri @freebsd_update_cache; } location @freebsd_update_cache { proxy_pass http://update.freebsd.org; proxy_set_header Host update.freebsd.org; proxy_cache_lock on; proxy_cache_lock_timeout 20s; proxy_cache_revalidate on; proxy_cache_valid 200 301 302 30d; proxy_store /var/cache/freebsd-update/$request_uri; } } Client config: Create /usr/local/etc/pkg/repos/FreeBSD.conf with this content: FreeBSD: { enabled: NO } MyRepo: { url: \"pkg+http://pkg.mydomain.local/${ABI}/latest\", enabled: true, signature_type: \"fingerprints\", fingerprints: \"/usr/share/keys/pkg\", mirror_type: \"srv\" } Edit /etc/freebsd-update.conf , change ServerName value to freebsd-update.mydomain.local .","title":"Caching freebsd-update and pkg files"},{"location":"firecracker/","text":"Firecracker \u00b6 Firecracker - Secure and fast microVMs for serverless computing. Quickstart \u00b6 Quickstart Get firecracker \u00b6 release_url=\"https://github.com/firecracker-microvm/firecracker/releases\" latest=$(basename $(curl -fsSLI -o /dev/null -w %{url_effective} ${release_url}/latest)) curl -L ${release_url}/download/${latest}/firecracker-${latest}-x86_64.tgz | tar -xz Get the kernel and rootfs \u00b6 # Official instructions dest_kernel=\"hello-vmlinux.bin\" dest_rootfs=\"hello-rootfs.ext4\" image_bucket_url=\"https://s3.amazonaws.com/spec.ccfc.min/img/quickstart_guide/x86_64\" kernel=\"${image_bucket_url}/kernels/vmlinux.bin\" rootfs=\"${image_bucket_url}/rootfs/bionic.rootfs.ext4\" curl -fsSL -o $dest_kernel $kernel curl -fsSL -o $dest_rootfs $rootfs # Same but with a recently built image as gleaned from the bucket list dest_kernel=\"hello-vmlinux.bin\" dest_rootfs=\"hello-rootfs.ext4\" kernel=\"https://s3.amazonaws.com/spec.ccfc.min/img-dev/x86_64/ubuntu/kernel/vmlinux.bin\" rootfs=\"https://s3.amazonaws.com/spec.ccfc.min/img-dev/x86_64/ubuntu/fsfiles/bionic.rootfs.ext4\" curl -fsSL -o $dest_kernel $kernel curl -fsSL -o $dest_rootfs $rootfs Using firectl \u00b6 Get firectl \u00b6 curl -Lo firectl https://firectl-release.s3.amazonaws.com/firectl-v0.1.0 chmod +x firectl Create microvm \u00b6 ./firectl \\ --kernel=hello-vmlinux.bin \\ --root-drive=hello-rootfs.ext4 Using the API \u00b6 Set the guest kernel \u00b6 kernel_path=$(pwd)\"/hello-vmlinux.bin\" curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/boot-source' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d \"{ \\\"kernel_image_path\\\": \\\"${kernel_path}\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" Set the guest rootf \u00b6 rootfs_path=$(pwd)\"/hello-rootfs.ext4\" curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/drives/rootfs' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d \"{ \\\"drive_id\\\": \\\"rootfs\\\", \\\"path_on_host\\\": \\\"${rootfs_path}\\\", \\\"is_root_device\\\": true, \\\"is_read_only\\\": false }\" Start the guest machine \u00b6 curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/actions' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"action_type\": \"InstanceStart\" }' Compile Kernel and FS manually \u00b6 Follow the steps in here to compile the kernel and base file. On Ubuntu when compiling you need to install dependencies like libssl-dev, libncurses-dev, bison, autoconf. Then if you try and compile and it complains about auto.conf not existing, run make menuconfig, then exit out immediately. That seems to have sorted it. Then when you run make vmlinux it asks lots of questions, but by using the preexisting config file from the repo a lot has already been decided. You could probably pipe yes into this, or otherwise just hold enter. Someone with more kernel experience needs to go over those options and decide if they're necessary. Once compiled continue with the getting started instructions but change the path to the kernel file to the vmlinux you created. I compiled 5.4 kernel and used the existing alpine base from the getting started and it boots just fine. Using the jailer \u00b6 Install \u00b6 The jailer is used to provide additional isolation for the VMs. The jailer binary is included in the tgz file from the firecracker release. # It needs to be built statically linked to musl. apt install -y musl-tools # install rust via rustup # cd into the jailer directory in the firecracker repo cargo build --target=\"x86_64-unknown-linux-musl\" --release # the built binary gets created at: ../../build/cargo_target/x86_64-unknown-linux-musl/release/jailer # you should probably build with tools/devtool build instead Run \u00b6 Rather than running the jailer binary manually, you can use the --jailer flag with firectl. Note that you must also include the --chroot-base-dir=\"/srv/jailer\" flag, otherwise you get the no such file or directory error as per this issue . You also need to copy or move the firectl, firecracker, and jailer binaries into a bin directory in your $PATH otherwise it complains. I copied them to /usr/local/bin /usr/local/bin/firectl --kernel=/root/release-v0.25.2-x86_64/hello-vmlinux.bin --root-drive=/root/release-v0.25.2-x86_64/hello-rootfs.ext4 --jailer=/usr/local/bin/jailer --exec-file=/usr/local/bin/firecracker --id=testvm4 --chroot-base-dir=\"/srv/jailer\" Firectl also doesnt handle cleaning up the /srv/jailer/firecracker/$vm_name chroot directory when you power off the VM. So you need to clean this up manually. Notes from running the jailer manually \u00b6 cp -v rootfs.ext4 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ cp -v vmlinux.bin /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"/root/vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"/srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" ls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ chown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin chown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/rootfs.ext4 curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"/srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"/root/vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" ls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"/root/vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" ls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"./vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/drives/rootfs' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"drive_id\\\": \\\"rootfs\\\", \\\"path_on_host\\\": \\\"./rootfs.ext4\\\", \\\"is_root_device\\\": true, \\\"is_read_only\\\": false }\" curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/actions' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"action_type\": \"InstanceStart\" }' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X GET 'http://localhost/' -H 'Accept: application/json' -H 'Content-Type: application/json' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/actions' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"action_type\": \"InstanceStop\" }' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/actions' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"action_type\": \"InstanceStop\" }' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/actions' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"action_type\": \"SendCtrlAltDel\" }' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X GET 'http://localhost/' -H 'Accept: application/json' -H 'Content-Type: application/json' ps aux | grep fire ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/ mkdir -p /srv/jailer cp -v ./release-v1.1.2-x86_64/jailer-v1.1.2-x86_64 /usr/bin/jailer ls -alh /usr/bin/jailer ls /var/run/netns ls /var/run /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --daemonize addgroup -G 1111 551e7604-e35c-42b3-b825-416853441234 addgroup ---gid 1111 551e7604-e35c-42b3-b825-416853441234 addgroup --gid 1111 551e7604 addgroup --gid 1111 551e7604-e35c-42b3-b825-416853441234 addgroup --gid 1111 551e7604 addgroup --gid 1111 5517604 addgroup --gid 1111 mygroupname adduser adduser myusername /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --daemonize --uid 1001 --gid 1111 ls /usr/bin/firecracker cp -v ./release-v1.1.2-x86_64/firecracker-v1.1.2-x86_64 /usr/bin/firecracker /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --daemonize --uid 1001 --gid 1111 /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ mkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ chown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ ls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize mkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ chown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize rm /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/dev/net/tun rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ mkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ chown -R 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ mkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ chown -R 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"./vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/drives/rootfs' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"drive_id\\\": \\\"rootfs\\\", \\\"path_on_host\\\": \\\"./rootfs.ext4\\\", \\\"is_root_device\\\": true, \\\"is_read_only\\\": false }\" curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/network-interfaces/eth0' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"iface_id\": \"eth0\", \"host_dev_name\": \"tap0\" } ' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/actions' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"action_type\": \"InstanceStart\" }' ps aux | grep jail ps aux | grep fire ip addr | less ping 10.0.0.1 ping 10.0.0.2 pkill firecracker ip tuntap add tap0 mode tap ip addr ip addr | grep bond0 ip addr add 10.0.0.1/24 dev tap0 ip link set tap0 up sh -c \"echo 1 > /proc/sys/net/ipv4/ip_forward\" iptables --help iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT iptables -A FORWARD -i tap0 -o eth0 -j ACCEPT ps aux | grep fire /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/dev /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/dev /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111","title":"Firecracker"},{"location":"firecracker/#firecracker","text":"Firecracker - Secure and fast microVMs for serverless computing.","title":"Firecracker"},{"location":"firecracker/#quickstart","text":"Quickstart","title":"Quickstart"},{"location":"firecracker/#get-firecracker","text":"release_url=\"https://github.com/firecracker-microvm/firecracker/releases\" latest=$(basename $(curl -fsSLI -o /dev/null -w %{url_effective} ${release_url}/latest)) curl -L ${release_url}/download/${latest}/firecracker-${latest}-x86_64.tgz | tar -xz","title":"Get firecracker"},{"location":"firecracker/#get-the-kernel-and-rootfs","text":"# Official instructions dest_kernel=\"hello-vmlinux.bin\" dest_rootfs=\"hello-rootfs.ext4\" image_bucket_url=\"https://s3.amazonaws.com/spec.ccfc.min/img/quickstart_guide/x86_64\" kernel=\"${image_bucket_url}/kernels/vmlinux.bin\" rootfs=\"${image_bucket_url}/rootfs/bionic.rootfs.ext4\" curl -fsSL -o $dest_kernel $kernel curl -fsSL -o $dest_rootfs $rootfs # Same but with a recently built image as gleaned from the bucket list dest_kernel=\"hello-vmlinux.bin\" dest_rootfs=\"hello-rootfs.ext4\" kernel=\"https://s3.amazonaws.com/spec.ccfc.min/img-dev/x86_64/ubuntu/kernel/vmlinux.bin\" rootfs=\"https://s3.amazonaws.com/spec.ccfc.min/img-dev/x86_64/ubuntu/fsfiles/bionic.rootfs.ext4\" curl -fsSL -o $dest_kernel $kernel curl -fsSL -o $dest_rootfs $rootfs","title":"Get the kernel and rootfs"},{"location":"firecracker/#using-firectl","text":"","title":"Using firectl"},{"location":"firecracker/#get-firectl","text":"curl -Lo firectl https://firectl-release.s3.amazonaws.com/firectl-v0.1.0 chmod +x firectl","title":"Get firectl"},{"location":"firecracker/#create-microvm","text":"./firectl \\ --kernel=hello-vmlinux.bin \\ --root-drive=hello-rootfs.ext4","title":"Create microvm"},{"location":"firecracker/#using-the-api","text":"","title":"Using the API"},{"location":"firecracker/#set-the-guest-kernel","text":"kernel_path=$(pwd)\"/hello-vmlinux.bin\" curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/boot-source' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d \"{ \\\"kernel_image_path\\\": \\\"${kernel_path}\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\"","title":"Set the guest kernel"},{"location":"firecracker/#set-the-guest-rootf","text":"rootfs_path=$(pwd)\"/hello-rootfs.ext4\" curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/drives/rootfs' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d \"{ \\\"drive_id\\\": \\\"rootfs\\\", \\\"path_on_host\\\": \\\"${rootfs_path}\\\", \\\"is_root_device\\\": true, \\\"is_read_only\\\": false }\"","title":"Set the guest rootf"},{"location":"firecracker/#start-the-guest-machine","text":"curl --unix-socket /tmp/firecracker.socket -i \\ -X PUT 'http://localhost/actions' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"action_type\": \"InstanceStart\" }'","title":"Start the guest machine"},{"location":"firecracker/#compile-kernel-and-fs-manually","text":"Follow the steps in here to compile the kernel and base file. On Ubuntu when compiling you need to install dependencies like libssl-dev, libncurses-dev, bison, autoconf. Then if you try and compile and it complains about auto.conf not existing, run make menuconfig, then exit out immediately. That seems to have sorted it. Then when you run make vmlinux it asks lots of questions, but by using the preexisting config file from the repo a lot has already been decided. You could probably pipe yes into this, or otherwise just hold enter. Someone with more kernel experience needs to go over those options and decide if they're necessary. Once compiled continue with the getting started instructions but change the path to the kernel file to the vmlinux you created. I compiled 5.4 kernel and used the existing alpine base from the getting started and it boots just fine.","title":"Compile Kernel and FS manually"},{"location":"firecracker/#using-the-jailer","text":"","title":"Using the jailer"},{"location":"firecracker/#install","text":"The jailer is used to provide additional isolation for the VMs. The jailer binary is included in the tgz file from the firecracker release. # It needs to be built statically linked to musl. apt install -y musl-tools # install rust via rustup # cd into the jailer directory in the firecracker repo cargo build --target=\"x86_64-unknown-linux-musl\" --release # the built binary gets created at: ../../build/cargo_target/x86_64-unknown-linux-musl/release/jailer # you should probably build with tools/devtool build instead","title":"Install"},{"location":"firecracker/#run","text":"Rather than running the jailer binary manually, you can use the --jailer flag with firectl. Note that you must also include the --chroot-base-dir=\"/srv/jailer\" flag, otherwise you get the no such file or directory error as per this issue . You also need to copy or move the firectl, firecracker, and jailer binaries into a bin directory in your $PATH otherwise it complains. I copied them to /usr/local/bin /usr/local/bin/firectl --kernel=/root/release-v0.25.2-x86_64/hello-vmlinux.bin --root-drive=/root/release-v0.25.2-x86_64/hello-rootfs.ext4 --jailer=/usr/local/bin/jailer --exec-file=/usr/local/bin/firecracker --id=testvm4 --chroot-base-dir=\"/srv/jailer\" Firectl also doesnt handle cleaning up the /srv/jailer/firecracker/$vm_name chroot directory when you power off the VM. So you need to clean this up manually.","title":"Run"},{"location":"firecracker/#notes-from-running-the-jailer-manually","text":"cp -v rootfs.ext4 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ cp -v vmlinux.bin /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"/root/vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"/srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" ls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ chown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin chown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/rootfs.ext4 curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"/srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"/root/vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" ls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"/root/vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" ls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"./vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/drives/rootfs' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"drive_id\\\": \\\"rootfs\\\", \\\"path_on_host\\\": \\\"./rootfs.ext4\\\", \\\"is_root_device\\\": true, \\\"is_read_only\\\": false }\" curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/actions' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"action_type\": \"InstanceStart\" }' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X GET 'http://localhost/' -H 'Accept: application/json' -H 'Content-Type: application/json' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/actions' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"action_type\": \"InstanceStop\" }' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/actions' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"action_type\": \"InstanceStop\" }' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/actions' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"action_type\": \"SendCtrlAltDel\" }' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X GET 'http://localhost/' -H 'Accept: application/json' -H 'Content-Type: application/json' ps aux | grep fire ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ ls /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/ mkdir -p /srv/jailer cp -v ./release-v1.1.2-x86_64/jailer-v1.1.2-x86_64 /usr/bin/jailer ls -alh /usr/bin/jailer ls /var/run/netns ls /var/run /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --daemonize addgroup -G 1111 551e7604-e35c-42b3-b825-416853441234 addgroup ---gid 1111 551e7604-e35c-42b3-b825-416853441234 addgroup --gid 1111 551e7604 addgroup --gid 1111 551e7604-e35c-42b3-b825-416853441234 addgroup --gid 1111 551e7604 addgroup --gid 1111 5517604 addgroup --gid 1111 mygroupname adduser adduser myusername /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --daemonize --uid 1001 --gid 1111 ls /usr/bin/firecracker cp -v ./release-v1.1.2-x86_64/firecracker-v1.1.2-x86_64 /usr/bin/firecracker /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --daemonize --uid 1001 --gid 1111 /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ mkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ chown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ ls -alh /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize mkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ chown 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize rm /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/dev/net/tun rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ mkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ chown -R 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 --daemonize /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ mkdir -p /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/ chown -R 1001:1111 /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/ /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/boot-source' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"kernel_image_path\\\": \\\"./vmlinux.bin\\\", \\\"boot_args\\\": \\\"console=ttyS0 reboot=k panic=1 pci=off\\\" }\" curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/drives/rootfs' -H 'Accept: application/json' -H 'Content-Type: application/json' -d \"{ \\\"drive_id\\\": \\\"rootfs\\\", \\\"path_on_host\\\": \\\"./rootfs.ext4\\\", \\\"is_root_device\\\": true, \\\"is_read_only\\\": false }\" curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/network-interfaces/eth0' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"iface_id\": \"eth0\", \"host_dev_name\": \"tap0\" } ' curl --unix-socket /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run/firecracker.socket -i -X PUT 'http://localhost/actions' -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{ \"action_type\": \"InstanceStart\" }' ps aux | grep jail ps aux | grep fire ip addr | less ping 10.0.0.1 ping 10.0.0.2 pkill firecracker ip tuntap add tap0 mode tap ip addr ip addr | grep bond0 ip addr add 10.0.0.1/24 dev tap0 ip link set tap0 up sh -c \"echo 1 > /proc/sys/net/ipv4/ip_forward\" iptables --help iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT iptables -A FORWARD -i tap0 -o eth0 -j ACCEPT ps aux | grep fire /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/dev /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111 rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/run rm -rf /srv/jailer/firecracker/551e7604-e35c-42b3-b825-416853441234/root/dev /usr/bin/jailer --id 551e7604-e35c-42b3-b825-416853441234 --exec-file /usr/bin/firecracker --uid 1001 --gid 1111","title":"Notes from running the jailer manually"},{"location":"gemini/","text":"Gemini Protocol \u00b6 Project Gemini Gemini is a new, collaboratively designed internet protocol, which explores the space inbetween gopher and the web, striving to address (perceived) limitations of one while avoiding the (undeniable) pitfalls of the other. For the server I'm using satellite . git clone https://git.sr.ht/~gsthnz/satellite cd satellite go build mkdir -p /var/lib/satellite/certs Create satellite.toml # Address to listen to requests (default: 0.0.0.0:1965) #listen = \"0.0.0.0\" [tls] # Directory to save certificates directory = \"/var/lib/satellite/certs\" # Multiple domains can be set with the [[domain]] section [[domain]] name = \"gemini.matrix\" root = \"/srv/gemini/gemini.matrix\" For the client I'm using bombadillo git clone https://tildegit.org/sloum/bombadillo cd bombadillo sudo make install bombadillo You will need to create a directory with some static files inside. These files should have a file extension of .gmi or .gemini . The content is structured like a subset of markdown: # Normal text Hello World! # Link => gemini://example.org/ An Example Link # Preformatted text # ``` preformatted text surrounded by 3 backticks # ``` # Headers using # # Title ## Sub Title ### Sub Sub Title # Unordered list * No * Particular * Order # Quote lines > This is a good quote","title":"Gemini"},{"location":"gemini/#gemini-protocol","text":"Project Gemini Gemini is a new, collaboratively designed internet protocol, which explores the space inbetween gopher and the web, striving to address (perceived) limitations of one while avoiding the (undeniable) pitfalls of the other. For the server I'm using satellite . git clone https://git.sr.ht/~gsthnz/satellite cd satellite go build mkdir -p /var/lib/satellite/certs Create satellite.toml # Address to listen to requests (default: 0.0.0.0:1965) #listen = \"0.0.0.0\" [tls] # Directory to save certificates directory = \"/var/lib/satellite/certs\" # Multiple domains can be set with the [[domain]] section [[domain]] name = \"gemini.matrix\" root = \"/srv/gemini/gemini.matrix\" For the client I'm using bombadillo git clone https://tildegit.org/sloum/bombadillo cd bombadillo sudo make install bombadillo You will need to create a directory with some static files inside. These files should have a file extension of .gmi or .gemini . The content is structured like a subset of markdown: # Normal text Hello World! # Link => gemini://example.org/ An Example Link # Preformatted text # ``` preformatted text surrounded by 3 backticks # ``` # Headers using # # Title ## Sub Title ### Sub Sub Title # Unordered list * No * Particular * Order # Quote lines > This is a good quote","title":"Gemini Protocol"},{"location":"go-bazel/","text":"Building Go with Bazel \u00b6 Handling Go Dependencies \u00b6 During development, you will often use go get to download libraries for import into the program which is useful for development but not so useful when building the finished product. Managing these dependencies over time is a hassle as they change frequently and can sometimes disappear entirely. The dep tool provides a way of automatically scanning your import statements and evaluating all of the dependencies. It create some files Gopkg.toml and Gopkg.lock which contain the location and latest Git SHA of your dependencies. dep is installed via: curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh Run dep init to create the initial files, then as your develop run dep ensure to update dependencies to the latest version. The dep tool also downloads a copy of all dependencies into a vendor folder at the root of your project. This provides a backup in case a dependency disappears and provides the facility for reproducible builds. Bazel / Gazelle \u00b6 With our dependencies being updated, we would also need to update the WORKSPACE file so that Bazel/Gazelle knows about them as well. Gazelle requires the location and git commit hash in order to pull down the correct dependencies, but this is laborious to update manually. Thankfully, we can run a command to have gazelle pull in all of the dependencies from the Gopkg.lock file and update the WORKSPACE file automatically. Bazel will then pull in all of the dependencies correctly without any manual intervention. gazelle update-repos -from_file Gopkg.lock As part of ongoing development, you would periodically run dep ensure followed by gazelle update-repos -from_file Gopkg.lock to keep all of the dependencies up to date and generate the new WORKSPACE file. Packaging Go Applications \u00b6 Now that we've built the go application and its dependencies we now need to package it up to distribute across the infrastructure. Packaging with fpm \u00b6 The below command is an example of what we would want to run: fpm -s dir -t freebsd -n ~/go_test --version 1.0.0 --prefix /usr/local/bin go_tests But this has a few issues. Rather than putting the finished package into ~/go_test , it would be better in a dedicated directory like /var/packages or similar. The version number is hard coded which obviously isn't always going to be correct. You would want to instead have your CI tool set to only run the packaging command when a new tag/release is created, and then have the version number derived from the tag/release number. It also includes the --prefix flag to specify the path to prepend to any files in the package. This is required as when the package is installed/extracted, the files will be extracted to the full path as specified in the package. So in this instance the /usr/local/bin/go_tests file is extracted. For now, I'm getting by with the following command which will overwrite the finished package if it already exists. fpm -f -s dir -t freebsd -n ~/go_test --prefix /usr/local/bin go_tests Building Go programs using Bazel \u00b6 Bazel is a build tool created by Google which operates similarly to their internal build tool, Blaze. It is primarily concerned with generating artifacts from compiled languages like C, C++, Go etc. pkg install -y bazel Bazel requires some files so that it knows what and where to build. As an example, we are going to compile a simple go program with no dependencies (literally print a single string to stdout). // ~/go/src/github.com/omussell/go_tests/main.go package main import \"fmt\" func main() { fmt.Println(\"test\") } A file called WORKSPACE should be created at the root of the directory. This is used by bazel to determine source code locations relative to the WORKSPACE file and differentiate other packages in the same directory. Then a BUILD.bazel file should also be created at the root of the directory. Gazelle \u00b6 Instead of creating BUILD files by hand, we can use the Gazelle tool to iterate over a go source tree and dynamically generate BUILD files. We can also let bazel itself run gazelle. Note that gazelle doesn't work without bash, and the gazelle.bash file has a hardcoded path to /bin/bash which of course is not available on FreeBSD by default. pkg install -y bash ln -s /usr/local/bin/bash /bin/bash In the WORKSPACE file: http_archive( name = \"io_bazel_rules_go\", url = \"https://github.com/bazelbuild/rules_go/releases/download/0.9.0/rules_go-0.9.0.tar.gz\", sha256 = \"4d8d6244320dd751590f9100cf39fd7a4b75cd901e1f3ffdfd6f048328883695\", ) http_archive( name = \"bazel_gazelle\", url = \"https://github.com/bazelbuild/bazel-gazelle/releases/download/0.9/bazel-gazelle-0.9.tar.gz\", sha256 = \"0103991d994db55b3b5d7b06336f8ae355739635e0c2379dea16b8213ea5a223\", ) load(\"@io_bazel_rules_go//go:def.bzl\", \"go_rules_dependencies\", \"go_register_toolchains\") go_rules_dependencies() go_register_toolchains(go_version=\"host\") load(\"@bazel_gazelle//:deps.bzl\", \"gazelle_dependencies\") gazelle_dependencies() In the BUILD.bazel file: load(\"@bazel_gazelle//:def.bzl\", \"gazelle\") gazelle( name = \"gazelle\", prefix = \"github.com/omussell/go_tests\", ) Then to run: bazel run //:gazelle bazel build //:go_tests A built binary should be output to the ~/.cache directory. Once a binary has been built once, Bazel will only build again if the source code changes. Otherwise, any subsequent runs just complete successfully extremely quickly. When attempting to use bazel in any capacity like bazel run ... or bazel build ... it would give the following error: ERROR: /root/.cache/bazel/_bazel_root/...285a1776/external/io_bazel_rules_go/ BUILD.bazel:7:1: every rule of type go_context_data implicitly depends upon the target '@go_sdk// :packages.txt', but this target could not be found because of: no such package '@go_sdk//': Unsupported operating system: freebsd ERROR: /root/.cache/bazel/_bazel_root/...1776/external/io_bazel_rules_go/ BUILD.bazel:7:1: every rule of type go_context_data implicitly depends upon the target '@go_sdk// :files', but this target could not be found because of: no such package '@go_sdk//': Unsupported operating system: freebsd ERROR: /root/.cache/bazel/_bazel_root/...5a1776/external/io_bazel_rules_go/ BUILD.bazel:7:1: every rule of type go_context_data implicitly depends upon the target '@go_sdk// :tools', but this target could not be found because of: no such package '@go_sdk//': Unsupported operating system: freebsd ERROR: Analysis of target '//:gazelle' failed; build aborted: no such package '@go_sdk//': Unsupported operating system: freebsd I think this is caused by bazel attempting to download and build go which isn't necessary as we've already installed via the package anyway. In the WORKSPACE file, change the go_register_toolchains() line to go_register_toolchains(go_version=\"host\") as documented at: https://github.com/bazelbuild/rules_go/blob/master/go/toolchains.rst#using-the-installed-go-sdk. This will force bazel to use the already installed go tools. CI with Buildbot \u00b6 Example buildbot config: factory.addStep(steps.Git(repourl='git://github.com/omussell/go_tests.git', mode='incremental')) factory.addStep(steps.ShellCommand(command=[\"go\", \"fix\"],)) factory.addStep(steps.ShellCommand(command=[\"go\", \"vet\"],)) factory.addStep(steps.ShellCommand(command=[\"go\", \"fmt\"],)) factory.addStep(steps.ShellCommand(command=[\"bazel\", \"run\", \"//:gazelle\"],)) factory.addStep(steps.ShellCommand(command=[\"bazel\", \"build\", \"//:go_tests\"],)) I needed to rebuild the buildbot jail because it was borked, and after rebuilding it I was surprised that bazel worked without any more configuration. I just needed to install the git, go and bazel packages and run the buildbot config as described above and it ran through and rebuilt everything from scratch. This is one of the major advantages of keeping the build files (WORKSPACE and BUILD.bazel) alongside the source code. I am sure that if desired, anyone with a bazel setup would be able to build this code as well and the outputs would be identical. Adding dependencies \u00b6 In order to have Bazel automatically build dependencies we need to make a some changes to the WORKSPACE file. I've extended the example program to pull in a library that generates fake data and prints a random name when invoked. package main import \"github.com/brianvoe/gofakeit\" import \"fmt\" func main() { gofakeit.Seed(0) fmt.Println(gofakeit.Name()) // fmt.Println(\"test\") } The following needs to be appended to the WORKSPACE file: load(\"@io_bazel_rules_go//go:def.bzl\", \"go_repository\") go_repository( name = \"com_github_brianvoe_gofakeit\", importpath = \"github.com/brianvoe/gofakeit\", commit = \"b0b2ecfdf447299dd6bcdef91001692fc349ce4c\", ) The go_repository rule is used when a dependency is required that does not have a BUILD.bzl file in their repo. Bazel Remote Cache \u00b6 When building with Bazel, by default you are connecting to a local Bazel server which runs the build. If multiple people are running the same builds, you are all independently having to build the whole thing from scratch every time. With a Remote Cache, some other storage service can cache parts of the build and artifacts which can then be reused by multiple people. This can be a plain HTTP server like NGINX or Google Cloud Storage. mkdir -p /var/cache/nginx chmod 777 /var/cache/nginx # nginx config: location / { root /var/cache/nginx; dav_methods PUT; create_full_put_path on; client_max_body_size 1G; allow all; } Then when running the Bazel build, add the --remote_cache=http://$ip:$port flag to the build parameter like bazel build --remote_cache=http://192.168.1.10:80 //...","title":"Building Go with Bazel"},{"location":"go-bazel/#building-go-with-bazel","text":"","title":"Building Go with Bazel"},{"location":"go-bazel/#handling-go-dependencies","text":"During development, you will often use go get to download libraries for import into the program which is useful for development but not so useful when building the finished product. Managing these dependencies over time is a hassle as they change frequently and can sometimes disappear entirely. The dep tool provides a way of automatically scanning your import statements and evaluating all of the dependencies. It create some files Gopkg.toml and Gopkg.lock which contain the location and latest Git SHA of your dependencies. dep is installed via: curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh Run dep init to create the initial files, then as your develop run dep ensure to update dependencies to the latest version. The dep tool also downloads a copy of all dependencies into a vendor folder at the root of your project. This provides a backup in case a dependency disappears and provides the facility for reproducible builds.","title":"Handling Go Dependencies"},{"location":"go-bazel/#bazel-gazelle","text":"With our dependencies being updated, we would also need to update the WORKSPACE file so that Bazel/Gazelle knows about them as well. Gazelle requires the location and git commit hash in order to pull down the correct dependencies, but this is laborious to update manually. Thankfully, we can run a command to have gazelle pull in all of the dependencies from the Gopkg.lock file and update the WORKSPACE file automatically. Bazel will then pull in all of the dependencies correctly without any manual intervention. gazelle update-repos -from_file Gopkg.lock As part of ongoing development, you would periodically run dep ensure followed by gazelle update-repos -from_file Gopkg.lock to keep all of the dependencies up to date and generate the new WORKSPACE file.","title":"Bazel / Gazelle"},{"location":"go-bazel/#packaging-go-applications","text":"Now that we've built the go application and its dependencies we now need to package it up to distribute across the infrastructure.","title":"Packaging Go Applications"},{"location":"go-bazel/#packaging-with-fpm","text":"The below command is an example of what we would want to run: fpm -s dir -t freebsd -n ~/go_test --version 1.0.0 --prefix /usr/local/bin go_tests But this has a few issues. Rather than putting the finished package into ~/go_test , it would be better in a dedicated directory like /var/packages or similar. The version number is hard coded which obviously isn't always going to be correct. You would want to instead have your CI tool set to only run the packaging command when a new tag/release is created, and then have the version number derived from the tag/release number. It also includes the --prefix flag to specify the path to prepend to any files in the package. This is required as when the package is installed/extracted, the files will be extracted to the full path as specified in the package. So in this instance the /usr/local/bin/go_tests file is extracted. For now, I'm getting by with the following command which will overwrite the finished package if it already exists. fpm -f -s dir -t freebsd -n ~/go_test --prefix /usr/local/bin go_tests","title":"Packaging with fpm"},{"location":"go-bazel/#building-go-programs-using-bazel","text":"Bazel is a build tool created by Google which operates similarly to their internal build tool, Blaze. It is primarily concerned with generating artifacts from compiled languages like C, C++, Go etc. pkg install -y bazel Bazel requires some files so that it knows what and where to build. As an example, we are going to compile a simple go program with no dependencies (literally print a single string to stdout). // ~/go/src/github.com/omussell/go_tests/main.go package main import \"fmt\" func main() { fmt.Println(\"test\") } A file called WORKSPACE should be created at the root of the directory. This is used by bazel to determine source code locations relative to the WORKSPACE file and differentiate other packages in the same directory. Then a BUILD.bazel file should also be created at the root of the directory.","title":"Building Go programs using Bazel"},{"location":"go-bazel/#gazelle","text":"Instead of creating BUILD files by hand, we can use the Gazelle tool to iterate over a go source tree and dynamically generate BUILD files. We can also let bazel itself run gazelle. Note that gazelle doesn't work without bash, and the gazelle.bash file has a hardcoded path to /bin/bash which of course is not available on FreeBSD by default. pkg install -y bash ln -s /usr/local/bin/bash /bin/bash In the WORKSPACE file: http_archive( name = \"io_bazel_rules_go\", url = \"https://github.com/bazelbuild/rules_go/releases/download/0.9.0/rules_go-0.9.0.tar.gz\", sha256 = \"4d8d6244320dd751590f9100cf39fd7a4b75cd901e1f3ffdfd6f048328883695\", ) http_archive( name = \"bazel_gazelle\", url = \"https://github.com/bazelbuild/bazel-gazelle/releases/download/0.9/bazel-gazelle-0.9.tar.gz\", sha256 = \"0103991d994db55b3b5d7b06336f8ae355739635e0c2379dea16b8213ea5a223\", ) load(\"@io_bazel_rules_go//go:def.bzl\", \"go_rules_dependencies\", \"go_register_toolchains\") go_rules_dependencies() go_register_toolchains(go_version=\"host\") load(\"@bazel_gazelle//:deps.bzl\", \"gazelle_dependencies\") gazelle_dependencies() In the BUILD.bazel file: load(\"@bazel_gazelle//:def.bzl\", \"gazelle\") gazelle( name = \"gazelle\", prefix = \"github.com/omussell/go_tests\", ) Then to run: bazel run //:gazelle bazel build //:go_tests A built binary should be output to the ~/.cache directory. Once a binary has been built once, Bazel will only build again if the source code changes. Otherwise, any subsequent runs just complete successfully extremely quickly. When attempting to use bazel in any capacity like bazel run ... or bazel build ... it would give the following error: ERROR: /root/.cache/bazel/_bazel_root/...285a1776/external/io_bazel_rules_go/ BUILD.bazel:7:1: every rule of type go_context_data implicitly depends upon the target '@go_sdk// :packages.txt', but this target could not be found because of: no such package '@go_sdk//': Unsupported operating system: freebsd ERROR: /root/.cache/bazel/_bazel_root/...1776/external/io_bazel_rules_go/ BUILD.bazel:7:1: every rule of type go_context_data implicitly depends upon the target '@go_sdk// :files', but this target could not be found because of: no such package '@go_sdk//': Unsupported operating system: freebsd ERROR: /root/.cache/bazel/_bazel_root/...5a1776/external/io_bazel_rules_go/ BUILD.bazel:7:1: every rule of type go_context_data implicitly depends upon the target '@go_sdk// :tools', but this target could not be found because of: no such package '@go_sdk//': Unsupported operating system: freebsd ERROR: Analysis of target '//:gazelle' failed; build aborted: no such package '@go_sdk//': Unsupported operating system: freebsd I think this is caused by bazel attempting to download and build go which isn't necessary as we've already installed via the package anyway. In the WORKSPACE file, change the go_register_toolchains() line to go_register_toolchains(go_version=\"host\") as documented at: https://github.com/bazelbuild/rules_go/blob/master/go/toolchains.rst#using-the-installed-go-sdk. This will force bazel to use the already installed go tools.","title":"Gazelle"},{"location":"go-bazel/#ci-with-buildbot","text":"Example buildbot config: factory.addStep(steps.Git(repourl='git://github.com/omussell/go_tests.git', mode='incremental')) factory.addStep(steps.ShellCommand(command=[\"go\", \"fix\"],)) factory.addStep(steps.ShellCommand(command=[\"go\", \"vet\"],)) factory.addStep(steps.ShellCommand(command=[\"go\", \"fmt\"],)) factory.addStep(steps.ShellCommand(command=[\"bazel\", \"run\", \"//:gazelle\"],)) factory.addStep(steps.ShellCommand(command=[\"bazel\", \"build\", \"//:go_tests\"],)) I needed to rebuild the buildbot jail because it was borked, and after rebuilding it I was surprised that bazel worked without any more configuration. I just needed to install the git, go and bazel packages and run the buildbot config as described above and it ran through and rebuilt everything from scratch. This is one of the major advantages of keeping the build files (WORKSPACE and BUILD.bazel) alongside the source code. I am sure that if desired, anyone with a bazel setup would be able to build this code as well and the outputs would be identical.","title":"CI with Buildbot"},{"location":"go-bazel/#adding-dependencies","text":"In order to have Bazel automatically build dependencies we need to make a some changes to the WORKSPACE file. I've extended the example program to pull in a library that generates fake data and prints a random name when invoked. package main import \"github.com/brianvoe/gofakeit\" import \"fmt\" func main() { gofakeit.Seed(0) fmt.Println(gofakeit.Name()) // fmt.Println(\"test\") } The following needs to be appended to the WORKSPACE file: load(\"@io_bazel_rules_go//go:def.bzl\", \"go_repository\") go_repository( name = \"com_github_brianvoe_gofakeit\", importpath = \"github.com/brianvoe/gofakeit\", commit = \"b0b2ecfdf447299dd6bcdef91001692fc349ce4c\", ) The go_repository rule is used when a dependency is required that does not have a BUILD.bzl file in their repo.","title":"Adding dependencies"},{"location":"go-bazel/#bazel-remote-cache","text":"When building with Bazel, by default you are connecting to a local Bazel server which runs the build. If multiple people are running the same builds, you are all independently having to build the whole thing from scratch every time. With a Remote Cache, some other storage service can cache parts of the build and artifacts which can then be reused by multiple people. This can be a plain HTTP server like NGINX or Google Cloud Storage. mkdir -p /var/cache/nginx chmod 777 /var/cache/nginx # nginx config: location / { root /var/cache/nginx; dav_methods PUT; create_full_put_path on; client_max_body_size 1G; allow all; } Then when running the Bazel build, add the --remote_cache=http://$ip:$port flag to the build parameter like bazel build --remote_cache=http://192.168.1.10:80 //...","title":"Bazel Remote Cache"},{"location":"jail-creation/","text":"Create a template dataset zfs create -o mountpoint=/usr/local/jails zroot/jails zfs create -p zroot/jails/template Download the base files into a new directory mkdir ~/jails fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/amd64/amd64/11.1-RELEASE/base.txz -o ~/jails Extract the base files into the template directory (mountpoint) tar -xf ~/jails/base.txz -C /usr/local/jails/template Copy the resolv.conf file from host to template so that we have working DNS resolution cp /etc/resolv.conf /usr/local/jails/template/etc/resolv.conf When finished, take a snapshot. Anything after the '@' symbol is the snapshot name. You can make changes to the template at any time, just make sure that you take another snapshot when you are finished and that any subsequently created jails use the new snapshot. zfs snapshot zroot/jails/template@1 New jails can then be created by cloning the snapshot of the template dataset zfs clone zroot/jails/template@1 zroot/jails/testjail Add the jails configuration to /etc/jail.conf # Global settings applied to all jails interface = \"re0\"; host.hostname = \"$name\"; ip4.addr = 192.168.1.$ip; path = \"/usr/local/jails/$name\"; exec.start = \"/bin/sh /etc/rc\"; exec.stop = \"/bin/sh /etc/rc.shutdown\"; exec.clean; mount.devfs; # Jail Definitions testjail { $ip = 15; } Run the jail jail -c testjail View running jails jls Login to the jail jexec testjail sh","title":"FreeBSD Jail Creation"},{"location":"lean/","text":"Lean \u00b6 A collection of notes about the Toyota way, Lean, Theory of Constraints etc. copied from Wikipedia and other sites 5S \u00b6 Seiri Sort Seiton Set in order Seso Shine Seiketsu Standardise Shitsuke Sustain Toyota Production System \u00b6 Goal \u00b6 Design out overburden (muri) and inconsistency (mura), and to eliminate waste (muda). Types of waste \u00b6 Overproduction Time on hand (waiting) Transportation Processing itself Excess inventory Movement Making defective products Underutilised workers Muda \u00b6 Waste Transport \u00b6 Moving products that are not actually required to perform processing Inventory \u00b6 All components, work in process and finished product not being processed Motion \u00b6 People or equipment moving or walking more than is required to perform the processing Waiting \u00b6 Waiting for the next production step Overproduction \u00b6 Production ahead of demand Over processing \u00b6 Resulting from poor tool or product design creating activity Defects \u00b6 The effort involved in inspecting for and fixing defects. Mura \u00b6 Unevenness Is avoided through just in time systems which are based on keeping little or no inventory. These systems supply the production process with the right part, at the right time, in the right amount, using first-in, first-out (FIFO) component flow. JIT systems create a pull system in which each sub-process withdraws its needs from the preceding sub-processes, and ultimately from an outside supplier. When a preceding process does not receive a request or withdrawal it does not make more parts. Muri \u00b6 Unreasonableness Muri can be avoided through standardized work. To achieve this a standard condition or output must be efined to assure effective judgment of quality. Concept \u00b6 Just-in-time \u00b6 Make only what is needed, only when it is needed and only in the amount that is needed Jidoka \u00b6 Autonomation, automation with a human touch If an abnormal situation arises, the machine stops and the worker will stop the production line. Autonomation aims to prevent the production of defective products, eliminate overproduction and focus attention on understanding the problems. Poka Yoke \u00b6 Any mechanism in a process that helps an equipment operator avoid (yokeru) mistakes (poka) and defects by preventing, correcting, or drawing attention to human errors as they occur Continuous Improvement \u00b6 We form a long term vision, then iteratively work towards it Kaizen \u00b6 We improve our business operations continuously, always driving for innovation and evolution Genchi Genbutsu \u00b6 Go to the source to find the facts to make correct decisions Heijunka \u00b6 Production levelling Reduce unevenness which in turn reduces waste. The goal is to produce intermediate goods at a constant rate so that further processing may also be carried out at a constant and predictable rate. The right process will produce the right results \u00b6 Create continuous process flow to bring problems to the surface Use the 'pull' system to avoid overproduction (Kanban) Level out the workload (Heijunka) Build a culture of stopping to fix problems, to get quality right from the start Standardised tasks are the foundation for continuous improvement and employee empowerment Use visual control so no problems are hidden Use only reliable, thoroughly tested technology that serves your people and processes Continuously solving root problems drives organisational learning \u00b6 Go and see for yourself to thoroughly understand the situation (Genchi Genbutsu) Make decisions slowly by consensus, thoroughly considering all options. Implement decisions rapidly (Nemawashi) Become a learning organisation through relentless reflection (Hansei) and continuous improvement (Kaizen) Andon \u00b6 The worker has the ability to stop production when a defect is found, and immediately call for assistance. Work is stopped until a solution is found. Stack light / Traffic light - Visual indicator of a machine state or process event Nemawashi \u00b6 An informal process of quietly laying the foundation for some proposed change or project, by talking to the people concerned, and gathering support and feedback. It is considered an important element in any major change, before any formal steps are taken and enables changes to be carried out with the consent of all sides. Obeya \u00b6 Large room During the product and process development, all individuals involved in managerial planning meet in a great room to speed communicationand decision making. Takt Time \u00b6 Describes the required product assembly duration that is needed to match the demand. Often confused with cycle time, takt time is a tool used to design work and it measures the average time interval between the start of production of one unit and the start of production of the next unit when items are produced sequentially. Theory of Constraints \u00b6 Organisations can be measured and controlled by variations on three measures: Inventory - The money that the system has invested in purchasing things which it intends to sell Operational expense - The money the system spends in order to turn inventory into throughput Throughput - The rate at which the system generates money through sales Constraints \u00b6 A constraint is anything that prevents the system from achieving its goal. There is always at least one, but at most only a few, at any given time. If a constraints throughput capacity is elevated to the point where it is no longer the systems limiting factor, this is said to 'break' the constraint. Buffers \u00b6 Buffers are placed before the governing constraint, thus ensuring that the constraint is never starved. Buffers are also placed behind the constraint to prevent downstream failure from blocking the constraints output. Buffers used in this way protect the constraint from variations in the rest of the system and should allow for normal variation of processing time and the occasional upset before and behind the constraint. With one constraint in the system, all other parts of the system must have sufficient capacity to keep up with the work at the constraint and to catch up if time was lost. Plant Types (VATI) \u00b6 The plant types specify the general flow of materials through a system V \u00b6 Flow is one to many, such as a plant that takes one raw material and can make many final products. The primary problem in V plants is 'robbing', where on operation (A) immediately after a diverging point 'steals' materials meant for the other operation (B). Once the material has been processed by A, it cannot come back and be run through B without significant rework. A \u00b6 Flow is many to one, such as where many sub-assemblies converge for a final assembly. The problem in A plants is synchronising the converging lines so that each supplies the final assembly point at the right time. T \u00b6 Flow is many to many, similar to I plant (or has multiple lines), which then splits into many assemblies. T plants suffer from both synchronisation problems of A plants and the robbing problems of V plants. I \u00b6 Material flows in a sequence of events in a straight line. The constraint is the slowest operation.","title":"Lean"},{"location":"lean/#lean","text":"A collection of notes about the Toyota way, Lean, Theory of Constraints etc. copied from Wikipedia and other sites","title":"Lean"},{"location":"lean/#5s","text":"Seiri Sort Seiton Set in order Seso Shine Seiketsu Standardise Shitsuke Sustain","title":"5S"},{"location":"lean/#toyota-production-system","text":"","title":"Toyota Production System"},{"location":"lean/#goal","text":"Design out overburden (muri) and inconsistency (mura), and to eliminate waste (muda).","title":"Goal"},{"location":"lean/#types-of-waste","text":"Overproduction Time on hand (waiting) Transportation Processing itself Excess inventory Movement Making defective products Underutilised workers","title":"Types of waste"},{"location":"lean/#muda","text":"Waste","title":"Muda"},{"location":"lean/#transport","text":"Moving products that are not actually required to perform processing","title":"Transport"},{"location":"lean/#inventory","text":"All components, work in process and finished product not being processed","title":"Inventory"},{"location":"lean/#motion","text":"People or equipment moving or walking more than is required to perform the processing","title":"Motion"},{"location":"lean/#waiting","text":"Waiting for the next production step","title":"Waiting"},{"location":"lean/#overproduction","text":"Production ahead of demand","title":"Overproduction"},{"location":"lean/#over-processing","text":"Resulting from poor tool or product design creating activity","title":"Over processing"},{"location":"lean/#defects","text":"The effort involved in inspecting for and fixing defects.","title":"Defects"},{"location":"lean/#mura","text":"Unevenness Is avoided through just in time systems which are based on keeping little or no inventory. These systems supply the production process with the right part, at the right time, in the right amount, using first-in, first-out (FIFO) component flow. JIT systems create a pull system in which each sub-process withdraws its needs from the preceding sub-processes, and ultimately from an outside supplier. When a preceding process does not receive a request or withdrawal it does not make more parts.","title":"Mura"},{"location":"lean/#muri","text":"Unreasonableness Muri can be avoided through standardized work. To achieve this a standard condition or output must be efined to assure effective judgment of quality.","title":"Muri"},{"location":"lean/#concept","text":"","title":"Concept"},{"location":"lean/#just-in-time","text":"Make only what is needed, only when it is needed and only in the amount that is needed","title":"Just-in-time"},{"location":"lean/#jidoka","text":"Autonomation, automation with a human touch If an abnormal situation arises, the machine stops and the worker will stop the production line. Autonomation aims to prevent the production of defective products, eliminate overproduction and focus attention on understanding the problems.","title":"Jidoka"},{"location":"lean/#poka-yoke","text":"Any mechanism in a process that helps an equipment operator avoid (yokeru) mistakes (poka) and defects by preventing, correcting, or drawing attention to human errors as they occur","title":"Poka Yoke"},{"location":"lean/#continuous-improvement","text":"We form a long term vision, then iteratively work towards it","title":"Continuous Improvement"},{"location":"lean/#kaizen","text":"We improve our business operations continuously, always driving for innovation and evolution","title":"Kaizen"},{"location":"lean/#genchi-genbutsu","text":"Go to the source to find the facts to make correct decisions","title":"Genchi Genbutsu"},{"location":"lean/#heijunka","text":"Production levelling Reduce unevenness which in turn reduces waste. The goal is to produce intermediate goods at a constant rate so that further processing may also be carried out at a constant and predictable rate.","title":"Heijunka"},{"location":"lean/#the-right-process-will-produce-the-right-results","text":"Create continuous process flow to bring problems to the surface Use the 'pull' system to avoid overproduction (Kanban) Level out the workload (Heijunka) Build a culture of stopping to fix problems, to get quality right from the start Standardised tasks are the foundation for continuous improvement and employee empowerment Use visual control so no problems are hidden Use only reliable, thoroughly tested technology that serves your people and processes","title":"The right process will produce the right results"},{"location":"lean/#continuously-solving-root-problems-drives-organisational-learning","text":"Go and see for yourself to thoroughly understand the situation (Genchi Genbutsu) Make decisions slowly by consensus, thoroughly considering all options. Implement decisions rapidly (Nemawashi) Become a learning organisation through relentless reflection (Hansei) and continuous improvement (Kaizen)","title":"Continuously solving root problems drives organisational learning"},{"location":"lean/#andon","text":"The worker has the ability to stop production when a defect is found, and immediately call for assistance. Work is stopped until a solution is found. Stack light / Traffic light - Visual indicator of a machine state or process event","title":"Andon"},{"location":"lean/#nemawashi","text":"An informal process of quietly laying the foundation for some proposed change or project, by talking to the people concerned, and gathering support and feedback. It is considered an important element in any major change, before any formal steps are taken and enables changes to be carried out with the consent of all sides.","title":"Nemawashi"},{"location":"lean/#obeya","text":"Large room During the product and process development, all individuals involved in managerial planning meet in a great room to speed communicationand decision making.","title":"Obeya"},{"location":"lean/#takt-time","text":"Describes the required product assembly duration that is needed to match the demand. Often confused with cycle time, takt time is a tool used to design work and it measures the average time interval between the start of production of one unit and the start of production of the next unit when items are produced sequentially.","title":"Takt Time"},{"location":"lean/#theory-of-constraints","text":"Organisations can be measured and controlled by variations on three measures: Inventory - The money that the system has invested in purchasing things which it intends to sell Operational expense - The money the system spends in order to turn inventory into throughput Throughput - The rate at which the system generates money through sales","title":"Theory of Constraints"},{"location":"lean/#constraints","text":"A constraint is anything that prevents the system from achieving its goal. There is always at least one, but at most only a few, at any given time. If a constraints throughput capacity is elevated to the point where it is no longer the systems limiting factor, this is said to 'break' the constraint.","title":"Constraints"},{"location":"lean/#buffers","text":"Buffers are placed before the governing constraint, thus ensuring that the constraint is never starved. Buffers are also placed behind the constraint to prevent downstream failure from blocking the constraints output. Buffers used in this way protect the constraint from variations in the rest of the system and should allow for normal variation of processing time and the occasional upset before and behind the constraint. With one constraint in the system, all other parts of the system must have sufficient capacity to keep up with the work at the constraint and to catch up if time was lost.","title":"Buffers"},{"location":"lean/#plant-types-vati","text":"The plant types specify the general flow of materials through a system","title":"Plant Types (VATI)"},{"location":"lean/#v","text":"Flow is one to many, such as a plant that takes one raw material and can make many final products. The primary problem in V plants is 'robbing', where on operation (A) immediately after a diverging point 'steals' materials meant for the other operation (B). Once the material has been processed by A, it cannot come back and be run through B without significant rework.","title":"V"},{"location":"lean/#a","text":"Flow is many to one, such as where many sub-assemblies converge for a final assembly. The problem in A plants is synchronising the converging lines so that each supplies the final assembly point at the right time.","title":"A"},{"location":"lean/#t","text":"Flow is many to many, similar to I plant (or has multiple lines), which then splits into many assemblies. T plants suffer from both synchronisation problems of A plants and the robbing problems of V plants.","title":"T"},{"location":"lean/#i","text":"Material flows in a sequence of events in a straight line. The constraint is the slowest operation.","title":"I"},{"location":"lxc/","text":"LXC/LXD Containers \u00b6 You should have either a blank disk or an existing zpool for storage. Run lxd init , answer the questions with Yes for the most part. Enter either the disk name like /dev/sdb or the zpool name tank when prompted. Once complete, you can start up an Alpine container with lxc launch images:alpine/3.12 alpinecontainer or a Ubuntu container with lxc launch ubuntu:20.04 ubuntucontainer You can then connect to the container with lxc exec alpinecontainer -- /bin/sh or lxc exec ubuntucontainer -- /bin/bash","title":"LXC/LXD Containers"},{"location":"lxc/#lxclxd-containers","text":"You should have either a blank disk or an existing zpool for storage. Run lxd init , answer the questions with Yes for the most part. Enter either the disk name like /dev/sdb or the zpool name tank when prompted. Once complete, you can start up an Alpine container with lxc launch images:alpine/3.12 alpinecontainer or a Ubuntu container with lxc launch ubuntu:20.04 ubuntucontainer You can then connect to the container with lxc exec alpinecontainer -- /bin/sh or lxc exec ubuntucontainer -- /bin/bash","title":"LXC/LXD Containers"},{"location":"microserver/","text":"NAS on HP Microserver Gen8 \u00b6 Hardware specs \u00b6 HP ProLiant G8 Microserver G1610T Intel Celeron G1610T (dual core 2.3 GHz) 16GB RAM 2 x 250GB SSD 2 x 3TB HDD Summary \u00b6 I previously ran FreeNAS on this Microserver, but that was installed about 6 years ago so its very out of date. I want to use this as a NAS, but Im not too bothered about running a specific NAS OS like FreeNAS/TrueNAS etc. So my plan is to install Ubuntu 20.04 (current latest LTS) onto a USB disk, then have the disks set up in zpools with ZFS. Setup \u00b6 Whenever you search the internet for installing Ubuntu onto a USB disk it assumes you want to use it as a LiveCD from which to install Ubuntu onto the HDDs. I initially tried installing onto a USB stick by using two sticks, one for the initial boot which is placed in the USB jack inside the case, then another blank one inserted in the USB jack on the front of the case. However for whatever reason, the subsequent USB stick didnt boot. I think just a dodgy stick. So instead I did the same thing of booting from a USB stick inside the case, but then inserted a micro-sd card into the slot inside the case. I then selected that SD card as the disk to install to. In order to boot from this SD card, you need to press F9 during boot to enter the system setup. Then, I cant remember which specific option, but one of them has a list of options for booting from USB sticks which says like \"Boot from internal USB drive first\", \"Boot from internal SD card first\". You need to select the \"Boot from internal SD first\" option. Then continue boot, and it should boot correctly. ZFS \u00b6 You need to install the zfsutils-linux package to manage zpools. I set up the disks so that the two SSDs were in one pool, just striped, no mirror. Then the two HDDs were in another pool, mirrored. This results in two zpools, one with 500GB and no redundancy plus one with 3TB and redundancy. # Amend device names as appropriate # SSD zpool zpool create SSD_storage /dev/sdb /dev/sdc # HDD zpool zpool create HDD_storage mirror /dev/sdd /dev/sde","title":"Microserver NAS"},{"location":"microserver/#nas-on-hp-microserver-gen8","text":"","title":"NAS on HP Microserver Gen8"},{"location":"microserver/#hardware-specs","text":"HP ProLiant G8 Microserver G1610T Intel Celeron G1610T (dual core 2.3 GHz) 16GB RAM 2 x 250GB SSD 2 x 3TB HDD","title":"Hardware specs"},{"location":"microserver/#summary","text":"I previously ran FreeNAS on this Microserver, but that was installed about 6 years ago so its very out of date. I want to use this as a NAS, but Im not too bothered about running a specific NAS OS like FreeNAS/TrueNAS etc. So my plan is to install Ubuntu 20.04 (current latest LTS) onto a USB disk, then have the disks set up in zpools with ZFS.","title":"Summary"},{"location":"microserver/#setup","text":"Whenever you search the internet for installing Ubuntu onto a USB disk it assumes you want to use it as a LiveCD from which to install Ubuntu onto the HDDs. I initially tried installing onto a USB stick by using two sticks, one for the initial boot which is placed in the USB jack inside the case, then another blank one inserted in the USB jack on the front of the case. However for whatever reason, the subsequent USB stick didnt boot. I think just a dodgy stick. So instead I did the same thing of booting from a USB stick inside the case, but then inserted a micro-sd card into the slot inside the case. I then selected that SD card as the disk to install to. In order to boot from this SD card, you need to press F9 during boot to enter the system setup. Then, I cant remember which specific option, but one of them has a list of options for booting from USB sticks which says like \"Boot from internal USB drive first\", \"Boot from internal SD card first\". You need to select the \"Boot from internal SD first\" option. Then continue boot, and it should boot correctly.","title":"Setup"},{"location":"microserver/#zfs","text":"You need to install the zfsutils-linux package to manage zpools. I set up the disks so that the two SSDs were in one pool, just striped, no mirror. Then the two HDDs were in another pool, mirrored. This results in two zpools, one with 500GB and no redundancy plus one with 3TB and redundancy. # Amend device names as appropriate # SSD zpool zpool create SSD_storage /dev/sdb /dev/sdc # HDD zpool zpool create HDD_storage mirror /dev/sdd /dev/sde","title":"ZFS"},{"location":"new-prog-lang/","text":"Finding a New Programming Language \u00b6 In my journey at work of learning how to program in Python, I've become increasingly annoyed by some of its behaviours. Initially creating a new project is difficult. Build a virtualenv, activate it, pip install the requirements. Each of these has their own problems. It also ends up making it hard to deploy projects because you need to do these steps wherever you want to use your project. The performance is poor because its interpreted. If you want to make things faster, you can link to C code, but then you have to write C code. The package ecosystem is a mess. Packages frequently break or change their dependencies. New updates happen frequently and bring backwards incompatible changes with them. So you end up with some packages that won't work on newer Python versions, and some packages that only work on new Python versions. You end up stuck in limbo. Popular libraries like requests and flask are mature but the way you use the libraries are similar to Python 2 era code. Newer libraries like FastAPI are nicer to use, but they arent stable and for some reason use async. So now every project has to decide between mature and old code, or immature and async code. Types hints are a pain. They arent evaluated unless you use a static checker like mypy or a validator like pydantic. You might put loads of effort into maintaining types but ultimately Python is dynamically typed and mypy will miss things so the types are wrong. Python seems to work fine for small or short lived projects. But if you want a project to last at least a couple of years, it ends up being painful. My requirements for a new language are: Compiled - Interpreted is inherently slower Statically typed - Dynamic typing is great but makes it harder in the long run Easy to install packages and pin their version Easy to deploy Good ecosystem - Web servers, ORM, Database connectors So far I have tried Go, Rust and even Ada. Somehow, I came across Nim, and it fits everything that I want. The syntax is similar enough to Python that it doesnt feel like a chore to learn like the other languages. It also compiles down to a single binary making deployment easy. It uses a C compiler so the resulting binary can be small and compile anywhere that supports C. The problem is, the ecosystem isnt there yet. Even basic things like, how to write tests, isnt documented clearly. I think it just needs some more popularity which would sort out the low hanging fruit. For now, I've decided to try Go. I dont like that its got policitical messages on the websites. I also dont like that they enforce CoC to permaban people. But I dont have much choice. Otherwise I'd have to use something like C# or Java, backed by Microsoft and Oracle, which are just as bad. At least Go compiles quickly. Go has a lot of stuff baked into the stdlib like DNS, HTTP, crypto, file operations etc. which I think should be a standard nowadays. You can get very far without ever having to import other packages. I tried Go while it was still young and endured the problems with importing dependencies. I set up Bazel with Gazelle before it was well documented. I used Hugo before it was popular and had lots of themes. So I think if I were starting a project now I would: - Use Go - Have Bazel set up on a remote server and use remote caching to perform quick builds during the develop/debug phase - Build the final app in Docker - Deploy to Kubernetes","title":"Find a New Programming Language"},{"location":"new-prog-lang/#finding-a-new-programming-language","text":"In my journey at work of learning how to program in Python, I've become increasingly annoyed by some of its behaviours. Initially creating a new project is difficult. Build a virtualenv, activate it, pip install the requirements. Each of these has their own problems. It also ends up making it hard to deploy projects because you need to do these steps wherever you want to use your project. The performance is poor because its interpreted. If you want to make things faster, you can link to C code, but then you have to write C code. The package ecosystem is a mess. Packages frequently break or change their dependencies. New updates happen frequently and bring backwards incompatible changes with them. So you end up with some packages that won't work on newer Python versions, and some packages that only work on new Python versions. You end up stuck in limbo. Popular libraries like requests and flask are mature but the way you use the libraries are similar to Python 2 era code. Newer libraries like FastAPI are nicer to use, but they arent stable and for some reason use async. So now every project has to decide between mature and old code, or immature and async code. Types hints are a pain. They arent evaluated unless you use a static checker like mypy or a validator like pydantic. You might put loads of effort into maintaining types but ultimately Python is dynamically typed and mypy will miss things so the types are wrong. Python seems to work fine for small or short lived projects. But if you want a project to last at least a couple of years, it ends up being painful. My requirements for a new language are: Compiled - Interpreted is inherently slower Statically typed - Dynamic typing is great but makes it harder in the long run Easy to install packages and pin their version Easy to deploy Good ecosystem - Web servers, ORM, Database connectors So far I have tried Go, Rust and even Ada. Somehow, I came across Nim, and it fits everything that I want. The syntax is similar enough to Python that it doesnt feel like a chore to learn like the other languages. It also compiles down to a single binary making deployment easy. It uses a C compiler so the resulting binary can be small and compile anywhere that supports C. The problem is, the ecosystem isnt there yet. Even basic things like, how to write tests, isnt documented clearly. I think it just needs some more popularity which would sort out the low hanging fruit. For now, I've decided to try Go. I dont like that its got policitical messages on the websites. I also dont like that they enforce CoC to permaban people. But I dont have much choice. Otherwise I'd have to use something like C# or Java, backed by Microsoft and Oracle, which are just as bad. At least Go compiles quickly. Go has a lot of stuff baked into the stdlib like DNS, HTTP, crypto, file operations etc. which I think should be a standard nowadays. You can get very far without ever having to import other packages. I tried Go while it was still young and endured the problems with importing dependencies. I set up Bazel with Gazelle before it was well documented. I used Hugo before it was popular and had lots of themes. So I think if I were starting a project now I would: - Use Go - Have Bazel set up on a remote server and use remote caching to perform quick builds during the develop/debug phase - Build the final app in Docker - Deploy to Kubernetes","title":"Finding a New Programming Language"},{"location":"nginx-chacha20/","text":"Compiling NGINX with ChaCha20 support \u00b6 Make a working directory mkdir ~/nginx cd ~/nginx Install some dependencies pkg install -y ca_root_nss pcre perl5 Pull the source files fetch https://nginx.org/download/nginx-1.13.0.tar.gz fetch https://www.openssl.org/source/openssl-1.1.0e.tar.gz Extract the tarballs tar -xzvf nginx-1.13.0.tar.gz tar -xzvf openssl-1.1.0e.tar.gz rm *.tar.gz Compile openssl cd ~/nginx/openssl-1.1.0e.tar.gz ./config make make install The compiled OpenSSL binary should be located in /usr/local/bin by default, unless the prefixdir variable has been set /usr/local/bin/openssl version # Should output OpenSSL 1.1.0e Compile NGINX #!/bin/sh cd ~/nginx/nginx-1.13.0/ #make clean ./configure \\ --with-http_ssl_module \\ --with-http_gzip_static_module \\ --with-file-aio \\ --with-ld-opt=\"-L /usr/local/lib\" \\ --without-http_browser_module \\ --without-http_fastcgi_module \\ --without-http_geo_module \\ --without-http_map_module \\ --without-http_proxy_module \\ --without-http_memcached_module \\ --without-http_ssi_module \\ --without-http_userid_module \\ --without-http_split_clients_module \\ --without-http_uwsgi_module \\ --without-http_scgi_module \\ --without-http_limit_conn_module \\ --without-http_referer_module \\ --without-http_http-cache \\ --without_upstream_ip_hash_module \\ --without-mail_pop3_module \\ --without-mail-imap_module \\ --without-mail_smtp_module --with-openssl=~/nginx/openssl-1.1.0e/ make make install After running the compile script, NGINX should be installed in /usr/local/nginx Start the service /usr/local/nginx/sbin/nginx If there are no issues, update the config file as appropriate in /usr/local/nginx/conf/nginx.conf Reload NGINX to apply the new config /usr/local/nginx/sbin/nginx -s reload Generate a self-signed certificate Current NGINX config worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root /usr/local/www/; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } server { listen 443 ssl; server_name localhost; ssl on; #ssl_certificate /root/nginx/server.pem; #ssl_certificate_key /root/nginx/private.pem; ssl_certificate /usr/local/www/nginx-selfsigned.crt; ssl_certificate_key /usr/local/www/nginx-selfsigned.key; ssl_ciphers \"ECDHE-RSA-CHACHA20-POLY1305\"; ssl_prefer_server_ciphers on; ssl_protocols TLSv1.2; ssl_ecdh_curve X25519; location / { root /usr/local/www/; index index.html index.htm; } } }","title":"Compiling NGINX with ChaCha20 support"},{"location":"nginx-chacha20/#compiling-nginx-with-chacha20-support","text":"Make a working directory mkdir ~/nginx cd ~/nginx Install some dependencies pkg install -y ca_root_nss pcre perl5 Pull the source files fetch https://nginx.org/download/nginx-1.13.0.tar.gz fetch https://www.openssl.org/source/openssl-1.1.0e.tar.gz Extract the tarballs tar -xzvf nginx-1.13.0.tar.gz tar -xzvf openssl-1.1.0e.tar.gz rm *.tar.gz Compile openssl cd ~/nginx/openssl-1.1.0e.tar.gz ./config make make install The compiled OpenSSL binary should be located in /usr/local/bin by default, unless the prefixdir variable has been set /usr/local/bin/openssl version # Should output OpenSSL 1.1.0e Compile NGINX #!/bin/sh cd ~/nginx/nginx-1.13.0/ #make clean ./configure \\ --with-http_ssl_module \\ --with-http_gzip_static_module \\ --with-file-aio \\ --with-ld-opt=\"-L /usr/local/lib\" \\ --without-http_browser_module \\ --without-http_fastcgi_module \\ --without-http_geo_module \\ --without-http_map_module \\ --without-http_proxy_module \\ --without-http_memcached_module \\ --without-http_ssi_module \\ --without-http_userid_module \\ --without-http_split_clients_module \\ --without-http_uwsgi_module \\ --without-http_scgi_module \\ --without-http_limit_conn_module \\ --without-http_referer_module \\ --without-http_http-cache \\ --without_upstream_ip_hash_module \\ --without-mail_pop3_module \\ --without-mail-imap_module \\ --without-mail_smtp_module --with-openssl=~/nginx/openssl-1.1.0e/ make make install After running the compile script, NGINX should be installed in /usr/local/nginx Start the service /usr/local/nginx/sbin/nginx If there are no issues, update the config file as appropriate in /usr/local/nginx/conf/nginx.conf Reload NGINX to apply the new config /usr/local/nginx/sbin/nginx -s reload Generate a self-signed certificate Current NGINX config worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root /usr/local/www/; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } server { listen 443 ssl; server_name localhost; ssl on; #ssl_certificate /root/nginx/server.pem; #ssl_certificate_key /root/nginx/private.pem; ssl_certificate /usr/local/www/nginx-selfsigned.crt; ssl_certificate_key /usr/local/www/nginx-selfsigned.key; ssl_ciphers \"ECDHE-RSA-CHACHA20-POLY1305\"; ssl_prefer_server_ciphers on; ssl_protocols TLSv1.2; ssl_ecdh_curve X25519; location / { root /usr/local/www/; index index.html index.htm; } } }","title":"Compiling NGINX with ChaCha20 support"},{"location":"nginx/","text":"NGINX \u00b6 TLS 1.3 0-RTT with NGINX \u00b6 NGINX Docs Early data var ssl_early_data on; proxy_set_header Early-Data $ssl_early_data; limit_except GET { deny all; } 0-RTT is vulnerable to replay attacks, so we should only use this with requests using the GET method. If passing the request to a backend, you can set a header with proxy_set_header Early-Data $ssl_early_data; . The value of the $ssl_early_data variable is \"1\" if early data is used, otherwise \"\". This header is passed to the upstream, so it can be used by the upstream application to determine the response. Only allow certain HTTP methods with NGINX \u00b6 NGNX Docs limit_except GET { deny all; } Only allows GET requests through, denies all other methods, with the exception of HEAD because if GET is allowed HEAD is too. Dynamic Certificate loading with NGINX \u00b6 NGINX Announcement NGINX Docs If you have a lot of NGINX servers/vhosts all served from the same box, you probably want to secure them with TLS. Normally this would mean a lot of duplicate configuration to specify which certificate is needed for each server_name. With Dynamic Certificate Loading, you can use a NGINX variable as part of the certificate name. So if you have certificate/key files named after the server name, you can load them dynamically with NGINX. server_name omuss.net omuss-test.net; ssl_certificate /usr/local/etc/nginx/ssl/$ssl_server_name.crt; ssl_certificate_key /usr/local/etc/nginx/ssl/$ssl_server_name.key; With certificate and key files named appropriately: /usr/local/etc/nginx/ssl/omuss.net.crt /usr/local/etc/nginx/ssl/omuss.net.key /usr/local/etc/nginx/ssl/omuss-test.net.crt /usr/local/etc/nginx/ssl/omuss-test.net.key Note that certificates are lazy loaded, as in they are only loaded when a request comes in. So all certificates aren't loaded into memory, which means less resource usage, but there is some overhead for the TLS negotiation because NGINX has to load the certificate from disk. TLS session caching may help alleviate this though. You would probably want the certificates stored on a fast disk to eliminate I/O overhead. Brotli Compression with NGINX \u00b6 Brotli can be used as an alternative to GZIP. It can give better compression in some cases. NGINX Brotli Docs Module Docs The normal nginx package does not include the brotli module. You can either compile NGINX yourself and include the Brotli module, or otherwise install the nginx-full package (though the package is big because of lots of dependencies and includes lots of other modules). Once you have a NGINX binary with the Brotli module included, you need to load the module in the NGINX configuration: load_module /usr/local/libexec/nginx/ngx_http_brotli_static_module.so; load_module /usr/local/libexec/nginx/ngx_http_brotli_filter_module.so; Also an important note, you MUST use HTTPS for Brotli to work. So make sure you set a server block to use HTTPS and set up a certificate etc. Now you have two options, compress you static files manually and put them where NGINX can find them, or let NGINX compress them on-the-fly. Static \u00b6 With brotli_static set to on or always , the files must already be compressed. This can be done by installing the brotli package on FreeBSD, or otherwise you can do it quick and dirty with python like: # pip install brotli import brotli with open('index.html', 'rb') as f: with open('index.html.br', 'wb') as brotted: brotted.write(brotli.compress(f.read())) Note that brotli prefers bytestrings. With the brotli_static option turned on, I found that using index.html.br didn't work, but if I set the filename to index.html but with Brotli-fied contents, it loaded correctly. You should also make sure to set add_header Content-Encoding \"br\"; so that the browser knows that it is Brotli encoded. Dynamic \u00b6 Otherwise, set brotli on; and it will compress file on-the-fly. NGINX TCP/UDP proxy \u00b6 NGINX needs to be compiled with the --with-stream option. It can't be dynamic, which is the default. In the config file you need to add: load_module /usr/local/libexec/nginx/ngx_stream_module.so; Then in the config file: stream { server { listen 80; proxy_pass 192.168.1.15:80; } server { # Override the default stream type of TCP with UDP listen 53; proxy_pass 192.168.1.15:53 udp; } }","title":"NGINX"},{"location":"nginx/#nginx","text":"","title":"NGINX"},{"location":"nginx/#tls-13-0-rtt-with-nginx","text":"NGINX Docs Early data var ssl_early_data on; proxy_set_header Early-Data $ssl_early_data; limit_except GET { deny all; } 0-RTT is vulnerable to replay attacks, so we should only use this with requests using the GET method. If passing the request to a backend, you can set a header with proxy_set_header Early-Data $ssl_early_data; . The value of the $ssl_early_data variable is \"1\" if early data is used, otherwise \"\". This header is passed to the upstream, so it can be used by the upstream application to determine the response.","title":"TLS 1.3 0-RTT with NGINX"},{"location":"nginx/#only-allow-certain-http-methods-with-nginx","text":"NGNX Docs limit_except GET { deny all; } Only allows GET requests through, denies all other methods, with the exception of HEAD because if GET is allowed HEAD is too.","title":"Only allow certain HTTP methods with NGINX"},{"location":"nginx/#dynamic-certificate-loading-with-nginx","text":"NGINX Announcement NGINX Docs If you have a lot of NGINX servers/vhosts all served from the same box, you probably want to secure them with TLS. Normally this would mean a lot of duplicate configuration to specify which certificate is needed for each server_name. With Dynamic Certificate Loading, you can use a NGINX variable as part of the certificate name. So if you have certificate/key files named after the server name, you can load them dynamically with NGINX. server_name omuss.net omuss-test.net; ssl_certificate /usr/local/etc/nginx/ssl/$ssl_server_name.crt; ssl_certificate_key /usr/local/etc/nginx/ssl/$ssl_server_name.key; With certificate and key files named appropriately: /usr/local/etc/nginx/ssl/omuss.net.crt /usr/local/etc/nginx/ssl/omuss.net.key /usr/local/etc/nginx/ssl/omuss-test.net.crt /usr/local/etc/nginx/ssl/omuss-test.net.key Note that certificates are lazy loaded, as in they are only loaded when a request comes in. So all certificates aren't loaded into memory, which means less resource usage, but there is some overhead for the TLS negotiation because NGINX has to load the certificate from disk. TLS session caching may help alleviate this though. You would probably want the certificates stored on a fast disk to eliminate I/O overhead.","title":"Dynamic Certificate loading with NGINX"},{"location":"nginx/#brotli-compression-with-nginx","text":"Brotli can be used as an alternative to GZIP. It can give better compression in some cases. NGINX Brotli Docs Module Docs The normal nginx package does not include the brotli module. You can either compile NGINX yourself and include the Brotli module, or otherwise install the nginx-full package (though the package is big because of lots of dependencies and includes lots of other modules). Once you have a NGINX binary with the Brotli module included, you need to load the module in the NGINX configuration: load_module /usr/local/libexec/nginx/ngx_http_brotli_static_module.so; load_module /usr/local/libexec/nginx/ngx_http_brotli_filter_module.so; Also an important note, you MUST use HTTPS for Brotli to work. So make sure you set a server block to use HTTPS and set up a certificate etc. Now you have two options, compress you static files manually and put them where NGINX can find them, or let NGINX compress them on-the-fly.","title":"Brotli Compression with NGINX"},{"location":"nginx/#static","text":"With brotli_static set to on or always , the files must already be compressed. This can be done by installing the brotli package on FreeBSD, or otherwise you can do it quick and dirty with python like: # pip install brotli import brotli with open('index.html', 'rb') as f: with open('index.html.br', 'wb') as brotted: brotted.write(brotli.compress(f.read())) Note that brotli prefers bytestrings. With the brotli_static option turned on, I found that using index.html.br didn't work, but if I set the filename to index.html but with Brotli-fied contents, it loaded correctly. You should also make sure to set add_header Content-Encoding \"br\"; so that the browser knows that it is Brotli encoded.","title":"Static"},{"location":"nginx/#dynamic","text":"Otherwise, set brotli on; and it will compress file on-the-fly.","title":"Dynamic"},{"location":"nginx/#nginx-tcpudp-proxy","text":"NGINX needs to be compiled with the --with-stream option. It can't be dynamic, which is the default. In the config file you need to add: load_module /usr/local/libexec/nginx/ngx_stream_module.so; Then in the config file: stream { server { listen 80; proxy_pass 192.168.1.15:80; } server { # Override the default stream type of TCP with UDP listen 53; proxy_pass 192.168.1.15:53 udp; } }","title":"NGINX TCP/UDP proxy"},{"location":"nsd-unbound/","text":"NSD and Unbound config \u00b6 Set up the unbound/nsd-control local-unbound-setup nsd-control-setup Enable NSD and Unbound to start in /etc/rc.conf sysrc nsd_enable=\"YES\" sysrc local_unbound_enable=\"YES\" Set a different listening port for NSD in /usr/local/etc/nsd.conf server: port: 5353 Create an inital zone file /usr/local/etc/nsd/home.lan.zone $ORIGIN home.lan. ; $TTL 86400 ; @ IN SOA ns1.home.lan. admin.home.lan. ( 2017080619 ; 28800 ; 7200 ; 864000 ; 86400 ; ) NS ns1.home.lan. ns1 IN A 192.168.1.15 jail IN A 192.168.1.15 Create the reverse lookup zone file /usr/local/etc/nsd/home.lan.reverse $ORIGIN home.lan. $TTL 86400 0.1.168.192.in-addr.arpa. IN SOA ns1.home.lan. admin.home.lan. ( 2017080619 28800 7200 864000 86400 ) NS ns1.home.lan. 15.1.168.192.in-addr.arpa. IN PTR jail 15.1.168.192.in-addr.arpa. IN PTR ns1 OpenDNSSEC \u00b6 Install the required packages pkg install -y opendnssec softhsm Set the softhsm database location in /usr/local/etc/softhsm.conf 0:/var/lib/softhsm/slot0.db Initialise the token database: softhsm --init-token --slot 0 --label \"OpenDNSSEC\" Enter the PIN for the SO and then the USER. Make sure opendnssec has permission to access the token database chown opendnssec /var/lib/softhsm/slot0.db chgrp opendnssec /var/lib/softhsm/slot0.db Set some options for OpenDNSSEC in /usr/local/etc/opendnssec/conf.xml <Repository name=\"SoftHSM\"> <Module>/usr/local/lib/softhsm/libsofthsm.so</Module> <TokenLabel>OpenDNSSEC</TokenLabel> <PIN>1234</PIN> <SkipPublicKey/> </Repository> Edit /usr/local/etc/opendnssec/kasp.xml . Change unixtime to datecounter in the Serial parameter. This allows us to use YYYYMMDDXX format for the SOA SERIAL values. <Zone> <PropagationDelay>PT300S</PropagationDelay> <SOA> <TTL>PT300S</TTL> <Minimum>PT300S</Minimum> <Serial>datecounter</Serial> </SOA> </Zone>","title":"NSD and Unbound config"},{"location":"nsd-unbound/#nsd-and-unbound-config","text":"Set up the unbound/nsd-control local-unbound-setup nsd-control-setup Enable NSD and Unbound to start in /etc/rc.conf sysrc nsd_enable=\"YES\" sysrc local_unbound_enable=\"YES\" Set a different listening port for NSD in /usr/local/etc/nsd.conf server: port: 5353 Create an inital zone file /usr/local/etc/nsd/home.lan.zone $ORIGIN home.lan. ; $TTL 86400 ; @ IN SOA ns1.home.lan. admin.home.lan. ( 2017080619 ; 28800 ; 7200 ; 864000 ; 86400 ; ) NS ns1.home.lan. ns1 IN A 192.168.1.15 jail IN A 192.168.1.15 Create the reverse lookup zone file /usr/local/etc/nsd/home.lan.reverse $ORIGIN home.lan. $TTL 86400 0.1.168.192.in-addr.arpa. IN SOA ns1.home.lan. admin.home.lan. ( 2017080619 28800 7200 864000 86400 ) NS ns1.home.lan. 15.1.168.192.in-addr.arpa. IN PTR jail 15.1.168.192.in-addr.arpa. IN PTR ns1","title":"NSD and Unbound config"},{"location":"nsd-unbound/#opendnssec","text":"Install the required packages pkg install -y opendnssec softhsm Set the softhsm database location in /usr/local/etc/softhsm.conf 0:/var/lib/softhsm/slot0.db Initialise the token database: softhsm --init-token --slot 0 --label \"OpenDNSSEC\" Enter the PIN for the SO and then the USER. Make sure opendnssec has permission to access the token database chown opendnssec /var/lib/softhsm/slot0.db chgrp opendnssec /var/lib/softhsm/slot0.db Set some options for OpenDNSSEC in /usr/local/etc/opendnssec/conf.xml <Repository name=\"SoftHSM\"> <Module>/usr/local/lib/softhsm/libsofthsm.so</Module> <TokenLabel>OpenDNSSEC</TokenLabel> <PIN>1234</PIN> <SkipPublicKey/> </Repository> Edit /usr/local/etc/opendnssec/kasp.xml . Change unixtime to datecounter in the Serial parameter. This allows us to use YYYYMMDDXX format for the SOA SERIAL values. <Zone> <PropagationDelay>PT300S</PropagationDelay> <SOA> <TTL>PT300S</TTL> <Minimum>PT300S</Minimum> <Serial>datecounter</Serial> </SOA> </Zone>","title":"OpenDNSSEC"},{"location":"pgsql-repl/","text":"PostgreSQL 10.1 with replication \u00b6 pkg install -y postgresql10-server postgresql10-client sysrc postgresql_enable=YES service postgresql initdb service postgresql start PostgreSQL 10.1 SCRAM Authentication \u00b6 su - postgres psql set password_encryption = 'scram-sha-256'; create role app_db with password 'foo'; select substring(rolpassword, 1, 14) from pg_authid where rolname = 'app_db'; PostgreSQL 10.1 using repmgr for database replication, WAL-G for WAL archiving, and minio for S3 compatible storage \u00b6 For this, I created two bhyve VMs to host postgresql and a jail on the host for minio Make sure postgresql is running Carry out the following steps on both primary and replicas The current packaged version of repmgr is 3.3.1 which isn't the latest. The latest is 4.0.1, so we need to compile it ourself, and put files into the correct locations fetch https://repmgr.org/download/repmgr-4.0.1.tar.gz tar -zvxf repmgr-4.0.1.tar.gz ./configure pkg install -y gmake gmake Copy the repmgr files to their correct locations cp -v repmgr /var/db/postgres cp -v repmgr--4.0.sql /usr/local/share/postgresql/extension/ cp -v repmgr.control /usr/local/share/postgresql/extension vim /var/db/postgrs/data10/postgresql.conf Add lines: include_dir = 'postgresql.conf.d' listen_addresses = '\\*' vim /var/db/postgres/data10/postgresql.conf.d/postgresql.replication.conf Add lines: max_wal_senders = 10 wal_level = 'replica' wal_keep_segments = 5000 hot_standby = on archive_mode = on archive_command = 'wal-g stuff here' vim /var/db/postgres/data10/pg_hba.conf Add lines: Please note, for testing purposes, these rules are wide open and allow everything. Dont do this in production, use a specific role with a password and restrict to a specific address local all all trust host all all 0.0.0.0/0 trust host replication all 0.0.0.0/0 trust vim /usr/local/etc/repmgr.conf Add lines: node_id=1 # arbitrary number, each node needs to be unique node_name=postgres-db1 # this nodes hostname conninfo='host=192.168.1.10 user=repmgr dbname=repmgr' # the host value should be a hostname if DNS is working On the primary su - postgres createuser -s repmgr createdb repmgr -O repmgr repmgr -f /usr/local/etc/repmgr.conf primary register repmgr -f /usr/local/etc/repmgr.conf cluster show On a standby su - postgres psql 'host=node1 user=repmgr dbname=repmgr' To clone the primary, the data directory on the standby node must exist but be empty rm -rf /var/db/postgres/data10/ mkdir -p /var/db/postgres/data10 chown postgres:postgres /var/db/postgres/data10 Dry run first to check for problems repmgr -h node1 -U repmgr -d repmgr -f /usr/local/etc/repmgr.conf standby clone --dry-run If its ok, run it repmgr -h node1 -U repmgr -d repmgr -f /usr/local/etc/repmgr.conf standby clone On the primary su - postgres psql -d repmgr select * from pg_stat_replication; On the standby repmgr -f /usr/local/etc/repmgr.conf standby register repmgr -f /usr/local/etc/repmgr.conf cluster show Install minio pkg install -y minio sysrc minio_enable=YES sysrc minio_disks=/home/user/test mkdir -p /home/user/test chown minio:minio /home/user/test service minio start # The access keys are in /usr/local/etc/minio/config.json # You can change them in this file and restart the service to take effect On the primary WAL-G pkg install -y go mkdir -p /root/go setenv GOPATH /root/go cd go go get github.com/wal-g/wal-g cd src/github.com/wal-g/wal-g make all make install cp /root/go/bin/wal-g /usr/local/bin WAL-G requires certain environment variables to be set. This can be done using envdir, part of the daemontools package pkg install -y daemontools Setup is now complete. For operations, a base backup needs to be taken on a regular basis probably via a cron job, running the following command as postgres user wal-g backup-push /var/db/postgres/data10 Then the archive_command in the postgresql.replication.conf should be set to the wal-push command wal-g wal-push /var/db/postgres/data10 To restore, backup-fetch and wal-fetch can be used to pull the latest base backup and the necessary wal logs to recover to the latest transaction","title":"PostgreSQL 10.1 with replication"},{"location":"pgsql-repl/#postgresql-101-with-replication","text":"pkg install -y postgresql10-server postgresql10-client sysrc postgresql_enable=YES service postgresql initdb service postgresql start","title":"PostgreSQL 10.1 with replication"},{"location":"pgsql-repl/#postgresql-101-scram-authentication","text":"su - postgres psql set password_encryption = 'scram-sha-256'; create role app_db with password 'foo'; select substring(rolpassword, 1, 14) from pg_authid where rolname = 'app_db';","title":"PostgreSQL 10.1 SCRAM Authentication"},{"location":"pgsql-repl/#postgresql-101-using-repmgr-for-database-replication-wal-g-for-wal-archiving-and-minio-for-s3-compatible-storage","text":"For this, I created two bhyve VMs to host postgresql and a jail on the host for minio Make sure postgresql is running Carry out the following steps on both primary and replicas The current packaged version of repmgr is 3.3.1 which isn't the latest. The latest is 4.0.1, so we need to compile it ourself, and put files into the correct locations fetch https://repmgr.org/download/repmgr-4.0.1.tar.gz tar -zvxf repmgr-4.0.1.tar.gz ./configure pkg install -y gmake gmake Copy the repmgr files to their correct locations cp -v repmgr /var/db/postgres cp -v repmgr--4.0.sql /usr/local/share/postgresql/extension/ cp -v repmgr.control /usr/local/share/postgresql/extension vim /var/db/postgrs/data10/postgresql.conf Add lines: include_dir = 'postgresql.conf.d' listen_addresses = '\\*' vim /var/db/postgres/data10/postgresql.conf.d/postgresql.replication.conf Add lines: max_wal_senders = 10 wal_level = 'replica' wal_keep_segments = 5000 hot_standby = on archive_mode = on archive_command = 'wal-g stuff here' vim /var/db/postgres/data10/pg_hba.conf Add lines: Please note, for testing purposes, these rules are wide open and allow everything. Dont do this in production, use a specific role with a password and restrict to a specific address local all all trust host all all 0.0.0.0/0 trust host replication all 0.0.0.0/0 trust vim /usr/local/etc/repmgr.conf Add lines: node_id=1 # arbitrary number, each node needs to be unique node_name=postgres-db1 # this nodes hostname conninfo='host=192.168.1.10 user=repmgr dbname=repmgr' # the host value should be a hostname if DNS is working On the primary su - postgres createuser -s repmgr createdb repmgr -O repmgr repmgr -f /usr/local/etc/repmgr.conf primary register repmgr -f /usr/local/etc/repmgr.conf cluster show On a standby su - postgres psql 'host=node1 user=repmgr dbname=repmgr' To clone the primary, the data directory on the standby node must exist but be empty rm -rf /var/db/postgres/data10/ mkdir -p /var/db/postgres/data10 chown postgres:postgres /var/db/postgres/data10 Dry run first to check for problems repmgr -h node1 -U repmgr -d repmgr -f /usr/local/etc/repmgr.conf standby clone --dry-run If its ok, run it repmgr -h node1 -U repmgr -d repmgr -f /usr/local/etc/repmgr.conf standby clone On the primary su - postgres psql -d repmgr select * from pg_stat_replication; On the standby repmgr -f /usr/local/etc/repmgr.conf standby register repmgr -f /usr/local/etc/repmgr.conf cluster show Install minio pkg install -y minio sysrc minio_enable=YES sysrc minio_disks=/home/user/test mkdir -p /home/user/test chown minio:minio /home/user/test service minio start # The access keys are in /usr/local/etc/minio/config.json # You can change them in this file and restart the service to take effect On the primary WAL-G pkg install -y go mkdir -p /root/go setenv GOPATH /root/go cd go go get github.com/wal-g/wal-g cd src/github.com/wal-g/wal-g make all make install cp /root/go/bin/wal-g /usr/local/bin WAL-G requires certain environment variables to be set. This can be done using envdir, part of the daemontools package pkg install -y daemontools Setup is now complete. For operations, a base backup needs to be taken on a regular basis probably via a cron job, running the following command as postgres user wal-g backup-push /var/db/postgres/data10 Then the archive_command in the postgresql.replication.conf should be set to the wal-push command wal-g wal-push /var/db/postgres/data10 To restore, backup-fetch and wal-fetch can be used to pull the latest base backup and the necessary wal logs to recover to the latest transaction","title":"PostgreSQL 10.1 using repmgr for database replication, WAL-G for WAL archiving, and minio for S3 compatible storage"},{"location":"pi-bitwarden/","text":"Running bitwarden_rs on a Raspberry Pi 4 \u00b6 Summary \u00b6 We will be setting up bitwarden_rs without Docker, by compiling it manually and then running as a service. In this example we are using SQLite, but you can change this to MySQL or PostgreSQL if you prefer. Install dependencies \u00b6 curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh # Answer Y when prompted apt install -y build-essential git pkg-config libssl-dev libsqlite3-dev Clone the repo \u00b6 git clone https://github.com/dani-garcia/bitwarden_rs.git cd bitwarden_rs Compile \u00b6 cargo build --features sqlite --release Admin \u00b6 After compilation, the built binary will be ./target/release/bitwarden_rs . This should be moved to /usr/bin with mv ./target/release/bitwarden_rs /usr/bin/bitwarden_rs The data directory needs to be created with mkdir -p /var/lib/bitwarden_rs/data . This is where the bitwarden keys and database are stored. Create a user account with adduser bitwarden_rs . Make sure the ownership of everything in /var/lib/bitwarden_rs is set to the bitwarden_rs user. Frontend \u00b6 Download the already built assets: cd /var/lib/bitwarden_rs # Amend the version as appropriate wget https://github.com/dani-garcia/bw_web_builds/releases/download/v2.17.1/bw_web_v2.17.1.tar.gz Extract them tar -xvf bw_web_v2.17.1.tar.gz Run \u00b6 Create the systemd service file. Copy the file from the wiki .","title":"Pi Bitwarden"},{"location":"pi-bitwarden/#running-bitwarden_rs-on-a-raspberry-pi-4","text":"","title":"Running bitwarden_rs on a Raspberry Pi 4"},{"location":"pi-bitwarden/#summary","text":"We will be setting up bitwarden_rs without Docker, by compiling it manually and then running as a service. In this example we are using SQLite, but you can change this to MySQL or PostgreSQL if you prefer.","title":"Summary"},{"location":"pi-bitwarden/#install-dependencies","text":"curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh # Answer Y when prompted apt install -y build-essential git pkg-config libssl-dev libsqlite3-dev","title":"Install dependencies"},{"location":"pi-bitwarden/#clone-the-repo","text":"git clone https://github.com/dani-garcia/bitwarden_rs.git cd bitwarden_rs","title":"Clone the repo"},{"location":"pi-bitwarden/#compile","text":"cargo build --features sqlite --release","title":"Compile"},{"location":"pi-bitwarden/#admin","text":"After compilation, the built binary will be ./target/release/bitwarden_rs . This should be moved to /usr/bin with mv ./target/release/bitwarden_rs /usr/bin/bitwarden_rs The data directory needs to be created with mkdir -p /var/lib/bitwarden_rs/data . This is where the bitwarden keys and database are stored. Create a user account with adduser bitwarden_rs . Make sure the ownership of everything in /var/lib/bitwarden_rs is set to the bitwarden_rs user.","title":"Admin"},{"location":"pi-bitwarden/#frontend","text":"Download the already built assets: cd /var/lib/bitwarden_rs # Amend the version as appropriate wget https://github.com/dani-garcia/bw_web_builds/releases/download/v2.17.1/bw_web_v2.17.1.tar.gz Extract them tar -xvf bw_web_v2.17.1.tar.gz","title":"Frontend"},{"location":"pi-bitwarden/#run","text":"Create the systemd service file. Copy the file from the wiki .","title":"Run"},{"location":"puppet-bolt/","text":"Puppet Bolt \u00b6 Install Bolt with: wget https://apt.puppet.com/puppet-tools-release-focal.deb sudo dpkg -i puppet-tools-release-focal.deb sudo apt-get update sudo apt-get install puppet-bolt If you want to create modules but dont want to publish them to the Puppet Forge, you can just add the directory to the module path .","title":"Puppet Bolt"},{"location":"puppet-bolt/#puppet-bolt","text":"Install Bolt with: wget https://apt.puppet.com/puppet-tools-release-focal.deb sudo dpkg -i puppet-tools-release-focal.deb sudo apt-get update sudo apt-get install puppet-bolt If you want to create modules but dont want to publish them to the Puppet Forge, you can just add the directory to the module path .","title":"Puppet Bolt"},{"location":"puppet-jails-deploy/","text":"Blue/Green Deployments with Puppet, NGINX and FreeBSD Jails \u00b6 At $WORK, we were going through a period of rapid growth and were planning on creating many more web apps. The apps are mainly Django/Python with a few NodeJS apps too. Previously they had been deployed on Debian servers and the deployment process was to SSH onto each app server and run git pull, then restart the processes. This solution works fine for apps with little traffic, but since we were growing rapidly, we were finding that this architecture wouldnt scale as much as we would like. At the time we were deciding on a new architecture (2017), the current modern solutions like Docker, Kubernetes and LXC/LXD were still very much in their infancy and not ready for production. In addition, one of the main gripes with Python apps is that they frequently link to C libraries. So when a new deployment contains a package updated via pip, what can happen is that it requires a new version of an OS package for a C library. Now you not only need to deploy the app, you also need to update the OS too. Proposed Architecture \u00b6 The solution we came up with was to deploy the apps inside FreeBSD jails, control the deployments via Puppet, and handle routing of requests from the load balancer to the correct jail using NGINX. FreeBSD Jails \u00b6 Jails work in a very similar way to LXC/LXD or like Docker but without immutable images. They are containers which share the host kernel but have their own copy of the base OS which runs isolated processes. Each jail can be allocated its own IP address and RW filesystem. To create a new jail, a filesystem containing the FreeBSD base system is created, the jail.conf files created and then the jail started. A jail runs the same init process tree as it would on bare metal or a VM. Once a jail is running, you can jexec inside the jail to run further commands. Puppet \u00b6 Puppet is a configuration management system. It lets you decide the desired state for a system and it will idempotently change the system to achieve the desired state. For example, if you wanted to install the NGINX package, make sure the config file was set up correctly and then start the service, you could write Puppet code like: package { 'nginx' : ensure => 'installed' , } file { '/etc/nginx/nginx.conf' : ensure => 'file' , content => template ( 'profiles/nginx/nginx.conf.erb' ), } service { 'nginx' : ensure => 'running' , enable => true , } That code is very generic and Puppet will work across multiple OSes and filesystems. You dont have to worry about OS specific actions, Puppet just does it all for you. NGINX \u00b6 With NGINX we can use it both to serve static assets like a web server and as a reverse proxy to the applications. The config can be set to route requests to specific domains or URLs to specific backends. In this case, the backends would be the IP addresses of the jails. Putting it all together \u00b6 I created a basic Puppet type and provider which can check for the presence or absence of jails. This was used in a module which performed the actions to set up the jails. The jail filesystem would run on ZFS which meant we could create a template dataset and then new jails were just clones of that dataset. When Puppet ran on the host system, it would perform the following actions in order to create a new jail: Download the FreeBSD base files tar.xz Create the template ZFS dataset and extract the base files into it Create some standard files like resolv.conf and install standard packages like Git and Puppet Create a ZFS snapshot of the template dataset Clone the template snapshot for the new jail Amend the jail.conf to include config for the new jail Start the new jail Run Puppet inside the jail to provision the app Subsequent Puppet runs on the host would amend the NGINX config to route traffic to the new jails IP address. When creating a new jail, we designated it as the \"test\" jail which had a specific domain or URL in the NGINX config. So all live traffic would continue to be served by the \"live\" jail, but any requests to a specific \"test\" domain/URL would instead be routed to the \"test\" jail. This meant that the QA department could run their tests in the \"test\" jail without it affect the \"live\" jail. The switchover was accomplished by changing the NGINX config and running nginx -s reload which gracefully reloads the NGINX processes with the new config without dropping any in flight requests. End Result \u00b6 This setup was running fine for years and allowed us to scale well. I wouldn't recommend going this route for future deployments. We spent a lot of time fighting FreeBSD because the support for third-party packages was so poor. We frequently had to reinvent the wheel to get things working. My recommendations would be: If you want to self-host and are a sufficiently large organisation, use Kubernetes. If you want to self-host and arent that big, use LXD and pair it with Ansbile or Saltstack. If you dont want to self-host, use something like Heroku or Fly.io","title":"Blue/Green deploymetns with Puppet, NGINX and FreeBSD Jails"},{"location":"puppet-jails-deploy/#bluegreen-deployments-with-puppet-nginx-and-freebsd-jails","text":"At $WORK, we were going through a period of rapid growth and were planning on creating many more web apps. The apps are mainly Django/Python with a few NodeJS apps too. Previously they had been deployed on Debian servers and the deployment process was to SSH onto each app server and run git pull, then restart the processes. This solution works fine for apps with little traffic, but since we were growing rapidly, we were finding that this architecture wouldnt scale as much as we would like. At the time we were deciding on a new architecture (2017), the current modern solutions like Docker, Kubernetes and LXC/LXD were still very much in their infancy and not ready for production. In addition, one of the main gripes with Python apps is that they frequently link to C libraries. So when a new deployment contains a package updated via pip, what can happen is that it requires a new version of an OS package for a C library. Now you not only need to deploy the app, you also need to update the OS too.","title":"Blue/Green Deployments with Puppet, NGINX and FreeBSD Jails"},{"location":"puppet-jails-deploy/#proposed-architecture","text":"The solution we came up with was to deploy the apps inside FreeBSD jails, control the deployments via Puppet, and handle routing of requests from the load balancer to the correct jail using NGINX.","title":"Proposed Architecture"},{"location":"puppet-jails-deploy/#freebsd-jails","text":"Jails work in a very similar way to LXC/LXD or like Docker but without immutable images. They are containers which share the host kernel but have their own copy of the base OS which runs isolated processes. Each jail can be allocated its own IP address and RW filesystem. To create a new jail, a filesystem containing the FreeBSD base system is created, the jail.conf files created and then the jail started. A jail runs the same init process tree as it would on bare metal or a VM. Once a jail is running, you can jexec inside the jail to run further commands.","title":"FreeBSD Jails"},{"location":"puppet-jails-deploy/#puppet","text":"Puppet is a configuration management system. It lets you decide the desired state for a system and it will idempotently change the system to achieve the desired state. For example, if you wanted to install the NGINX package, make sure the config file was set up correctly and then start the service, you could write Puppet code like: package { 'nginx' : ensure => 'installed' , } file { '/etc/nginx/nginx.conf' : ensure => 'file' , content => template ( 'profiles/nginx/nginx.conf.erb' ), } service { 'nginx' : ensure => 'running' , enable => true , } That code is very generic and Puppet will work across multiple OSes and filesystems. You dont have to worry about OS specific actions, Puppet just does it all for you.","title":"Puppet"},{"location":"puppet-jails-deploy/#nginx","text":"With NGINX we can use it both to serve static assets like a web server and as a reverse proxy to the applications. The config can be set to route requests to specific domains or URLs to specific backends. In this case, the backends would be the IP addresses of the jails.","title":"NGINX"},{"location":"puppet-jails-deploy/#putting-it-all-together","text":"I created a basic Puppet type and provider which can check for the presence or absence of jails. This was used in a module which performed the actions to set up the jails. The jail filesystem would run on ZFS which meant we could create a template dataset and then new jails were just clones of that dataset. When Puppet ran on the host system, it would perform the following actions in order to create a new jail: Download the FreeBSD base files tar.xz Create the template ZFS dataset and extract the base files into it Create some standard files like resolv.conf and install standard packages like Git and Puppet Create a ZFS snapshot of the template dataset Clone the template snapshot for the new jail Amend the jail.conf to include config for the new jail Start the new jail Run Puppet inside the jail to provision the app Subsequent Puppet runs on the host would amend the NGINX config to route traffic to the new jails IP address. When creating a new jail, we designated it as the \"test\" jail which had a specific domain or URL in the NGINX config. So all live traffic would continue to be served by the \"live\" jail, but any requests to a specific \"test\" domain/URL would instead be routed to the \"test\" jail. This meant that the QA department could run their tests in the \"test\" jail without it affect the \"live\" jail. The switchover was accomplished by changing the NGINX config and running nginx -s reload which gracefully reloads the NGINX processes with the new config without dropping any in flight requests.","title":"Putting it all together"},{"location":"puppet-jails-deploy/#end-result","text":"This setup was running fine for years and allowed us to scale well. I wouldn't recommend going this route for future deployments. We spent a lot of time fighting FreeBSD because the support for third-party packages was so poor. We frequently had to reinvent the wheel to get things working. My recommendations would be: If you want to self-host and are a sufficiently large organisation, use Kubernetes. If you want to self-host and arent that big, use LXD and pair it with Ansbile or Saltstack. If you dont want to self-host, use something like Heroku or Fly.io","title":"End Result"},{"location":"python/","text":"Modern Python \u00b6 Summary \u00b6 The following tools and libraries are known to work well with modern python. They are modular, so you can pick and choose the components you want based on your need. HTTP \u00b6 Client \u00b6 HTTPX Server \u00b6 FastAPI Uvicorn Databases \u00b6 SQLModel Alembic Async Processes \u00b6 Arq Cryptography \u00b6 pynacl secrets Tools \u00b6 Linting \u00b6 black Typing \u00b6 Pydantic Mypy Documentation \u00b6 mkdocs mkdocs-material mkdocstrings Code Complexity \u00b6 lizard radon Lizard \u00b6 lizard -x'*/tests/*' -l python -w src Radon \u00b6 radon cc --min B --average --total-average src radon mi --min B src Formatting/Styling \u00b6 isort flake8 flake8-blind-except flake8-bugbear flake8-coding flake8-commas flake8-debugger flake8-docstrings flake8-isort flake8-quotes flake8-sfs","title":"Modern Python"},{"location":"python/#modern-python","text":"","title":"Modern Python"},{"location":"python/#summary","text":"The following tools and libraries are known to work well with modern python. They are modular, so you can pick and choose the components you want based on your need.","title":"Summary"},{"location":"python/#http","text":"","title":"HTTP"},{"location":"python/#client","text":"HTTPX","title":"Client"},{"location":"python/#server","text":"FastAPI Uvicorn","title":"Server"},{"location":"python/#databases","text":"SQLModel Alembic","title":"Databases"},{"location":"python/#async-processes","text":"Arq","title":"Async Processes"},{"location":"python/#cryptography","text":"pynacl secrets","title":"Cryptography"},{"location":"python/#tools","text":"","title":"Tools"},{"location":"python/#linting","text":"black","title":"Linting"},{"location":"python/#typing","text":"Pydantic Mypy","title":"Typing"},{"location":"python/#documentation","text":"mkdocs mkdocs-material mkdocstrings","title":"Documentation"},{"location":"python/#code-complexity","text":"lizard radon","title":"Code Complexity"},{"location":"python/#lizard","text":"lizard -x'*/tests/*' -l python -w src","title":"Lizard"},{"location":"python/#radon","text":"radon cc --min B --average --total-average src radon mi --min B src","title":"Radon"},{"location":"python/#formattingstyling","text":"isort flake8 flake8-blind-except flake8-bugbear flake8-coding flake8-commas flake8-debugger flake8-docstrings flake8-isort flake8-quotes flake8-sfs","title":"Formatting/Styling"},{"location":"quick-k8s/","text":"Quick Multi-Node Kubernetes Cluster \u00b6 Multipass \u00b6 Multipass lets you easily spin up Ubuntu VMs on a workstation. # Install snap install multipass --classic Then to create a new instance, just run multipass launch . It will create a new instance based on an Ubuntu LTS image. To access the instance, just run multipass shell $name . You then have full access to the instance. The instances can also be bootstrapped via cloud-init in the same way that instances on cloud providers are. Microk8s \u00b6 Microk8s is a small Kubernetes distribution designed for appliances. # Install sudo snap install microk8s --classic --channel=1.16/stable sudo usermod -a -G microk8s $USER su - $USER Cluster \u00b6 So with two Multipass instances launched, and Microk8s installed on each, we can now join them together to form a cluster by running microk8s.add-node on the proposed master and then the requisite microk8s.join command on the other node.","title":"Quick Multi-Node Kubernetes Cluster"},{"location":"quick-k8s/#quick-multi-node-kubernetes-cluster","text":"","title":"Quick Multi-Node Kubernetes Cluster"},{"location":"quick-k8s/#multipass","text":"Multipass lets you easily spin up Ubuntu VMs on a workstation. # Install snap install multipass --classic Then to create a new instance, just run multipass launch . It will create a new instance based on an Ubuntu LTS image. To access the instance, just run multipass shell $name . You then have full access to the instance. The instances can also be bootstrapped via cloud-init in the same way that instances on cloud providers are.","title":"Multipass"},{"location":"quick-k8s/#microk8s","text":"Microk8s is a small Kubernetes distribution designed for appliances. # Install sudo snap install microk8s --classic --channel=1.16/stable sudo usermod -a -G microk8s $USER su - $USER","title":"Microk8s"},{"location":"quick-k8s/#cluster","text":"So with two Multipass instances launched, and Microk8s installed on each, we can now join them together to form a cluster by running microk8s.add-node on the proposed master and then the requisite microk8s.join command on the other node.","title":"Cluster"},{"location":"rqlite/","text":"RQLite \u00b6 SQLite, distributed over many nodes with consensus achieved with the Raft protocol. go get github.com/rqlite/rqlite cd ~/go/src/github.com/rqlite/rqlite/cmd/rqlite go get -t -d -v ./... go build # You now have the rqlite binary cd ~/go/src/github.com/rqlite/rqlite/cmd/rqlited go build # You now have the rqlited binary Set up the first cluster node: ./rqlited ~/node.1 Then subsequent cluster nodes: rqlited -http-addr localhost:4003 -raft-addr localhost:4004 -join http://localhost:4001 ~/node.2 Presumably you'd have the HTTP address and Raft address to be the same port on different servers, and you'd join to the same master node.","title":"RQLite"},{"location":"rqlite/#rqlite","text":"SQLite, distributed over many nodes with consensus achieved with the Raft protocol. go get github.com/rqlite/rqlite cd ~/go/src/github.com/rqlite/rqlite/cmd/rqlite go get -t -d -v ./... go build # You now have the rqlite binary cd ~/go/src/github.com/rqlite/rqlite/cmd/rqlited go build # You now have the rqlited binary Set up the first cluster node: ./rqlited ~/node.1 Then subsequent cluster nodes: rqlited -http-addr localhost:4003 -raft-addr localhost:4004 -join http://localhost:4001 ~/node.2 Presumably you'd have the HTTP address and Raft address to be the same port on different servers, and you'd join to the same master node.","title":"RQLite"},{"location":"saltstack/","text":"Saltstack install and config \u00b6 Install the salt package pkg install -y py36-salt Copy the sample files to create the master and/or minion configuration files cp -v /usr/local/etc/salt/master{.sample,\"\"} cp -v /usr/local/etc/salt/minion{.sample,\"\"} Set the master/minion services to start on boot sysrc salt_master_enable=\"YES\" sysrc salt_minion_enable=\"YES\" Salt expects state files to exist in the /srv/salt or /etc/salt directories which don't exist by default on FreeBSD so make symlinks instead: ln -s /usr/local/etc/salt /etc/salt ln -s /usr/local/etc/salt /srv/salt Start the services service salt_master onestart service salt_minion onestart Accept minion keys sent to the master salt-key -A # Press y to accept Create a test state file vi /usr/local/etc/salt/states/examples.sls --- install_packages: pkg.installed: - pkgs: - vim-lite Then apply the examples state salt '*' state.apply examples Salt Formulas \u00b6 Install the GitFS backend, this allows you to serve files from git repos. pkg install -y git py36-gitpython Edit the /usr/local/etc/salt/master configuration file: fileserver_backend: - git - roots gitfs_remotes: - https://github.com/saltstack-formulas/lynis-formula Restart the master. If master and minion are the same node, restart the minion service as well. service salt_master onerestart The formulas can then be used in the state file include: - lynis Salt equivalent to R10K and using git as a pillar source \u00b6 If the git server is also a minion, you can use Reactor to signal to the master to update the fileserver on each git push: https://docs.saltstack.com/en/latest/topics/tutorials/gitfs.html#refreshing-gitfs-upon-push You can also use git as a pillar source (host your specific config data in version control) https://docs.saltstack.com/en/latest/topics/tutorials/gitfs.html#using-git-as-an-external-pillar-source Installing RAET \u00b6 RAET support isn't enabled in the default package. If you install py27-salt and run pkg info py27-salt you can see in the options RAET: off . In order to use RAET, you need to build the py27-salt port. Compile the port pkg remove -y py27-salt portsnap fetch extract cd /usr/ports/sysutil/py-salt make config # Press space to select RAET make install Edit /srv/salt/master and /srv/salt/minion and add transport: raet Then restart the services service salt_master restart service salt_minion restart You will need to accept keys again salt-key salt-key -A Salt equivalent of hiera-eyaml \u00b6 Salt.runners.nacl Similar to hiera-eyaml, it is used for encrypting data stored in pillar: https://docs.saltstack.com/en/latest/ref/runners/all/salt.runners.nacl.html","title":"Saltstack install and config"},{"location":"saltstack/#saltstack-install-and-config","text":"Install the salt package pkg install -y py36-salt Copy the sample files to create the master and/or minion configuration files cp -v /usr/local/etc/salt/master{.sample,\"\"} cp -v /usr/local/etc/salt/minion{.sample,\"\"} Set the master/minion services to start on boot sysrc salt_master_enable=\"YES\" sysrc salt_minion_enable=\"YES\" Salt expects state files to exist in the /srv/salt or /etc/salt directories which don't exist by default on FreeBSD so make symlinks instead: ln -s /usr/local/etc/salt /etc/salt ln -s /usr/local/etc/salt /srv/salt Start the services service salt_master onestart service salt_minion onestart Accept minion keys sent to the master salt-key -A # Press y to accept Create a test state file vi /usr/local/etc/salt/states/examples.sls --- install_packages: pkg.installed: - pkgs: - vim-lite Then apply the examples state salt '*' state.apply examples","title":"Saltstack install and config"},{"location":"saltstack/#salt-formulas","text":"Install the GitFS backend, this allows you to serve files from git repos. pkg install -y git py36-gitpython Edit the /usr/local/etc/salt/master configuration file: fileserver_backend: - git - roots gitfs_remotes: - https://github.com/saltstack-formulas/lynis-formula Restart the master. If master and minion are the same node, restart the minion service as well. service salt_master onerestart The formulas can then be used in the state file include: - lynis","title":"Salt Formulas"},{"location":"saltstack/#salt-equivalent-to-r10k-and-using-git-as-a-pillar-source","text":"If the git server is also a minion, you can use Reactor to signal to the master to update the fileserver on each git push: https://docs.saltstack.com/en/latest/topics/tutorials/gitfs.html#refreshing-gitfs-upon-push You can also use git as a pillar source (host your specific config data in version control) https://docs.saltstack.com/en/latest/topics/tutorials/gitfs.html#using-git-as-an-external-pillar-source","title":"Salt equivalent to R10K and using git as a pillar source"},{"location":"saltstack/#installing-raet","text":"RAET support isn't enabled in the default package. If you install py27-salt and run pkg info py27-salt you can see in the options RAET: off . In order to use RAET, you need to build the py27-salt port. Compile the port pkg remove -y py27-salt portsnap fetch extract cd /usr/ports/sysutil/py-salt make config # Press space to select RAET make install Edit /srv/salt/master and /srv/salt/minion and add transport: raet Then restart the services service salt_master restart service salt_minion restart You will need to accept keys again salt-key salt-key -A","title":"Installing RAET"},{"location":"saltstack/#salt-equivalent-of-hiera-eyaml","text":"Salt.runners.nacl Similar to hiera-eyaml, it is used for encrypting data stored in pillar: https://docs.saltstack.com/en/latest/ref/runners/all/salt.runners.nacl.html","title":"Salt equivalent of hiera-eyaml"},{"location":"serverless/","text":"Serverless with Knative running in gVisor sandbox on Minikube \u00b6 Minikube - A Kubernetes distribution which starts a single-node cluster gVisor - A user-space kernel, written in Go, that implements a substantial portion of the Linux system call interface. Knative - Run serverless services on Kubernetes Install Minikube as described in the documentation. Install gVisor as per the docs : minikube start --container-runtime=containerd \\ --docker-opt containerd=/var/run/containerd/containerd.sock minikube addons enable gvisor kubectl get pod,runtimeclass gvisor -n kube-system","title":"Serverless"},{"location":"serverless/#serverless-with-knative-running-in-gvisor-sandbox-on-minikube","text":"Minikube - A Kubernetes distribution which starts a single-node cluster gVisor - A user-space kernel, written in Go, that implements a substantial portion of the Linux system call interface. Knative - Run serverless services on Kubernetes Install Minikube as described in the documentation. Install gVisor as per the docs : minikube start --container-runtime=containerd \\ --docker-opt containerd=/var/run/containerd/containerd.sock minikube addons enable gvisor kubectl get pod,runtimeclass gvisor -n kube-system","title":"Serverless with Knative running in gVisor sandbox on Minikube"},{"location":"signify/","text":"Signify \u00b6 Sign and verify files Generate keys without password (remove -n flag to ask for a password) signify-openbsd -G -p keyname.pub -s keyname.sec -n Sign a file signify-openbsd -S -s keyname.sec -m $file_to_sign -x $signature_file Verify a file signify-openbsd -V -p keyname.pub -m $file_to_verify -x $signature_file","title":"Signify"},{"location":"signify/#signify","text":"Sign and verify files Generate keys without password (remove -n flag to ask for a password) signify-openbsd -G -p keyname.pub -s keyname.sec -n Sign a file signify-openbsd -S -s keyname.sec -m $file_to_sign -x $signature_file Verify a file signify-openbsd -V -p keyname.pub -m $file_to_verify -x $signature_file","title":"Signify"}]}